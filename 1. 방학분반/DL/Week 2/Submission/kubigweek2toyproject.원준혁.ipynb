{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "19h7ZKlN5QlJ"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1=torch.tensor([[[[1.,2.],[3.,4.],[5.,6.],[7.,8.]]]])\n",
        "tensor2=torch.tensor([[[[9.,10.],[11.,12.],[13.,14.],[15.,16.]]]])"
      ],
      "metadata": {
        "id": "0efaxGor5WCV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1+tensor2)\n",
        "print(torch.add(tensor1,tensor2))\n",
        "print(tensor1-tensor2)\n",
        "print(torch.sub(tensor1, tensor2))\n",
        "print(tensor1 * tensor2)\n",
        "print(torch.mul(tensor1,tensor2))\n",
        "print(tensor2/tensor1)\n",
        "print(torch.div(tensor2, tensor1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXtnWG7Q5kZw",
        "outputId": "eb5a9909-2d82-4423-9e42-0e5ac3d7edc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[10., 12.],\n",
            "          [14., 16.],\n",
            "          [18., 20.],\n",
            "          [22., 24.]]]])\n",
            "tensor([[[[10., 12.],\n",
            "          [14., 16.],\n",
            "          [18., 20.],\n",
            "          [22., 24.]]]])\n",
            "tensor([[[[-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.]]]])\n",
            "tensor([[[[-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.]]]])\n",
            "tensor([[[[  9.,  20.],\n",
            "          [ 33.,  48.],\n",
            "          [ 65.,  84.],\n",
            "          [105., 128.]]]])\n",
            "tensor([[[[  9.,  20.],\n",
            "          [ 33.,  48.],\n",
            "          [ 65.,  84.],\n",
            "          [105., 128.]]]])\n",
            "tensor([[[[9.0000, 5.0000],\n",
            "          [3.6667, 3.0000],\n",
            "          [2.6000, 2.3333],\n",
            "          [2.1429, 2.0000]]]])\n",
            "tensor([[[[9.0000, 5.0000],\n",
            "          [3.6667, 3.0000],\n",
            "          [2.6000, 2.3333],\n",
            "          [2.1429, 2.0000]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.randn(10, 3, 4)\n",
        "tensor2 = torch.randn(10, 4, 5)\n",
        "print(torch.matmul(tensor1, tensor2).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOI6CUJr5ldA",
        "outputId": "d31c021d-e937-4aea-de73-8b443c62d96c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that cuda is operating appropriately\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda') #relatively fast\n",
        "else:\n",
        "  device=torch.device('cpu') #only cpu for training & evaluating #Very slow\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFhtvAof5u33",
        "outputId": "6f169165-5aa8-4e2a-a0e7-d04c962d3370"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#Classification Loss\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "loss=nn.BCELoss()\n",
        "loss=nn.MultiMarginLoss()\n",
        "\n",
        "#Regression Loss\n",
        "loss=nn.MSELoss()\n",
        "loss=nn.L1Loss()\n",
        "loss=nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "JbibCSxz9nXa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# important variable is just written on each:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "optimizer =torch.optim.Adagrad(model.parameters(), lr=0.1, weight_decay=0.01)\n",
        "optimizer =torch.optim.RMSprop(model.parameters(), lr=0.1, alpha=0.9)\n",
        "optimizer =torch.optim.Adadelta(model.parameters(), lr=0.01, rho=0.95)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.953))\n",
        "\n",
        "\n",
        "optimizer.zero_grad() #Since optimizer cumulates the gradient, we have to make it clear by .zero_grad()\n",
        "loss(model(input), target).backward() #backward() means backpropagation, and in torch, we can implement backpropagation by only one-line code: loss.backward()\n",
        "optimizer.step() #Then, optimizer updates the gradient on weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "lFEFD4Aq9gSK",
        "outputId": "f1bfecb9-8dd1-420c-faef-afe538cc5aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bf70e00a2940>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Since optimizer cumulates the gradient, we have to make it clear by .zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#backward() means backpropagation, and in torch, we can implement backpropagation by only one-line code: loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Then, optimizer updates the gradient on weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-18e7ac7f4adb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/flatten.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'flatten'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "#FashionMNIST is a quite famous dataset: Fashion Image Dataset\n",
        "\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()"
      ],
      "metadata": {
        "id": "P_e7LAcFA-G0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#You can control the Step Size & batch size & Epochs: How many times do you want to train the model?\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n"
      ],
      "metadata": {
        "id": "cybfjQtty5VM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Define your loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "#Define your optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "mrZx05Uny56I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train() #Training mode\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X) #forward\n",
        "        loss = loss_fn(pred, y) #compute the loss\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() #zero grad first! Always !\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #Update your weights\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0 #define the variable\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "\n",
        "    with torch.no_grad(): #For test mode, testing is just a 'forward' process, not backprop\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches #mean\n",
        "    correct /= size #mean\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "I1oPpkRAy7ZU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 초기 설정일 때 -> epoch가 증가할수록 accuracy는 커지고 avg loss는 점점 감소함\n",
        "## 마지막에 Accuracy: 62.2%, Avg loss: 1.957136 도출됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLv_FA-pzEfJ",
        "outputId": "6ccb49ac-81df-44bc-8b4b-bfb92c18a507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302019  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a11d3b5b8fce>:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.297754  [ 6464/60000]\n",
            "loss: 2.289533  [12864/60000]\n",
            "loss: 2.290251  [19264/60000]\n",
            "loss: 2.285597  [25664/60000]\n",
            "loss: 2.275407  [32064/60000]\n",
            "loss: 2.277343  [38464/60000]\n",
            "loss: 2.260961  [44864/60000]\n",
            "loss: 2.277278  [51264/60000]\n",
            "loss: 2.268801  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.1%, Avg loss: 2.262665 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.270101  [   64/60000]\n",
            "loss: 2.266349  [ 6464/60000]\n",
            "loss: 2.243391  [12864/60000]\n",
            "loss: 2.255875  [19264/60000]\n",
            "loss: 2.227624  [25664/60000]\n",
            "loss: 2.215235  [32064/60000]\n",
            "loss: 2.227305  [38464/60000]\n",
            "loss: 2.195194  [44864/60000]\n",
            "loss: 2.238938  [51264/60000]\n",
            "loss: 2.216415  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.211797 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.227814  [   64/60000]\n",
            "loss: 2.228519  [ 6464/60000]\n",
            "loss: 2.192428  [12864/60000]\n",
            "loss: 2.213388  [19264/60000]\n",
            "loss: 2.169387  [25664/60000]\n",
            "loss: 2.163352  [32064/60000]\n",
            "loss: 2.176763  [38464/60000]\n",
            "loss: 2.137134  [44864/60000]\n",
            "loss: 2.203932  [51264/60000]\n",
            "loss: 2.174816  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 2.171662 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.195711  [   64/60000]\n",
            "loss: 2.203745  [ 6464/60000]\n",
            "loss: 2.160300  [12864/60000]\n",
            "loss: 2.181980  [19264/60000]\n",
            "loss: 2.131579  [25664/60000]\n",
            "loss: 2.129651  [32064/60000]\n",
            "loss: 2.138626  [38464/60000]\n",
            "loss: 2.097004  [44864/60000]\n",
            "loss: 2.172076  [51264/60000]\n",
            "loss: 2.142321  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 37.8%, Avg loss: 2.138772 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.164398  [   64/60000]\n",
            "loss: 2.171954  [ 6464/60000]\n",
            "loss: 2.132253  [12864/60000]\n",
            "loss: 2.157070  [19264/60000]\n",
            "loss: 2.089704  [25664/60000]\n",
            "loss: 2.098960  [32064/60000]\n",
            "loss: 2.095932  [38464/60000]\n",
            "loss: 2.058567  [44864/60000]\n",
            "loss: 2.131703  [51264/60000]\n",
            "loss: 2.103564  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.1%, Avg loss: 2.100612 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.126404  [   64/60000]\n",
            "loss: 2.129657  [ 6464/60000]\n",
            "loss: 2.100401  [12864/60000]\n",
            "loss: 2.124927  [19264/60000]\n",
            "loss: 2.046025  [25664/60000]\n",
            "loss: 2.069845  [32064/60000]\n",
            "loss: 2.050967  [38464/60000]\n",
            "loss: 2.023162  [44864/60000]\n",
            "loss: 2.095253  [51264/60000]\n",
            "loss: 2.061909  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.5%, Avg loss: 2.060289 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.092566  [   64/60000]\n",
            "loss: 2.087067  [ 6464/60000]\n",
            "loss: 2.060161  [12864/60000]\n",
            "loss: 2.082335  [19264/60000]\n",
            "loss: 2.008953  [25664/60000]\n",
            "loss: 2.039971  [32064/60000]\n",
            "loss: 2.013529  [38464/60000]\n",
            "loss: 1.990432  [44864/60000]\n",
            "loss: 2.062537  [51264/60000]\n",
            "loss: 2.028349  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.7%, Avg loss: 2.024224 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.057092  [   64/60000]\n",
            "loss: 2.049363  [ 6464/60000]\n",
            "loss: 2.019485  [12864/60000]\n",
            "loss: 2.048036  [19264/60000]\n",
            "loss: 1.982489  [25664/60000]\n",
            "loss: 2.007957  [32064/60000]\n",
            "loss: 1.983327  [38464/60000]\n",
            "loss: 1.958951  [44864/60000]\n",
            "loss: 2.032411  [51264/60000]\n",
            "loss: 2.001814  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.8%, Avg loss: 1.995755 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.023812  [   64/60000]\n",
            "loss: 2.020123  [ 6464/60000]\n",
            "loss: 1.986692  [12864/60000]\n",
            "loss: 2.022151  [19264/60000]\n",
            "loss: 1.962936  [25664/60000]\n",
            "loss: 1.981491  [32064/60000]\n",
            "loss: 1.959747  [38464/60000]\n",
            "loss: 1.934165  [44864/60000]\n",
            "loss: 2.009334  [51264/60000]\n",
            "loss: 1.980794  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 1.973957 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.998860  [   64/60000]\n",
            "loss: 1.998782  [ 6464/60000]\n",
            "loss: 1.962972  [12864/60000]\n",
            "loss: 2.002222  [19264/60000]\n",
            "loss: 1.947605  [25664/60000]\n",
            "loss: 1.961265  [32064/60000]\n",
            "loss: 1.941543  [38464/60000]\n",
            "loss: 1.915173  [44864/60000]\n",
            "loss: 1.991968  [51264/60000]\n",
            "loss: 1.964217  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.957136 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 2.\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 똑같은 설정에서 초기화 안하고 5번 더 실행하니 accuarcy가 조금 더 커지고 avg loss도 조금 더 작아짐\n",
        "## Accuracy: 62.4%, Avg loss: 1.910942"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPYI5GwHz2aI",
        "outputId": "9460ed12-ad3a-4bce-a2c6-4b67f76f8afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.980227  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a11d3b5b8fce>:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.982850  [ 6464/60000]\n",
            "loss: 1.945252  [12864/60000]\n",
            "loss: 1.986613  [19264/60000]\n",
            "loss: 1.935581  [25664/60000]\n",
            "loss: 1.945526  [32064/60000]\n",
            "loss: 1.927302  [38464/60000]\n",
            "loss: 1.900296  [44864/60000]\n",
            "loss: 1.978459  [51264/60000]\n",
            "loss: 1.951011  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 1.943891 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.965826  [   64/60000]\n",
            "loss: 1.970529  [ 6464/60000]\n",
            "loss: 1.931482  [12864/60000]\n",
            "loss: 1.974138  [19264/60000]\n",
            "loss: 1.926034  [25664/60000]\n",
            "loss: 1.932991  [32064/60000]\n",
            "loss: 1.915908  [38464/60000]\n",
            "loss: 1.888377  [44864/60000]\n",
            "loss: 1.967656  [51264/60000]\n",
            "loss: 1.940295  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.933233 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.954393  [   64/60000]\n",
            "loss: 1.960721  [ 6464/60000]\n",
            "loss: 1.920483  [12864/60000]\n",
            "loss: 1.963968  [19264/60000]\n",
            "loss: 1.918303  [25664/60000]\n",
            "loss: 1.922800  [32064/60000]\n",
            "loss: 1.906582  [38464/60000]\n",
            "loss: 1.878630  [44864/60000]\n",
            "loss: 1.958819  [51264/60000]\n",
            "loss: 1.931432  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 1.924479 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.945118  [   64/60000]\n",
            "loss: 1.952723  [ 6464/60000]\n",
            "loss: 1.911504  [12864/60000]\n",
            "loss: 1.955524  [19264/60000]\n",
            "loss: 1.911916  [25664/60000]\n",
            "loss: 1.914358  [32064/60000]\n",
            "loss: 1.898790  [38464/60000]\n",
            "loss: 1.870512  [44864/60000]\n",
            "loss: 1.951450  [51264/60000]\n",
            "loss: 1.923980  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.917160 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.937448  [   64/60000]\n",
            "loss: 1.946064  [ 6464/60000]\n",
            "loss: 1.904039  [12864/60000]\n",
            "loss: 1.948400  [19264/60000]\n",
            "loss: 1.906544  [25664/60000]\n",
            "loss: 1.907247  [32064/60000]\n",
            "loss: 1.892163  [38464/60000]\n",
            "loss: 1.863642  [44864/60000]\n",
            "loss: 1.945203  [51264/60000]\n",
            "loss: 1.917621  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.910942 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 3. loss를 multimargin으로 변경 ->\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "loss=nn.MultiMarginLoss()\n",
        "\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## Accuracy: 46.2%, Avg loss: 0.589631 가 나와서 CrossEntropyLoss function이 더 좋은 것으로 추정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpsK9LWy00qM",
        "outputId": "cdc6b568-ba1a-48ae-d561-c5934149d94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.901103  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-02df7b91a53e>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.898486  [ 6464/60000]\n",
            "loss: 0.900320  [12864/60000]\n",
            "loss: 0.896388  [19264/60000]\n",
            "loss: 0.878001  [25664/60000]\n",
            "loss: 0.879069  [32064/60000]\n",
            "loss: 0.881796  [38464/60000]\n",
            "loss: 0.875863  [44864/60000]\n",
            "loss: 0.875505  [51264/60000]\n",
            "loss: 0.858771  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 0.864688 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.875179  [   64/60000]\n",
            "loss: 0.867678  [ 6464/60000]\n",
            "loss: 0.862857  [12864/60000]\n",
            "loss: 0.864196  [19264/60000]\n",
            "loss: 0.823300  [25664/60000]\n",
            "loss: 0.811023  [32064/60000]\n",
            "loss: 0.843168  [38464/60000]\n",
            "loss: 0.816018  [44864/60000]\n",
            "loss: 0.832732  [51264/60000]\n",
            "loss: 0.788126  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.9%, Avg loss: 0.806026 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.831437  [   64/60000]\n",
            "loss: 0.812420  [ 6464/60000]\n",
            "loss: 0.797430  [12864/60000]\n",
            "loss: 0.817775  [19264/60000]\n",
            "loss: 0.732962  [25664/60000]\n",
            "loss: 0.734231  [32064/60000]\n",
            "loss: 0.788054  [38464/60000]\n",
            "loss: 0.751548  [44864/60000]\n",
            "loss: 0.783771  [51264/60000]\n",
            "loss: 0.721869  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.6%, Avg loss: 0.748228 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.785548  [   64/60000]\n",
            "loss: 0.764207  [ 6464/60000]\n",
            "loss: 0.750154  [12864/60000]\n",
            "loss: 0.771284  [19264/60000]\n",
            "loss: 0.662700  [25664/60000]\n",
            "loss: 0.686154  [32064/60000]\n",
            "loss: 0.729421  [38464/60000]\n",
            "loss: 0.699712  [44864/60000]\n",
            "loss: 0.733888  [51264/60000]\n",
            "loss: 0.655177  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.5%, Avg loss: 0.696240 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.746650  [   64/60000]\n",
            "loss: 0.725708  [ 6464/60000]\n",
            "loss: 0.712620  [12864/60000]\n",
            "loss: 0.727841  [19264/60000]\n",
            "loss: 0.614250  [25664/60000]\n",
            "loss: 0.648237  [32064/60000]\n",
            "loss: 0.674790  [38464/60000]\n",
            "loss: 0.658081  [44864/60000]\n",
            "loss: 0.690113  [51264/60000]\n",
            "loss: 0.595997  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 0.657041 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.713603  [   64/60000]\n",
            "loss: 0.696685  [ 6464/60000]\n",
            "loss: 0.685584  [12864/60000]\n",
            "loss: 0.696094  [19264/60000]\n",
            "loss: 0.580866  [25664/60000]\n",
            "loss: 0.620288  [32064/60000]\n",
            "loss: 0.634653  [38464/60000]\n",
            "loss: 0.629834  [44864/60000]\n",
            "loss: 0.658581  [51264/60000]\n",
            "loss: 0.556131  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 0.631475 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.691125  [   64/60000]\n",
            "loss: 0.678857  [ 6464/60000]\n",
            "loss: 0.667618  [12864/60000]\n",
            "loss: 0.675616  [19264/60000]\n",
            "loss: 0.558725  [25664/60000]\n",
            "loss: 0.601230  [32064/60000]\n",
            "loss: 0.610006  [38464/60000]\n",
            "loss: 0.612071  [44864/60000]\n",
            "loss: 0.638147  [51264/60000]\n",
            "loss: 0.531665  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 0.615119 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.676521  [   64/60000]\n",
            "loss: 0.667756  [ 6464/60000]\n",
            "loss: 0.655546  [12864/60000]\n",
            "loss: 0.662005  [19264/60000]\n",
            "loss: 0.543553  [25664/60000]\n",
            "loss: 0.588105  [32064/60000]\n",
            "loss: 0.594118  [38464/60000]\n",
            "loss: 0.600192  [44864/60000]\n",
            "loss: 0.624180  [51264/60000]\n",
            "loss: 0.515511  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 0.603944 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.666431  [   64/60000]\n",
            "loss: 0.660241  [ 6464/60000]\n",
            "loss: 0.646996  [12864/60000]\n",
            "loss: 0.652250  [19264/60000]\n",
            "loss: 0.532605  [25664/60000]\n",
            "loss: 0.578675  [32064/60000]\n",
            "loss: 0.583043  [38464/60000]\n",
            "loss: 0.591715  [44864/60000]\n",
            "loss: 0.613992  [51264/60000]\n",
            "loss: 0.504016  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 0.595824 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.659040  [   64/60000]\n",
            "loss: 0.654805  [ 6464/60000]\n",
            "loss: 0.640626  [12864/60000]\n",
            "loss: 0.644821  [19264/60000]\n",
            "loss: 0.524313  [25664/60000]\n",
            "loss: 0.571596  [32064/60000]\n",
            "loss: 0.574848  [38464/60000]\n",
            "loss: 0.585338  [44864/60000]\n",
            "loss: 0.606187  [51264/60000]\n",
            "loss: 0.495371  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 0.589631 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 4. -adam\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.953))\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 좋은 model인 adam 답게 accuaracy가 많이 상승함 최종 단계에서  Accuracy: 65.5%, Avg loss: 1.944865가 도출됨."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzwy0PnN2R_y",
        "outputId": "5c461fc3-f76a-43fa-fc3c-b386cbaa9d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.299693  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-9ab97015cc43>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.299712  [ 6464/60000]\n",
            "loss: 2.291980  [12864/60000]\n",
            "loss: 2.293207  [19264/60000]\n",
            "loss: 2.291901  [25664/60000]\n",
            "loss: 2.271936  [32064/60000]\n",
            "loss: 2.278277  [38464/60000]\n",
            "loss: 2.262625  [44864/60000]\n",
            "loss: 2.279109  [51264/60000]\n",
            "loss: 2.267031  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 25.1%, Avg loss: 2.266836 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.265261  [   64/60000]\n",
            "loss: 2.273867  [ 6464/60000]\n",
            "loss: 2.251684  [12864/60000]\n",
            "loss: 2.264967  [19264/60000]\n",
            "loss: 2.248590  [25664/60000]\n",
            "loss: 2.217466  [32064/60000]\n",
            "loss: 2.236955  [38464/60000]\n",
            "loss: 2.206085  [44864/60000]\n",
            "loss: 2.244308  [51264/60000]\n",
            "loss: 2.218927  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 30.4%, Avg loss: 2.220415 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.223910  [   64/60000]\n",
            "loss: 2.231414  [ 6464/60000]\n",
            "loss: 2.196985  [12864/60000]\n",
            "loss: 2.223028  [19264/60000]\n",
            "loss: 2.179059  [25664/60000]\n",
            "loss: 2.157420  [32064/60000]\n",
            "loss: 2.185885  [38464/60000]\n",
            "loss: 2.147485  [44864/60000]\n",
            "loss: 2.204463  [51264/60000]\n",
            "loss: 2.165495  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 34.1%, Avg loss: 2.172221 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.186955  [   64/60000]\n",
            "loss: 2.196157  [ 6464/60000]\n",
            "loss: 2.155910  [12864/60000]\n",
            "loss: 2.180324  [19264/60000]\n",
            "loss: 2.123851  [25664/60000]\n",
            "loss: 2.115188  [32064/60000]\n",
            "loss: 2.136579  [38464/60000]\n",
            "loss: 2.101807  [44864/60000]\n",
            "loss: 2.163263  [51264/60000]\n",
            "loss: 2.111576  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 2.127415 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.149998  [   64/60000]\n",
            "loss: 2.157570  [ 6464/60000]\n",
            "loss: 2.120409  [12864/60000]\n",
            "loss: 2.143648  [19264/60000]\n",
            "loss: 2.070619  [25664/60000]\n",
            "loss: 2.076587  [32064/60000]\n",
            "loss: 2.085464  [38464/60000]\n",
            "loss: 2.061215  [44864/60000]\n",
            "loss: 2.119965  [51264/60000]\n",
            "loss: 2.057621  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.2%, Avg loss: 2.083596 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.112091  [   64/60000]\n",
            "loss: 2.117034  [ 6464/60000]\n",
            "loss: 2.084879  [12864/60000]\n",
            "loss: 2.105212  [19264/60000]\n",
            "loss: 2.023999  [25664/60000]\n",
            "loss: 2.042421  [32064/60000]\n",
            "loss: 2.043316  [38464/60000]\n",
            "loss: 2.029975  [44864/60000]\n",
            "loss: 2.085486  [51264/60000]\n",
            "loss: 2.014470  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.6%, Avg loss: 2.044962 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.081545  [   64/60000]\n",
            "loss: 2.080758  [ 6464/60000]\n",
            "loss: 2.047330  [12864/60000]\n",
            "loss: 2.066733  [19264/60000]\n",
            "loss: 1.987547  [25664/60000]\n",
            "loss: 2.011109  [32064/60000]\n",
            "loss: 2.012002  [38464/60000]\n",
            "loss: 2.002406  [44864/60000]\n",
            "loss: 2.055327  [51264/60000]\n",
            "loss: 1.982454  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.2%, Avg loss: 2.012730 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.050448  [   64/60000]\n",
            "loss: 2.049157  [ 6464/60000]\n",
            "loss: 2.008621  [12864/60000]\n",
            "loss: 2.036347  [19264/60000]\n",
            "loss: 1.961911  [25664/60000]\n",
            "loss: 1.978942  [32064/60000]\n",
            "loss: 1.987551  [38464/60000]\n",
            "loss: 1.974775  [44864/60000]\n",
            "loss: 2.024358  [51264/60000]\n",
            "loss: 1.957278  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 1.985453 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.016602  [   64/60000]\n",
            "loss: 2.020212  [ 6464/60000]\n",
            "loss: 1.970056  [12864/60000]\n",
            "loss: 2.011699  [19264/60000]\n",
            "loss: 1.943682  [25664/60000]\n",
            "loss: 1.949481  [32064/60000]\n",
            "loss: 1.967616  [38464/60000]\n",
            "loss: 1.950576  [44864/60000]\n",
            "loss: 1.997182  [51264/60000]\n",
            "loss: 1.935971  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.962991 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.988025  [   64/60000]\n",
            "loss: 1.997207  [ 6464/60000]\n",
            "loss: 1.939291  [12864/60000]\n",
            "loss: 1.991557  [19264/60000]\n",
            "loss: 1.928642  [25664/60000]\n",
            "loss: 1.926008  [32064/60000]\n",
            "loss: 1.951242  [38464/60000]\n",
            "loss: 1.931621  [44864/60000]\n",
            "loss: 1.975964  [51264/60000]\n",
            "loss: 1.917358  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.5%, Avg loss: 1.944865 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 5. -adam에서의 beta값 변경\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.98))\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## beta2값을 0.98로 1에 더 가깝게 하여 훈련시켜봄\n",
        "## 첫 정확도가 18.5%로 많이 낮고 훈련을 시켜보니 62%까지 정확도가 높아지긴 하는데 이전보다는 낮음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC-B40J6zE9x",
        "outputId": "fa21a5fa-80f9-4f3b-9317-d61e5f7c1af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298690  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-1bed1e3a2582>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.299908  [ 6464/60000]\n",
            "loss: 2.295314  [12864/60000]\n",
            "loss: 2.295183  [19264/60000]\n",
            "loss: 2.295446  [25664/60000]\n",
            "loss: 2.281963  [32064/60000]\n",
            "loss: 2.274585  [38464/60000]\n",
            "loss: 2.267216  [44864/60000]\n",
            "loss: 2.278887  [51264/60000]\n",
            "loss: 2.270106  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 18.5%, Avg loss: 2.272347 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.266613  [   64/60000]\n",
            "loss: 2.280055  [ 6464/60000]\n",
            "loss: 2.264188  [12864/60000]\n",
            "loss: 2.272315  [19264/60000]\n",
            "loss: 2.266539  [25664/60000]\n",
            "loss: 2.241961  [32064/60000]\n",
            "loss: 2.238240  [38464/60000]\n",
            "loss: 2.220519  [44864/60000]\n",
            "loss: 2.245748  [51264/60000]\n",
            "loss: 2.231546  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 35.1%, Avg loss: 2.232625 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.231263  [   64/60000]\n",
            "loss: 2.241509  [ 6464/60000]\n",
            "loss: 2.215904  [12864/60000]\n",
            "loss: 2.232483  [19264/60000]\n",
            "loss: 2.195396  [25664/60000]\n",
            "loss: 2.180177  [32064/60000]\n",
            "loss: 2.179650  [38464/60000]\n",
            "loss: 2.151511  [44864/60000]\n",
            "loss: 2.201318  [51264/60000]\n",
            "loss: 2.172066  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.1%, Avg loss: 2.174924 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.185569  [   64/60000]\n",
            "loss: 2.191582  [ 6464/60000]\n",
            "loss: 2.164222  [12864/60000]\n",
            "loss: 2.186687  [19264/60000]\n",
            "loss: 2.120729  [25664/60000]\n",
            "loss: 2.127120  [32064/60000]\n",
            "loss: 2.118391  [38464/60000]\n",
            "loss: 2.091787  [44864/60000]\n",
            "loss: 2.155980  [51264/60000]\n",
            "loss: 2.116961  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.8%, Avg loss: 2.125098 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.144855  [   64/60000]\n",
            "loss: 2.149708  [ 6464/60000]\n",
            "loss: 2.128860  [12864/60000]\n",
            "loss: 2.148581  [19264/60000]\n",
            "loss: 2.066401  [25664/60000]\n",
            "loss: 2.089682  [32064/60000]\n",
            "loss: 2.064618  [38464/60000]\n",
            "loss: 2.047556  [44864/60000]\n",
            "loss: 2.117211  [51264/60000]\n",
            "loss: 2.067843  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 2.083199 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.112295  [   64/60000]\n",
            "loss: 2.113589  [ 6464/60000]\n",
            "loss: 2.095949  [12864/60000]\n",
            "loss: 2.107989  [19264/60000]\n",
            "loss: 2.024209  [25664/60000]\n",
            "loss: 2.061367  [32064/60000]\n",
            "loss: 2.023553  [38464/60000]\n",
            "loss: 2.017460  [44864/60000]\n",
            "loss: 2.090354  [51264/60000]\n",
            "loss: 2.031469  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.0%, Avg loss: 2.046064 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.087782  [   64/60000]\n",
            "loss: 2.078450  [ 6464/60000]\n",
            "loss: 2.060842  [12864/60000]\n",
            "loss: 2.068315  [19264/60000]\n",
            "loss: 1.990253  [25664/60000]\n",
            "loss: 2.035208  [32064/60000]\n",
            "loss: 1.993180  [38464/60000]\n",
            "loss: 1.989819  [44864/60000]\n",
            "loss: 2.065969  [51264/60000]\n",
            "loss: 2.004752  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.5%, Avg loss: 2.016526 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.060329  [   64/60000]\n",
            "loss: 2.048691  [ 6464/60000]\n",
            "loss: 2.027040  [12864/60000]\n",
            "loss: 2.039898  [19264/60000]\n",
            "loss: 1.968512  [25664/60000]\n",
            "loss: 2.005717  [32064/60000]\n",
            "loss: 1.969173  [38464/60000]\n",
            "loss: 1.960472  [44864/60000]\n",
            "loss: 2.037869  [51264/60000]\n",
            "loss: 1.984729  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 1.991412 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.027121  [   64/60000]\n",
            "loss: 2.019467  [ 6464/60000]\n",
            "loss: 1.990260  [12864/60000]\n",
            "loss: 2.016095  [19264/60000]\n",
            "loss: 1.955000  [25664/60000]\n",
            "loss: 1.977580  [32064/60000]\n",
            "loss: 1.949748  [38464/60000]\n",
            "loss: 1.934734  [44864/60000]\n",
            "loss: 2.012964  [51264/60000]\n",
            "loss: 1.968267  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.971058 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.999502  [   64/60000]\n",
            "loss: 1.997159  [ 6464/60000]\n",
            "loss: 1.963089  [12864/60000]\n",
            "loss: 1.997218  [19264/60000]\n",
            "loss: 1.943243  [25664/60000]\n",
            "loss: 1.957199  [32064/60000]\n",
            "loss: 1.933679  [38464/60000]\n",
            "loss: 1.915630  [44864/60000]\n",
            "loss: 1.995415  [51264/60000]\n",
            "loss: 1.953976  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.955147 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 6. -adam에서 lr 수정\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02, betas=(0.9, 0.953))\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 이번엔 adam의 lr을 0.02로 수정\n",
        "## 첫 epoch에서 accuarcy는 28%였지만 학습이 진행되면서 accuarcy가 많이 상승하진 않음. avg loss는 최종적으로 1.9 정도로 비슷하긴 함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy5pIA8v1GQH",
        "outputId": "193b0f36-fc46-4f95-a320-8ff593c95f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.309590  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-7ca5ce3511a6>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.302191  [ 6464/60000]\n",
            "loss: 2.296564  [12864/60000]\n",
            "loss: 2.289904  [19264/60000]\n",
            "loss: 2.288234  [25664/60000]\n",
            "loss: 2.291231  [32064/60000]\n",
            "loss: 2.285020  [38464/60000]\n",
            "loss: 2.283772  [44864/60000]\n",
            "loss: 2.286830  [51264/60000]\n",
            "loss: 2.276348  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 28.8%, Avg loss: 2.271382 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.287292  [   64/60000]\n",
            "loss: 2.277210  [ 6464/60000]\n",
            "loss: 2.262167  [12864/60000]\n",
            "loss: 2.255956  [19264/60000]\n",
            "loss: 2.246526  [25664/60000]\n",
            "loss: 2.253341  [32064/60000]\n",
            "loss: 2.247293  [38464/60000]\n",
            "loss: 2.237858  [44864/60000]\n",
            "loss: 2.254161  [51264/60000]\n",
            "loss: 2.230322  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 2.224002 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.256889  [   64/60000]\n",
            "loss: 2.239934  [ 6464/60000]\n",
            "loss: 2.207743  [12864/60000]\n",
            "loss: 2.205168  [19264/60000]\n",
            "loss: 2.174203  [25664/60000]\n",
            "loss: 2.193615  [32064/60000]\n",
            "loss: 2.190497  [38464/60000]\n",
            "loss: 2.172441  [44864/60000]\n",
            "loss: 2.214535  [51264/60000]\n",
            "loss: 2.176895  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 2.170028 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.218332  [   64/60000]\n",
            "loss: 2.199730  [ 6464/60000]\n",
            "loss: 2.157548  [12864/60000]\n",
            "loss: 2.159372  [19264/60000]\n",
            "loss: 2.116713  [25664/60000]\n",
            "loss: 2.145296  [32064/60000]\n",
            "loss: 2.142461  [38464/60000]\n",
            "loss: 2.121513  [44864/60000]\n",
            "loss: 2.179074  [51264/60000]\n",
            "loss: 2.136300  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 2.125769 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.182979  [   64/60000]\n",
            "loss: 2.160472  [ 6464/60000]\n",
            "loss: 2.120846  [12864/60000]\n",
            "loss: 2.126008  [19264/60000]\n",
            "loss: 2.063369  [25664/60000]\n",
            "loss: 2.107019  [32064/60000]\n",
            "loss: 2.096488  [38464/60000]\n",
            "loss: 2.077120  [44864/60000]\n",
            "loss: 2.140578  [51264/60000]\n",
            "loss: 2.097296  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.1%, Avg loss: 2.084786 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.144551  [   64/60000]\n",
            "loss: 2.120833  [ 6464/60000]\n",
            "loss: 2.091421  [12864/60000]\n",
            "loss: 2.097259  [19264/60000]\n",
            "loss: 2.018391  [25664/60000]\n",
            "loss: 2.075335  [32064/60000]\n",
            "loss: 2.051514  [38464/60000]\n",
            "loss: 2.038718  [44864/60000]\n",
            "loss: 2.107949  [51264/60000]\n",
            "loss: 2.054668  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.4%, Avg loss: 2.050146 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.112782  [   64/60000]\n",
            "loss: 2.091801  [ 6464/60000]\n",
            "loss: 2.067779  [12864/60000]\n",
            "loss: 2.068688  [19264/60000]\n",
            "loss: 1.985618  [25664/60000]\n",
            "loss: 2.049434  [32064/60000]\n",
            "loss: 2.011307  [38464/60000]\n",
            "loss: 2.008711  [44864/60000]\n",
            "loss: 2.081987  [51264/60000]\n",
            "loss: 2.017770  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.8%, Avg loss: 2.023088 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.088293  [   64/60000]\n",
            "loss: 2.070848  [ 6464/60000]\n",
            "loss: 2.050030  [12864/60000]\n",
            "loss: 2.046582  [19264/60000]\n",
            "loss: 1.961776  [25664/60000]\n",
            "loss: 2.029543  [32064/60000]\n",
            "loss: 1.982751  [38464/60000]\n",
            "loss: 1.987442  [44864/60000]\n",
            "loss: 2.063988  [51264/60000]\n",
            "loss: 1.992797  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.9%, Avg loss: 2.003807 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.070989  [   64/60000]\n",
            "loss: 2.056121  [ 6464/60000]\n",
            "loss: 2.036831  [12864/60000]\n",
            "loss: 2.030837  [19264/60000]\n",
            "loss: 1.944860  [25664/60000]\n",
            "loss: 2.014544  [32064/60000]\n",
            "loss: 1.963034  [38464/60000]\n",
            "loss: 1.971984  [44864/60000]\n",
            "loss: 2.051328  [51264/60000]\n",
            "loss: 1.975594  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.0%, Avg loss: 1.989770 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.058367  [   64/60000]\n",
            "loss: 2.045169  [ 6464/60000]\n",
            "loss: 2.026681  [12864/60000]\n",
            "loss: 2.019212  [19264/60000]\n",
            "loss: 1.932507  [25664/60000]\n",
            "loss: 2.002914  [32064/60000]\n",
            "loss: 1.948613  [38464/60000]\n",
            "loss: 1.960162  [44864/60000]\n",
            "loss: 2.041777  [51264/60000]\n",
            "loss: 1.962915  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.1%, Avg loss: 1.979059 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 7. -lr을 0.015로\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.015, betas=(0.9, 0.953))\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 0.02일 때와 마찬가지로 초기 accuaracy만 높고 학습할수록 accuaracy의 증가 정도가 0.01일 때에 비해 좋지 않음\n",
        "## 그래도 0.02일 때와 달리 62% 정도까지 상승함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ugJ3Yhw5WOf",
        "outputId": "8fcfa774-988b-4c95-a921-cb89adc0ffa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300702  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-6cb2232c690b>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.295272  [ 6464/60000]\n",
            "loss: 2.294997  [12864/60000]\n",
            "loss: 2.293041  [19264/60000]\n",
            "loss: 2.284528  [25664/60000]\n",
            "loss: 2.284814  [32064/60000]\n",
            "loss: 2.287852  [38464/60000]\n",
            "loss: 2.280040  [44864/60000]\n",
            "loss: 2.282138  [51264/60000]\n",
            "loss: 2.278817  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.6%, Avg loss: 2.271210 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.277457  [   64/60000]\n",
            "loss: 2.267062  [ 6464/60000]\n",
            "loss: 2.259187  [12864/60000]\n",
            "loss: 2.263643  [19264/60000]\n",
            "loss: 2.241386  [25664/60000]\n",
            "loss: 2.238833  [32064/60000]\n",
            "loss: 2.255466  [38464/60000]\n",
            "loss: 2.233873  [44864/60000]\n",
            "loss: 2.251667  [51264/60000]\n",
            "loss: 2.233652  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 39.9%, Avg loss: 2.226557 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.242087  [   64/60000]\n",
            "loss: 2.221360  [ 6464/60000]\n",
            "loss: 2.203871  [12864/60000]\n",
            "loss: 2.223008  [19264/60000]\n",
            "loss: 2.172460  [25664/60000]\n",
            "loss: 2.177943  [32064/60000]\n",
            "loss: 2.213364  [38464/60000]\n",
            "loss: 2.182442  [44864/60000]\n",
            "loss: 2.214511  [51264/60000]\n",
            "loss: 2.184608  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.7%, Avg loss: 2.177882 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.207716  [   64/60000]\n",
            "loss: 2.176863  [ 6464/60000]\n",
            "loss: 2.156691  [12864/60000]\n",
            "loss: 2.177974  [19264/60000]\n",
            "loss: 2.110809  [25664/60000]\n",
            "loss: 2.133512  [32064/60000]\n",
            "loss: 2.171868  [38464/60000]\n",
            "loss: 2.140881  [44864/60000]\n",
            "loss: 2.175772  [51264/60000]\n",
            "loss: 2.135681  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.7%, Avg loss: 2.129240 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.179277  [   64/60000]\n",
            "loss: 2.138004  [ 6464/60000]\n",
            "loss: 2.116288  [12864/60000]\n",
            "loss: 2.133009  [19264/60000]\n",
            "loss: 2.057180  [25664/60000]\n",
            "loss: 2.096465  [32064/60000]\n",
            "loss: 2.134650  [38464/60000]\n",
            "loss: 2.106462  [44864/60000]\n",
            "loss: 2.141673  [51264/60000]\n",
            "loss: 2.093530  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.8%, Avg loss: 2.089819 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.154500  [   64/60000]\n",
            "loss: 2.107234  [ 6464/60000]\n",
            "loss: 2.083737  [12864/60000]\n",
            "loss: 2.099186  [19264/60000]\n",
            "loss: 2.019323  [25664/60000]\n",
            "loss: 2.065474  [32064/60000]\n",
            "loss: 2.106602  [38464/60000]\n",
            "loss: 2.079194  [44864/60000]\n",
            "loss: 2.113737  [51264/60000]\n",
            "loss: 2.060508  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.0%, Avg loss: 2.060059 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.131256  [   64/60000]\n",
            "loss: 2.081358  [ 6464/60000]\n",
            "loss: 2.055814  [12864/60000]\n",
            "loss: 2.071831  [19264/60000]\n",
            "loss: 1.991820  [25664/60000]\n",
            "loss: 2.038509  [32064/60000]\n",
            "loss: 2.079916  [38464/60000]\n",
            "loss: 2.052954  [44864/60000]\n",
            "loss: 2.086057  [51264/60000]\n",
            "loss: 2.027735  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.6%, Avg loss: 2.032937 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.103704  [   64/60000]\n",
            "loss: 2.053727  [ 6464/60000]\n",
            "loss: 2.025873  [12864/60000]\n",
            "loss: 2.044244  [19264/60000]\n",
            "loss: 1.968478  [25664/60000]\n",
            "loss: 2.010075  [32064/60000]\n",
            "loss: 2.046668  [38464/60000]\n",
            "loss: 2.022529  [44864/60000]\n",
            "loss: 2.052580  [51264/60000]\n",
            "loss: 1.989134  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.8%, Avg loss: 2.004336 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.068163  [   64/60000]\n",
            "loss: 2.022665  [ 6464/60000]\n",
            "loss: 1.992299  [12864/60000]\n",
            "loss: 2.016199  [19264/60000]\n",
            "loss: 1.948608  [25664/60000]\n",
            "loss: 1.981366  [32064/60000]\n",
            "loss: 2.013376  [38464/60000]\n",
            "loss: 1.994580  [44864/60000]\n",
            "loss: 2.021148  [51264/60000]\n",
            "loss: 1.955800  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.2%, Avg loss: 1.979703 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.036218  [   64/60000]\n",
            "loss: 1.997258  [ 6464/60000]\n",
            "loss: 1.963793  [12864/60000]\n",
            "loss: 1.993552  [19264/60000]\n",
            "loss: 1.932803  [25664/60000]\n",
            "loss: 1.957888  [32064/60000]\n",
            "loss: 1.988845  [38464/60000]\n",
            "loss: 1.973549  [44864/60000]\n",
            "loss: 1.998101  [51264/60000]\n",
            "loss: 1.932071  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.7%, Avg loss: 1.960716 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 8. -batch size 변경\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=128)\n",
        "test_dataloader = DataLoader(test_data, batch_size=128)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.953))\n",
        "\n",
        "#Define your optimizer\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 좋은 model인 adam 답게 accuaracy가 많이 상승함 최종 단계에서  Accuracy: 65.5%, Avg loss: 1.944865가 도출됨."
      ],
      "metadata": {
        "id": "j9Z81jl15fT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## case 9. 다른 opimizer - RMSprop\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Define your optimizer\n",
        "optimizer =torch.optim.RMSprop(model.parameters(), lr=0.1, alpha=0.9)\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## RMSprop이 첫 epoch에 대해 accuarcy가 36.2%로 꽤 준수하게 나옴\n",
        "## 다른 방법들과 달리 accuarcy가 많이 상승하지 않고 조금만 상승하고 하락하는 부분도 있음"
      ],
      "metadata": {
        "id": "5UlqjZkO6NWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f463d6-671f-41e4-df6e-bfc00cdf8a96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305105  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-b6b98659d31c>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.117322  [ 6464/60000]\n",
            "loss: 1.978526  [12864/60000]\n",
            "loss: 2.134007  [19264/60000]\n",
            "loss: 2.070266  [25664/60000]\n",
            "loss: 2.173953  [32064/60000]\n",
            "loss: 2.132961  [38464/60000]\n",
            "loss: 2.070508  [44864/60000]\n",
            "loss: 2.054830  [51264/60000]\n",
            "loss: 2.179875  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 36.2%, Avg loss: 2.097889 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.023085  [   64/60000]\n",
            "loss: 2.008026  [ 6464/60000]\n",
            "loss: 1.961119  [12864/60000]\n",
            "loss: 2.133013  [19264/60000]\n",
            "loss: 2.130444  [25664/60000]\n",
            "loss: 2.086147  [32064/60000]\n",
            "loss: 2.132799  [38464/60000]\n",
            "loss: 2.070670  [44864/60000]\n",
            "loss: 2.023244  [51264/60000]\n",
            "loss: 2.162285  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.2%, Avg loss: 2.048410 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.898651  [   64/60000]\n",
            "loss: 1.992349  [ 6464/60000]\n",
            "loss: 1.914262  [12864/60000]\n",
            "loss: 2.086124  [19264/60000]\n",
            "loss: 2.054272  [25664/60000]\n",
            "loss: 1.945152  [32064/60000]\n",
            "loss: 2.117397  [38464/60000]\n",
            "loss: 2.054899  [44864/60000]\n",
            "loss: 1.992400  [51264/60000]\n",
            "loss: 2.101109  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.8%, Avg loss: 2.042154 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.957264  [   64/60000]\n",
            "loss: 2.023648  [ 6464/60000]\n",
            "loss: 1.929898  [12864/60000]\n",
            "loss: 2.070519  [19264/60000]\n",
            "loss: 2.039276  [25664/60000]\n",
            "loss: 2.008026  [32064/60000]\n",
            "loss: 2.117394  [38464/60000]\n",
            "loss: 2.039274  [44864/60000]\n",
            "loss: 2.021975  [51264/60000]\n",
            "loss: 2.085607  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.8%, Avg loss: 2.032954 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.929940  [   64/60000]\n",
            "loss: 1.991963  [ 6464/60000]\n",
            "loss: 1.898618  [12864/60000]\n",
            "loss: 2.055102  [19264/60000]\n",
            "loss: 2.038705  [25664/60000]\n",
            "loss: 1.961067  [32064/60000]\n",
            "loss: 2.086082  [38464/60000]\n",
            "loss: 2.039240  [44864/60000]\n",
            "loss: 2.023262  [51264/60000]\n",
            "loss: 2.070376  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.4%, Avg loss: 2.046195 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.976776  [   64/60000]\n",
            "loss: 2.007947  [ 6464/60000]\n",
            "loss: 1.930389  [12864/60000]\n",
            "loss: 2.039276  [19264/60000]\n",
            "loss: 2.023658  [25664/60000]\n",
            "loss: 1.992401  [32064/60000]\n",
            "loss: 2.104111  [38464/60000]\n",
            "loss: 2.039273  [44864/60000]\n",
            "loss: 2.026392  [51264/60000]\n",
            "loss: 2.106781  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 43.5%, Avg loss: 2.026002 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.898651  [   64/60000]\n",
            "loss: 2.000214  [ 6464/60000]\n",
            "loss: 1.898506  [12864/60000]\n",
            "loss: 2.086112  [19264/60000]\n",
            "loss: 2.039232  [25664/60000]\n",
            "loss: 2.007511  [32064/60000]\n",
            "loss: 2.101633  [38464/60000]\n",
            "loss: 2.054472  [44864/60000]\n",
            "loss: 1.991797  [51264/60000]\n",
            "loss: 2.085143  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 2.036286 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.934482  [   64/60000]\n",
            "loss: 2.038005  [ 6464/60000]\n",
            "loss: 1.898251  [12864/60000]\n",
            "loss: 2.054899  [19264/60000]\n",
            "loss: 2.039276  [25664/60000]\n",
            "loss: 1.945526  [32064/60000]\n",
            "loss: 2.132486  [38464/60000]\n",
            "loss: 2.039053  [44864/60000]\n",
            "loss: 1.992401  [51264/60000]\n",
            "loss: 2.086964  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.7%, Avg loss: 2.032926 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.898651  [   64/60000]\n",
            "loss: 2.023625  [ 6464/60000]\n",
            "loss: 1.883024  [12864/60000]\n",
            "loss: 2.086151  [19264/60000]\n",
            "loss: 2.023225  [25664/60000]\n",
            "loss: 1.961151  [32064/60000]\n",
            "loss: 2.101504  [38464/60000]\n",
            "loss: 2.038807  [44864/60000]\n",
            "loss: 2.007963  [51264/60000]\n",
            "loss: 2.054901  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.5%, Avg loss: 2.045287 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.960734  [   64/60000]\n",
            "loss: 1.976752  [ 6464/60000]\n",
            "loss: 1.914276  [12864/60000]\n",
            "loss: 2.101624  [19264/60000]\n",
            "loss: 2.039239  [25664/60000]\n",
            "loss: 1.976766  [32064/60000]\n",
            "loss: 2.101429  [38464/60000]\n",
            "loss: 2.038955  [44864/60000]\n",
            "loss: 2.007994  [51264/60000]\n",
            "loss: 2.070467  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 43.4%, Avg loss: 2.026349 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## case 10. 다른 모델 2\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Define your optimizer\n",
        "optimizer =torch.optim.Adagrad(model.parameters(), lr=0.1, weight_decay=0.01)\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "## 초기 accuarcy는 50% 정도로 매우 높음 그러나 10번째의 accuarcy도 더 이상 상승하지 않음"
      ],
      "metadata": {
        "id": "8ZeYsr9W6OjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cc4999-cfbd-4ef8-aeea-440d741dfd7c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297706  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-128dd45e1aee>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.047168  [ 6464/60000]\n",
            "loss: 1.997652  [12864/60000]\n",
            "loss: 2.029772  [19264/60000]\n",
            "loss: 2.091245  [25664/60000]\n",
            "loss: 2.129770  [32064/60000]\n",
            "loss: 1.954162  [38464/60000]\n",
            "loss: 1.971112  [44864/60000]\n",
            "loss: 1.932868  [51264/60000]\n",
            "loss: 2.001116  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 1.989031 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.921435  [   64/60000]\n",
            "loss: 1.978165  [ 6464/60000]\n",
            "loss: 1.941131  [12864/60000]\n",
            "loss: 2.039953  [19264/60000]\n",
            "loss: 2.004966  [25664/60000]\n",
            "loss: 2.078372  [32064/60000]\n",
            "loss: 1.946797  [38464/60000]\n",
            "loss: 1.970541  [44864/60000]\n",
            "loss: 1.937416  [51264/60000]\n",
            "loss: 2.001656  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.4%, Avg loss: 1.988481 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.922911  [   64/60000]\n",
            "loss: 1.972581  [ 6464/60000]\n",
            "loss: 1.942212  [12864/60000]\n",
            "loss: 2.036058  [19264/60000]\n",
            "loss: 2.002882  [25664/60000]\n",
            "loss: 2.075602  [32064/60000]\n",
            "loss: 1.948562  [38464/60000]\n",
            "loss: 1.970610  [44864/60000]\n",
            "loss: 1.934847  [51264/60000]\n",
            "loss: 2.001884  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.988926 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.923974  [   64/60000]\n",
            "loss: 1.970198  [ 6464/60000]\n",
            "loss: 1.942497  [12864/60000]\n",
            "loss: 2.034606  [19264/60000]\n",
            "loss: 2.002254  [25664/60000]\n",
            "loss: 2.073916  [32064/60000]\n",
            "loss: 1.949159  [38464/60000]\n",
            "loss: 1.970759  [44864/60000]\n",
            "loss: 1.933975  [51264/60000]\n",
            "loss: 2.001916  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.989042 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.924523  [   64/60000]\n",
            "loss: 1.968959  [ 6464/60000]\n",
            "loss: 1.942755  [12864/60000]\n",
            "loss: 2.033860  [19264/60000]\n",
            "loss: 2.001955  [25664/60000]\n",
            "loss: 2.072781  [32064/60000]\n",
            "loss: 1.949343  [38464/60000]\n",
            "loss: 1.970937  [44864/60000]\n",
            "loss: 1.933658  [51264/60000]\n",
            "loss: 2.001886  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.989033 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.924836  [   64/60000]\n",
            "loss: 1.968215  [ 6464/60000]\n",
            "loss: 1.943006  [12864/60000]\n",
            "loss: 2.033425  [19264/60000]\n",
            "loss: 2.001762  [25664/60000]\n",
            "loss: 2.071969  [32064/60000]\n",
            "loss: 1.949368  [38464/60000]\n",
            "loss: 1.971113  [44864/60000]\n",
            "loss: 1.933556  [51264/60000]\n",
            "loss: 2.001833  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.988979 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.925033  [   64/60000]\n",
            "loss: 1.967723  [ 6464/60000]\n",
            "loss: 1.943235  [12864/60000]\n",
            "loss: 2.033149  [19264/60000]\n",
            "loss: 2.001613  [25664/60000]\n",
            "loss: 2.071361  [32064/60000]\n",
            "loss: 1.949328  [38464/60000]\n",
            "loss: 1.971273  [44864/60000]\n",
            "loss: 1.933549  [51264/60000]\n",
            "loss: 2.001774  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.988911 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.925167  [   64/60000]\n",
            "loss: 1.967373  [ 6464/60000]\n",
            "loss: 1.943441  [12864/60000]\n",
            "loss: 2.032964  [19264/60000]\n",
            "loss: 2.001487  [25664/60000]\n",
            "loss: 2.070892  [32064/60000]\n",
            "loss: 1.949265  [38464/60000]\n",
            "loss: 1.971412  [44864/60000]\n",
            "loss: 1.933586  [51264/60000]\n",
            "loss: 2.001711  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.988842 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.925263  [   64/60000]\n",
            "loss: 1.967113  [ 6464/60000]\n",
            "loss: 1.943624  [12864/60000]\n",
            "loss: 2.032837  [19264/60000]\n",
            "loss: 2.001376  [25664/60000]\n",
            "loss: 2.070520  [32064/60000]\n",
            "loss: 1.949194  [38464/60000]\n",
            "loss: 1.971534  [44864/60000]\n",
            "loss: 1.933643  [51264/60000]\n",
            "loss: 2.001647  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.988774 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.925335  [   64/60000]\n",
            "loss: 1.966912  [ 6464/60000]\n",
            "loss: 1.943786  [12864/60000]\n",
            "loss: 2.032746  [19264/60000]\n",
            "loss: 2.001277  [25664/60000]\n",
            "loss: 2.070220  [32064/60000]\n",
            "loss: 1.949124  [38464/60000]\n",
            "loss: 1.971640  [44864/60000]\n",
            "loss: 1.933712  [51264/60000]\n",
            "loss: 2.001583  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.988711 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}