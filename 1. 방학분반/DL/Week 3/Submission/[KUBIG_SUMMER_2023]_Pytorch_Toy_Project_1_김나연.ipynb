{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w3JboQbFpAc"
      },
      "source": [
        "#Pytorch Toy Project\n",
        "\n",
        "---\n",
        "\n",
        "by imjjun(KUBIG 16th)\n",
        "\n",
        "We would learn about:\n",
        "\n",
        "- Tensor\n",
        "\n",
        "- Autograd\n",
        "\n",
        "- Loss\n",
        "\n",
        "- Optimizer\n",
        "\n",
        "- & Overview of Training with Pytorch !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-18T11:43:17.656652Z",
          "start_time": "2023-07-18T11:42:49.781508Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVJS1YwUHDKQ",
        "outputId": "fd101253-d454-4a02-9704-c8279bab31c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-07-18T11:43:21.901Z"
        },
        "id": "N338oaJoKqr_"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37wOnfgNGE_u"
      },
      "source": [
        "##1. Tensor\n",
        "\n",
        "Tensor is the array which is more than 3 dimensions 'originally' But in Pytorch, we use this term on the whole array.\n",
        "\n",
        "Therefore, np.array([1]) is also a tensor in pytorch ! You have to figure out this notion in advance.\n",
        "\n",
        "The rests are very Similiar with Numpy :) Just see what difference it has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-18T11:42:37.391772Z",
          "start_time": "2023-07-18T11:42:37.271740Z"
        },
        "id": "l2yKkJWVFeJ8"
      },
      "outputs": [],
      "source": [
        "#Tensor\n",
        "\n",
        "tensor1=torch.tensor([[[[1.,2.],[3.,4.],[5.,6.],[7.,8.]]]])\n",
        "tensor2=torch.tensor([[[[9.,10.],[11.,12.],[13.,14.],[15.,16.]]]])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1.shape)\n",
        "print(tensor2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EzQDVsgHnpb",
        "outputId": "34619f7d-cfbd-435d-adbd-1da695f63f84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 4, 2])\n",
            "torch.Size([1, 1, 4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAuEGntNJ5qZ",
        "outputId": "b7c6bd4f-babf-4e53-b960-1470b50cf68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[10., 12.],\n",
            "          [14., 16.],\n",
            "          [18., 20.],\n",
            "          [22., 24.]]]])\n",
            "tensor([[[[10., 12.],\n",
            "          [14., 16.],\n",
            "          [18., 20.],\n",
            "          [22., 24.]]]])\n"
          ]
        }
      ],
      "source": [
        "#Add\n",
        "\n",
        "print(tensor1+tensor2)\n",
        "print(torch.add(tensor1,tensor2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## tensor summation reference\n",
        "# https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n",
        "# refer to understand dimension(dim=0 / dim=1)"
      ],
      "metadata": {
        "id": "5f5od80dJ1yW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYz5-9-PJ-HA",
        "outputId": "3aacad16-b1d7-4281-d939-8aa8b4a2a25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.]]]])\n",
            "tensor([[[[-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.],\n",
            "          [-8., -8.]]]])\n"
          ]
        }
      ],
      "source": [
        "#Subtract\n",
        "\n",
        "print(tensor1-tensor2)\n",
        "print(torch.sub(tensor1, tensor2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYFlQ5ixKbcz",
        "outputId": "f167af53-0ff3-40d4-b122-b9db2409c74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[  9.,  20.],\n",
            "          [ 33.,  48.],\n",
            "          [ 65.,  84.],\n",
            "          [105., 128.]]]])\n",
            "tensor([[[[  9.,  20.],\n",
            "          [ 33.,  48.],\n",
            "          [ 65.,  84.],\n",
            "          [105., 128.]]]])\n"
          ]
        }
      ],
      "source": [
        "#Multiplication\n",
        "\n",
        "print(tensor1 * tensor2)\n",
        "print(torch.mul(tensor1,tensor2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s852CHgKopO",
        "outputId": "39e7353e-a38c-4497-d3db-edc993370050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[9.0000, 5.0000],\n",
            "          [3.6667, 3.0000],\n",
            "          [2.6000, 2.3333],\n",
            "          [2.1429, 2.0000]]]])\n",
            "tensor([[[[9.0000, 5.0000],\n",
            "          [3.6667, 3.0000],\n",
            "          [2.6000, 2.3333],\n",
            "          [2.1429, 2.0000]]]])\n"
          ]
        }
      ],
      "source": [
        "#division\n",
        "\n",
        "print(tensor2/tensor1)\n",
        "print(torch.div(tensor2, tensor1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPEWCYEDK50n",
        "outputId": "456f6d21-b80d-4d6e-e2bd-213133f07cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 5])\n"
          ]
        }
      ],
      "source": [
        "#Matrix Multiplication (mul + dot)\n",
        "tensor1 = torch.randn(10, 3, 4)\n",
        "tensor2 = torch.randn(10, 4, 5)\n",
        "print(torch.matmul(tensor1, tensor2).size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L519RDfLEnf"
      },
      "source": [
        "##2. Autograd\n",
        "\n",
        "Backpropagation is 'very' important in DL since it makes the model be closer to the desirable model which we have wished. The detailed content would be covered in session.\n",
        "\n",
        "역전파라는 개념은 굉장히 중요합니다. 모든 모델은 '학습' 과정을 거쳐야하기에 어떠한 방법으로, 어떠한 초매개변수(hyperparameter)를 활용하여 학습할지 결정하고 이 값을 모델의 가중치에 반영해주는 것이 역전파입니다.\n",
        "\n",
        "1. What is CUDA?\n",
        "\n",
        "- Since you know that we use 'GPU' for accelerating the operations of tensors, we have to load the operation to GPU. And CUDA is a Good Tool for GPGPU.\n",
        "\n",
        "2. Autograd?\n",
        "\n",
        "- Thankfully, torch fundamentally contains the gradient operation methods so we don't need to implement it one by one. We call this convenient function 'Autograd'_자동미분\n",
        "\n",
        "- In pytorch, there is a variable, 'require_grad' and it can determine the necessity of gradient when training!\n",
        "\n",
        "3. Batch Size?\n",
        "\n",
        "- Of course, mentioned in course, we could not handle whole of dataset at once, considering the size of dataset(very huge)\n",
        "\n",
        "- So, ideally, calculating whole things at once seems wonderful but we can't. Instead, we separate dataset into 'batch' or 'mini batch'.\n",
        "\n",
        "- Multiplier of 2 is usually used for batch sizing i.e) 2,4,16, 32, 64 ,..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo6hEiJlLEgJ",
        "outputId": "5a394f34-9900-4a36-f75f-d0e5a6523be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#Check that cuda is operating appropriately\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda') #relatively fast\n",
        "else:\n",
        "  device=torch.device('cpu') #only cpu for training & evaluating #Very slow\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xZ5qXT1CQLeT"
      },
      "outputs": [],
      "source": [
        "batch_size=32 # batch size : # of samples that will be propagated through the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoP3vNAgbbQa",
        "outputId": "d9f70439-0e9f-4f2b-a747-32f2b58f6a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.5722,  0.8736,  2.1359,  1.2126], requires_grad=True)\n",
            "tensor([ 0.6999, -0.7314,  0.8749,  0.5686])\n"
          ]
        }
      ],
      "source": [
        "#We can use this variable as below :)\n",
        "x=torch.randn(4 , requires_grad=True)\n",
        "y=torch.randn(4, requires_grad=False)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPtvMb2Jb0SJ"
      },
      "source": [
        "##3 Loss\n",
        "\n",
        "\n",
        "For many tasks on Deep Learning, there are some several loss functions to cover:\n",
        "\n",
        "1. Classification\n",
        "\n",
        "- Cross Entropy Loss: loss between input logits(the layer that feeds into softmax) and target\n",
        "\n",
        "  it is used for classification about Multiple Classes\n",
        "\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        " - Binary Cross Entropy Loss:\n",
        "   \n",
        "   Used to Binary Classification(ex. dog or not)\n",
        "\n",
        " - (Soft) Margin Loss:\n",
        "\n",
        "  which is similar to SVM Loss\n",
        "\n",
        "\n",
        "-  etc,,,\n",
        "\n",
        "\n",
        "2. Regression\n",
        "\n",
        "- MSE loss: squared L2\n",
        "\n",
        "\n",
        "- (Smooth) L1 loss: MAE\n",
        "\n",
        "- etc,,,\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5UhULPYRdZ5p"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#Classification Loss\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "loss=nn.BCELoss()\n",
        "loss=nn.MultiMarginLoss()\n",
        "\n",
        "#Regression Loss\n",
        "loss=nn.MSELoss()\n",
        "loss=nn.L1Loss()\n",
        "loss=nn.SmoothL1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkrlo9kyd0DW"
      },
      "source": [
        "#4 Optimizer\n",
        "\n",
        "\n",
        "In most cases, we use Adam due to its performace but there are still many optimizers that we could use :)\n",
        "\n",
        "\n",
        "- SGD\n",
        "\n",
        "- Adagrad\n",
        "\n",
        "- RMSprop\n",
        "\n",
        "- Adadelta\n",
        "\n",
        "- & Adam !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hrvi1Iqkee7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "da8b80d8-5a8e-4a08-db04-322d43f3b363"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bf70e00a2940>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# important variable is just written on each:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# important variable is just written on each:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "optimizer =torch.optim.Adagrad(model.parameters(), lr=0.1, weight_decay=0.01)\n",
        "optimizer =torch.optim.RMSprop(model.parameters(), lr=0.1, alpha=0.9)\n",
        "optimizer =torch.optim.Adadelta(model.parameters(), lr=0.01, rho=0.95)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.953))\n",
        "\n",
        "\n",
        "optimizer.zero_grad() #Since optimizer cumulates the gradient, we have to make it clear by .zero_grad()\n",
        "loss(model(input), target).backward() #backward() means backpropagation, and in torch, we can implement backpropagation by only one-line code: loss.backward()\n",
        "optimizer.step() #Then, optimizer updates the gradient on weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkrkZQNMf6o8"
      },
      "source": [
        "#5 Just training Very simple model !\n",
        "\n",
        "This code is based on tutorials of pytorch official site.\n",
        "\n",
        "Just see the whole process(details would be handled later) and make your own code to make better model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4VDnUiXgl7e",
        "outputId": "590e8c8d-3152-4bfe-ac40-a38e40f697c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8210448.35it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 143586.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2655351.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5471940.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "#FashionMNIST is a quite famous dataset: Fashion Image Dataset\n",
        "\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sURVPayIhN2z"
      },
      "outputs": [],
      "source": [
        "#You can control the Step Size & batch size & Epochs: How many times do you want to train the model?\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 256\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "B6gGQkNzhbYH"
      },
      "outputs": [],
      "source": [
        "#Define your loss function\n",
        "loss=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Q2ITInVShzr2"
      },
      "outputs": [],
      "source": [
        "#Define your optimizer\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH4MngIpcu1R",
        "outputId": "a114aeb9-6424-440a-c5ce-d1b282ca15e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6egCM5HiU_3"
      },
      "source": [
        "Training Code\n",
        "\n",
        "Just see the process! It contains the basic process of DL :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OUz3Fo0Qh8vR"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train() #Training mode\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X) #forward\n",
        "        loss = loss_fn(pred, y) #compute the loss\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() #zero grad first! Always !\n",
        "        loss.backward() #backprop\n",
        "        optimizer.step() #Update your weights\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59QAoPBYibu4"
      },
      "source": [
        "Test process\n",
        "\n",
        "This process will tell the model's performance on the point of view of 'generalization'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "BkqvXW2IiUHG"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval() #test mode\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0 #define the variable\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "\n",
        "    with torch.no_grad(): #For test mode, testing is just a 'forward' process, not backprop\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches #mean\n",
        "    correct /= size #mean\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X-tla3sjACT"
      },
      "source": [
        "#Toy Project\n",
        "\n",
        "여러분들이 사용하시고 싶은 Optimizer, Loss function을 적극 활용하셔서 Accuracy를 높여보시길 바랍니다!\n",
        "\n",
        "비단 optimizer, loss뿐 아니라, epoch, batch size, 심지어는 model 내부에 들어가는 nn.Linear 차원 변경이나 다른 hidden layer를 쌓아보셔도 됩니다.\n",
        "\n",
        "차원에 대한 고민, 그리고 torch 코드에 대한 고민을 쭉 해보시면서 이번 project에 접근해보시면 좋을 것 같습니다.\n",
        "\n",
        "아마 numpy로 구현하신 loss 값과 accuracy가 비슷하게 나올겁니다,,\n",
        "\n",
        "런타임 유형을 GPU로 놓고 돌리셔야 빨리 되십니다 ! 상기된 cuda.is_available()도 확인해주세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcD6X0Hpi7Be"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "#FashionMNIST is a quite famous dataset: Fashion Image Dataset\n",
        "\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "#You can control the Step Size & batch size & Epochs: How many times do you want to train the model?\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "\n",
        "#Define your loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Define your optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.953))\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "# 84.9%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CqcJJyQYdvi",
        "outputId": "b319468a-ac65-4e75-ef53-1fca5b3892cb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303942  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-fddd726d69d5>:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.891160  [ 6464/60000]\n",
            "loss: 1.703602  [12864/60000]\n",
            "loss: 1.798745  [19264/60000]\n",
            "loss: 1.734306  [25664/60000]\n",
            "loss: 1.715216  [32064/60000]\n",
            "loss: 1.711131  [38464/60000]\n",
            "loss: 1.704012  [44864/60000]\n",
            "loss: 1.681817  [51264/60000]\n",
            "loss: 1.708835  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 1.673170 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.645914  [   64/60000]\n",
            "loss: 1.702087  [ 6464/60000]\n",
            "loss: 1.629381  [12864/60000]\n",
            "loss: 1.718114  [19264/60000]\n",
            "loss: 1.672982  [25664/60000]\n",
            "loss: 1.681881  [32064/60000]\n",
            "loss: 1.669285  [38464/60000]\n",
            "loss: 1.685268  [44864/60000]\n",
            "loss: 1.669910  [51264/60000]\n",
            "loss: 1.682073  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 1.651257 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.623215  [   64/60000]\n",
            "loss: 1.657051  [ 6464/60000]\n",
            "loss: 1.618110  [12864/60000]\n",
            "loss: 1.691344  [19264/60000]\n",
            "loss: 1.656721  [25664/60000]\n",
            "loss: 1.668377  [32064/60000]\n",
            "loss: 1.650350  [38464/60000]\n",
            "loss: 1.677969  [44864/60000]\n",
            "loss: 1.663712  [51264/60000]\n",
            "loss: 1.667193  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 1.641831 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.608248  [   64/60000]\n",
            "loss: 1.639850  [ 6464/60000]\n",
            "loss: 1.613553  [12864/60000]\n",
            "loss: 1.677082  [19264/60000]\n",
            "loss: 1.645589  [25664/60000]\n",
            "loss: 1.662464  [32064/60000]\n",
            "loss: 1.637585  [38464/60000]\n",
            "loss: 1.672734  [44864/60000]\n",
            "loss: 1.659561  [51264/60000]\n",
            "loss: 1.661842  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 1.636517 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.600365  [   64/60000]\n",
            "loss: 1.630481  [ 6464/60000]\n",
            "loss: 1.611038  [12864/60000]\n",
            "loss: 1.669348  [19264/60000]\n",
            "loss: 1.637086  [25664/60000]\n",
            "loss: 1.658081  [32064/60000]\n",
            "loss: 1.627376  [38464/60000]\n",
            "loss: 1.668785  [44864/60000]\n",
            "loss: 1.656247  [51264/60000]\n",
            "loss: 1.658833  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 1.632933 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.595198  [   64/60000]\n",
            "loss: 1.623582  [ 6464/60000]\n",
            "loss: 1.608897  [12864/60000]\n",
            "loss: 1.665171  [19264/60000]\n",
            "loss: 1.630310  [25664/60000]\n",
            "loss: 1.654251  [32064/60000]\n",
            "loss: 1.619791  [38464/60000]\n",
            "loss: 1.665771  [44864/60000]\n",
            "loss: 1.653708  [51264/60000]\n",
            "loss: 1.656299  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 1.630109 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.591482  [   64/60000]\n",
            "loss: 1.617614  [ 6464/60000]\n",
            "loss: 1.606832  [12864/60000]\n",
            "loss: 1.662472  [19264/60000]\n",
            "loss: 1.624501  [25664/60000]\n",
            "loss: 1.651651  [32064/60000]\n",
            "loss: 1.613966  [38464/60000]\n",
            "loss: 1.663681  [44864/60000]\n",
            "loss: 1.652170  [51264/60000]\n",
            "loss: 1.653841  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 1.627876 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.588613  [   64/60000]\n",
            "loss: 1.612359  [ 6464/60000]\n",
            "loss: 1.604955  [12864/60000]\n",
            "loss: 1.660422  [19264/60000]\n",
            "loss: 1.619301  [25664/60000]\n",
            "loss: 1.649663  [32064/60000]\n",
            "loss: 1.609373  [38464/60000]\n",
            "loss: 1.662216  [44864/60000]\n",
            "loss: 1.651390  [51264/60000]\n",
            "loss: 1.651402  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 1.626066 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.586181  [   64/60000]\n",
            "loss: 1.607692  [ 6464/60000]\n",
            "loss: 1.603259  [12864/60000]\n",
            "loss: 1.658886  [19264/60000]\n",
            "loss: 1.614861  [25664/60000]\n",
            "loss: 1.647954  [32064/60000]\n",
            "loss: 1.605760  [38464/60000]\n",
            "loss: 1.661198  [44864/60000]\n",
            "loss: 1.650990  [51264/60000]\n",
            "loss: 1.649007  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 1.624535 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.584059  [   64/60000]\n",
            "loss: 1.603623  [ 6464/60000]\n",
            "loss: 1.601794  [12864/60000]\n",
            "loss: 1.657784  [19264/60000]\n",
            "loss: 1.611020  [25664/60000]\n",
            "loss: 1.646508  [32064/60000]\n",
            "loss: 1.602983  [38464/60000]\n",
            "loss: 1.660465  [44864/60000]\n",
            "loss: 1.650787  [51264/60000]\n",
            "loss: 1.646854  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 1.623233 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.582193  [   64/60000]\n",
            "loss: 1.600163  [ 6464/60000]\n",
            "loss: 1.600668  [12864/60000]\n",
            "loss: 1.657119  [19264/60000]\n",
            "loss: 1.607787  [25664/60000]\n",
            "loss: 1.645288  [32064/60000]\n",
            "loss: 1.600909  [38464/60000]\n",
            "loss: 1.659881  [44864/60000]\n",
            "loss: 1.650551  [51264/60000]\n",
            "loss: 1.645138  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 1.622183 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.580553  [   64/60000]\n",
            "loss: 1.597394  [ 6464/60000]\n",
            "loss: 1.599708  [12864/60000]\n",
            "loss: 1.656700  [19264/60000]\n",
            "loss: 1.604935  [25664/60000]\n",
            "loss: 1.644228  [32064/60000]\n",
            "loss: 1.599152  [38464/60000]\n",
            "loss: 1.659476  [44864/60000]\n",
            "loss: 1.650356  [51264/60000]\n",
            "loss: 1.643748  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 1.621311 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.579176  [   64/60000]\n",
            "loss: 1.595241  [ 6464/60000]\n",
            "loss: 1.598978  [12864/60000]\n",
            "loss: 1.656350  [19264/60000]\n",
            "loss: 1.602241  [25664/60000]\n",
            "loss: 1.643302  [32064/60000]\n",
            "loss: 1.597386  [38464/60000]\n",
            "loss: 1.658998  [44864/60000]\n",
            "loss: 1.650030  [51264/60000]\n",
            "loss: 1.642629  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 1.620565 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.578013  [   64/60000]\n",
            "loss: 1.593519  [ 6464/60000]\n",
            "loss: 1.598390  [12864/60000]\n",
            "loss: 1.656111  [19264/60000]\n",
            "loss: 1.599702  [25664/60000]\n",
            "loss: 1.642430  [32064/60000]\n",
            "loss: 1.595492  [38464/60000]\n",
            "loss: 1.658350  [44864/60000]\n",
            "loss: 1.649458  [51264/60000]\n",
            "loss: 1.641724  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 1.619925 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.576953  [   64/60000]\n",
            "loss: 1.591927  [ 6464/60000]\n",
            "loss: 1.597779  [12864/60000]\n",
            "loss: 1.655840  [19264/60000]\n",
            "loss: 1.597349  [25664/60000]\n",
            "loss: 1.641570  [32064/60000]\n",
            "loss: 1.593557  [38464/60000]\n",
            "loss: 1.657435  [44864/60000]\n",
            "loss: 1.648742  [51264/60000]\n",
            "loss: 1.640943  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 1.619372 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.575909  [   64/60000]\n",
            "loss: 1.590312  [ 6464/60000]\n",
            "loss: 1.597162  [12864/60000]\n",
            "loss: 1.655452  [19264/60000]\n",
            "loss: 1.595278  [25664/60000]\n",
            "loss: 1.640653  [32064/60000]\n",
            "loss: 1.591735  [38464/60000]\n",
            "loss: 1.656388  [44864/60000]\n",
            "loss: 1.648026  [51264/60000]\n",
            "loss: 1.640211  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.618878 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.574845  [   64/60000]\n",
            "loss: 1.588706  [ 6464/60000]\n",
            "loss: 1.596640  [12864/60000]\n",
            "loss: 1.654911  [19264/60000]\n",
            "loss: 1.593613  [25664/60000]\n",
            "loss: 1.639638  [32064/60000]\n",
            "loss: 1.589969  [38464/60000]\n",
            "loss: 1.655381  [44864/60000]\n",
            "loss: 1.647386  [51264/60000]\n",
            "loss: 1.639437  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.618417 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.573771  [   64/60000]\n",
            "loss: 1.587195  [ 6464/60000]\n",
            "loss: 1.596220  [12864/60000]\n",
            "loss: 1.654238  [19264/60000]\n",
            "loss: 1.592267  [25664/60000]\n",
            "loss: 1.638632  [32064/60000]\n",
            "loss: 1.588246  [38464/60000]\n",
            "loss: 1.654418  [44864/60000]\n",
            "loss: 1.646826  [51264/60000]\n",
            "loss: 1.638559  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.617981 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.572702  [   64/60000]\n",
            "loss: 1.585750  [ 6464/60000]\n",
            "loss: 1.595888  [12864/60000]\n",
            "loss: 1.653481  [19264/60000]\n",
            "loss: 1.591047  [25664/60000]\n",
            "loss: 1.637778  [32064/60000]\n",
            "loss: 1.586612  [38464/60000]\n",
            "loss: 1.653503  [44864/60000]\n",
            "loss: 1.646333  [51264/60000]\n",
            "loss: 1.637596  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.617576 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.571687  [   64/60000]\n",
            "loss: 1.584236  [ 6464/60000]\n",
            "loss: 1.595619  [12864/60000]\n",
            "loss: 1.652649  [19264/60000]\n",
            "loss: 1.589854  [25664/60000]\n",
            "loss: 1.637050  [32064/60000]\n",
            "loss: 1.585116  [38464/60000]\n",
            "loss: 1.652640  [44864/60000]\n",
            "loss: 1.645879  [51264/60000]\n",
            "loss: 1.636611  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.617198 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "#FashionMNIST is a quite famous dataset: Fashion Image Dataset\n",
        "\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear=nn.Linear(28*28, 10)\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = SoftmaxClassifier()\n",
        "#You can control the Step Size & batch size & Epochs: How many times do you want to train the model?\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 32\n",
        "\n",
        "#Define your loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Define your optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "# 85.1%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-3MA2B1bnCK",
        "outputId": "ed5b2e34-4fe2-4c29-a841-b3aa62ac7fa5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.305087  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-44501bfb981e>:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.894075  [ 6464/60000]\n",
            "loss: 1.714115  [12864/60000]\n",
            "loss: 1.810669  [19264/60000]\n",
            "loss: 1.740424  [25664/60000]\n",
            "loss: 1.714627  [32064/60000]\n",
            "loss: 1.719003  [38464/60000]\n",
            "loss: 1.709558  [44864/60000]\n",
            "loss: 1.693224  [51264/60000]\n",
            "loss: 1.713854  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 1.680553 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.651262  [   64/60000]\n",
            "loss: 1.708883  [ 6464/60000]\n",
            "loss: 1.631420  [12864/60000]\n",
            "loss: 1.729965  [19264/60000]\n",
            "loss: 1.682954  [25664/60000]\n",
            "loss: 1.690001  [32064/60000]\n",
            "loss: 1.676408  [38464/60000]\n",
            "loss: 1.686532  [44864/60000]\n",
            "loss: 1.678908  [51264/60000]\n",
            "loss: 1.682341  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 1.655854 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.629831  [   64/60000]\n",
            "loss: 1.662647  [ 6464/60000]\n",
            "loss: 1.617158  [12864/60000]\n",
            "loss: 1.703049  [19264/60000]\n",
            "loss: 1.665765  [25664/60000]\n",
            "loss: 1.676503  [32064/60000]\n",
            "loss: 1.654150  [38464/60000]\n",
            "loss: 1.680258  [44864/60000]\n",
            "loss: 1.669248  [51264/60000]\n",
            "loss: 1.667003  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 1.645470 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.613918  [   64/60000]\n",
            "loss: 1.643640  [ 6464/60000]\n",
            "loss: 1.610103  [12864/60000]\n",
            "loss: 1.687970  [19264/60000]\n",
            "loss: 1.652075  [25664/60000]\n",
            "loss: 1.667831  [32064/60000]\n",
            "loss: 1.639076  [38464/60000]\n",
            "loss: 1.674993  [44864/60000]\n",
            "loss: 1.663303  [51264/60000]\n",
            "loss: 1.661630  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 1.639362 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.603717  [   64/60000]\n",
            "loss: 1.633591  [ 6464/60000]\n",
            "loss: 1.605620  [12864/60000]\n",
            "loss: 1.679071  [19264/60000]\n",
            "loss: 1.640882  [25664/60000]\n",
            "loss: 1.660617  [32064/60000]\n",
            "loss: 1.628568  [38464/60000]\n",
            "loss: 1.671052  [44864/60000]\n",
            "loss: 1.659469  [51264/60000]\n",
            "loss: 1.659340  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 1.635146 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.597487  [   64/60000]\n",
            "loss: 1.626244  [ 6464/60000]\n",
            "loss: 1.603902  [12864/60000]\n",
            "loss: 1.674277  [19264/60000]\n",
            "loss: 1.632988  [25664/60000]\n",
            "loss: 1.656085  [32064/60000]\n",
            "loss: 1.620261  [38464/60000]\n",
            "loss: 1.667952  [44864/60000]\n",
            "loss: 1.656529  [51264/60000]\n",
            "loss: 1.657507  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 1.631807 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.592959  [   64/60000]\n",
            "loss: 1.620011  [ 6464/60000]\n",
            "loss: 1.602480  [12864/60000]\n",
            "loss: 1.670910  [19264/60000]\n",
            "loss: 1.626631  [25664/60000]\n",
            "loss: 1.652552  [32064/60000]\n",
            "loss: 1.611615  [38464/60000]\n",
            "loss: 1.666993  [44864/60000]\n",
            "loss: 1.654622  [51264/60000]\n",
            "loss: 1.655412  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 1.629101 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.589111  [   64/60000]\n",
            "loss: 1.613170  [ 6464/60000]\n",
            "loss: 1.601730  [12864/60000]\n",
            "loss: 1.667914  [19264/60000]\n",
            "loss: 1.620910  [25664/60000]\n",
            "loss: 1.650277  [32064/60000]\n",
            "loss: 1.594230  [38464/60000]\n",
            "loss: 1.664634  [44864/60000]\n",
            "loss: 1.653458  [51264/60000]\n",
            "loss: 1.653771  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 1.627099 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.586693  [   64/60000]\n",
            "loss: 1.606642  [ 6464/60000]\n",
            "loss: 1.600839  [12864/60000]\n",
            "loss: 1.666055  [19264/60000]\n",
            "loss: 1.615845  [25664/60000]\n",
            "loss: 1.648425  [32064/60000]\n",
            "loss: 1.589745  [38464/60000]\n",
            "loss: 1.662822  [44864/60000]\n",
            "loss: 1.652931  [51264/60000]\n",
            "loss: 1.652020  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 1.625381 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.585632  [   64/60000]\n",
            "loss: 1.601642  [ 6464/60000]\n",
            "loss: 1.600745  [12864/60000]\n",
            "loss: 1.664905  [19264/60000]\n",
            "loss: 1.611852  [25664/60000]\n",
            "loss: 1.647093  [32064/60000]\n",
            "loss: 1.585975  [38464/60000]\n",
            "loss: 1.661412  [44864/60000]\n",
            "loss: 1.652438  [51264/60000]\n",
            "loss: 1.649908  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.4%, Avg loss: 1.623994 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.584602  [   64/60000]\n",
            "loss: 1.597009  [ 6464/60000]\n",
            "loss: 1.600342  [12864/60000]\n",
            "loss: 1.664198  [19264/60000]\n",
            "loss: 1.608710  [25664/60000]\n",
            "loss: 1.645847  [32064/60000]\n",
            "loss: 1.583085  [38464/60000]\n",
            "loss: 1.660228  [44864/60000]\n",
            "loss: 1.651920  [51264/60000]\n",
            "loss: 1.647671  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 1.622867 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.583725  [   64/60000]\n",
            "loss: 1.592915  [ 6464/60000]\n",
            "loss: 1.599730  [12864/60000]\n",
            "loss: 1.663744  [19264/60000]\n",
            "loss: 1.606117  [25664/60000]\n",
            "loss: 1.644515  [32064/60000]\n",
            "loss: 1.580749  [38464/60000]\n",
            "loss: 1.659240  [44864/60000]\n",
            "loss: 1.651574  [51264/60000]\n",
            "loss: 1.645332  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 1.621934 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.582935  [   64/60000]\n",
            "loss: 1.589382  [ 6464/60000]\n",
            "loss: 1.599034  [12864/60000]\n",
            "loss: 1.663398  [19264/60000]\n",
            "loss: 1.603882  [25664/60000]\n",
            "loss: 1.643108  [32064/60000]\n",
            "loss: 1.578739  [38464/60000]\n",
            "loss: 1.658357  [44864/60000]\n",
            "loss: 1.651369  [51264/60000]\n",
            "loss: 1.642986  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 1.621137 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.582015  [   64/60000]\n",
            "loss: 1.586450  [ 6464/60000]\n",
            "loss: 1.598317  [12864/60000]\n",
            "loss: 1.663088  [19264/60000]\n",
            "loss: 1.601874  [25664/60000]\n",
            "loss: 1.641755  [32064/60000]\n",
            "loss: 1.576945  [38464/60000]\n",
            "loss: 1.657504  [44864/60000]\n",
            "loss: 1.651066  [51264/60000]\n",
            "loss: 1.640734  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 1.620433 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.580834  [   64/60000]\n",
            "loss: 1.584096  [ 6464/60000]\n",
            "loss: 1.597618  [12864/60000]\n",
            "loss: 1.662767  [19264/60000]\n",
            "loss: 1.600016  [25664/60000]\n",
            "loss: 1.640538  [32064/60000]\n",
            "loss: 1.575283  [38464/60000]\n",
            "loss: 1.656672  [44864/60000]\n",
            "loss: 1.650598  [51264/60000]\n",
            "loss: 1.638692  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.619794 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.579592  [   64/60000]\n",
            "loss: 1.582230  [ 6464/60000]\n",
            "loss: 1.596972  [12864/60000]\n",
            "loss: 1.662392  [19264/60000]\n",
            "loss: 1.598283  [25664/60000]\n",
            "loss: 1.639446  [32064/60000]\n",
            "loss: 1.573704  [38464/60000]\n",
            "loss: 1.655849  [44864/60000]\n",
            "loss: 1.650072  [51264/60000]\n",
            "loss: 1.636966  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.619208 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.578502  [   64/60000]\n",
            "loss: 1.580723  [ 6464/60000]\n",
            "loss: 1.596420  [12864/60000]\n",
            "loss: 1.661968  [19264/60000]\n",
            "loss: 1.596669  [25664/60000]\n",
            "loss: 1.638425  [32064/60000]\n",
            "loss: 1.572208  [38464/60000]\n",
            "loss: 1.655000  [44864/60000]\n",
            "loss: 1.649543  [51264/60000]\n",
            "loss: 1.635597  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 1.618675 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.577675  [   64/60000]\n",
            "loss: 1.579460  [ 6464/60000]\n",
            "loss: 1.595982  [12864/60000]\n",
            "loss: 1.661510  [19264/60000]\n",
            "loss: 1.595165  [25664/60000]\n",
            "loss: 1.637423  [32064/60000]\n",
            "loss: 1.570827  [38464/60000]\n",
            "loss: 1.654118  [44864/60000]\n",
            "loss: 1.649048  [51264/60000]\n",
            "loss: 1.634542  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.618197 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.577188  [   64/60000]\n",
            "loss: 1.578368  [ 6464/60000]\n",
            "loss: 1.595650  [12864/60000]\n",
            "loss: 1.660995  [19264/60000]\n",
            "loss: 1.593757  [25664/60000]\n",
            "loss: 1.636445  [32064/60000]\n",
            "loss: 1.569587  [38464/60000]\n",
            "loss: 1.653228  [44864/60000]\n",
            "loss: 1.648604  [51264/60000]\n",
            "loss: 1.633670  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.617776 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.577022  [   64/60000]\n",
            "loss: 1.577398  [ 6464/60000]\n",
            "loss: 1.595390  [12864/60000]\n",
            "loss: 1.660384  [19264/60000]\n",
            "loss: 1.592443  [25664/60000]\n",
            "loss: 1.635519  [32064/60000]\n",
            "loss: 1.568518  [38464/60000]\n",
            "loss: 1.652367  [44864/60000]\n",
            "loss: 1.648189  [51264/60000]\n",
            "loss: 1.632863  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.617407 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.577096  [   64/60000]\n",
            "loss: 1.576513  [ 6464/60000]\n",
            "loss: 1.595167  [12864/60000]\n",
            "loss: 1.659667  [19264/60000]\n",
            "loss: 1.591222  [25664/60000]\n",
            "loss: 1.634672  [32064/60000]\n",
            "loss: 1.567644  [38464/60000]\n",
            "loss: 1.651571  [44864/60000]\n",
            "loss: 1.647777  [51264/60000]\n",
            "loss: 1.632054  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.617078 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.577294  [   64/60000]\n",
            "loss: 1.575703  [ 6464/60000]\n",
            "loss: 1.594959  [12864/60000]\n",
            "loss: 1.658882  [19264/60000]\n",
            "loss: 1.590089  [25664/60000]\n",
            "loss: 1.633940  [32064/60000]\n",
            "loss: 1.566935  [38464/60000]\n",
            "loss: 1.650860  [44864/60000]\n",
            "loss: 1.647366  [51264/60000]\n",
            "loss: 1.631219  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.616763 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.577448  [   64/60000]\n",
            "loss: 1.574994  [ 6464/60000]\n",
            "loss: 1.594755  [12864/60000]\n",
            "loss: 1.658066  [19264/60000]\n",
            "loss: 1.589027  [25664/60000]\n",
            "loss: 1.633347  [32064/60000]\n",
            "loss: 1.566345  [38464/60000]\n",
            "loss: 1.650218  [44864/60000]\n",
            "loss: 1.646968  [51264/60000]\n",
            "loss: 1.630345  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.616443 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.577467  [   64/60000]\n",
            "loss: 1.574416  [ 6464/60000]\n",
            "loss: 1.594559  [12864/60000]\n",
            "loss: 1.657237  [19264/60000]\n",
            "loss: 1.588018  [25664/60000]\n",
            "loss: 1.632902  [32064/60000]\n",
            "loss: 1.565845  [38464/60000]\n",
            "loss: 1.649605  [44864/60000]\n",
            "loss: 1.646594  [51264/60000]\n",
            "loss: 1.629431  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.9%, Avg loss: 1.616135 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.577403  [   64/60000]\n",
            "loss: 1.573949  [ 6464/60000]\n",
            "loss: 1.594389  [12864/60000]\n",
            "loss: 1.656391  [19264/60000]\n",
            "loss: 1.587044  [25664/60000]\n",
            "loss: 1.632588  [32064/60000]\n",
            "loss: 1.565439  [38464/60000]\n",
            "loss: 1.648983  [44864/60000]\n",
            "loss: 1.646238  [51264/60000]\n",
            "loss: 1.628477  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 1.615857 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.577303  [   64/60000]\n",
            "loss: 1.573564  [ 6464/60000]\n",
            "loss: 1.594255  [12864/60000]\n",
            "loss: 1.655528  [19264/60000]\n",
            "loss: 1.586092  [25664/60000]\n",
            "loss: 1.632337  [32064/60000]\n",
            "loss: 1.565144  [38464/60000]\n",
            "loss: 1.648347  [44864/60000]\n",
            "loss: 1.645887  [51264/60000]\n",
            "loss: 1.627476  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 1.615606 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.577143  [   64/60000]\n",
            "loss: 1.573264  [ 6464/60000]\n",
            "loss: 1.594162  [12864/60000]\n",
            "loss: 1.654662  [19264/60000]\n",
            "loss: 1.585155  [25664/60000]\n",
            "loss: 1.632094  [32064/60000]\n",
            "loss: 1.564960  [38464/60000]\n",
            "loss: 1.647716  [44864/60000]\n",
            "loss: 1.645542  [51264/60000]\n",
            "loss: 1.626439  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 1.615380 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.576904  [   64/60000]\n",
            "loss: 1.573037  [ 6464/60000]\n",
            "loss: 1.594106  [12864/60000]\n",
            "loss: 1.653805  [19264/60000]\n",
            "loss: 1.584226  [25664/60000]\n",
            "loss: 1.631841  [32064/60000]\n",
            "loss: 1.564863  [38464/60000]\n",
            "loss: 1.647116  [44864/60000]\n",
            "loss: 1.645207  [51264/60000]\n",
            "loss: 1.625405  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 1.615175 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.576602  [   64/60000]\n",
            "loss: 1.572836  [ 6464/60000]\n",
            "loss: 1.594072  [12864/60000]\n",
            "loss: 1.652965  [19264/60000]\n",
            "loss: 1.583298  [25664/60000]\n",
            "loss: 1.631571  [32064/60000]\n",
            "loss: 1.564827  [38464/60000]\n",
            "loss: 1.646553  [44864/60000]\n",
            "loss: 1.644879  [51264/60000]\n",
            "loss: 1.624426  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 1.614986 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 1.576259  [   64/60000]\n",
            "loss: 1.572632  [ 6464/60000]\n",
            "loss: 1.594045  [12864/60000]\n",
            "loss: 1.652155  [19264/60000]\n",
            "loss: 1.582362  [25664/60000]\n",
            "loss: 1.631288  [32064/60000]\n",
            "loss: 1.564829  [38464/60000]\n",
            "loss: 1.646021  [44864/60000]\n",
            "loss: 1.644554  [51264/60000]\n",
            "loss: 1.623539  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 1.614809 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 1.575890  [   64/60000]\n",
            "loss: 1.572423  [ 6464/60000]\n",
            "loss: 1.594012  [12864/60000]\n",
            "loss: 1.651386  [19264/60000]\n",
            "loss: 1.581405  [25664/60000]\n",
            "loss: 1.631004  [32064/60000]\n",
            "loss: 1.564861  [38464/60000]\n",
            "loss: 1.645511  [44864/60000]\n",
            "loss: 1.644230  [51264/60000]\n",
            "loss: 1.622744  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 1.614639 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 1.575503  [   64/60000]\n",
            "loss: 1.572208  [ 6464/60000]\n",
            "loss: 1.593969  [12864/60000]\n",
            "loss: 1.650656  [19264/60000]\n",
            "loss: 1.580412  [25664/60000]\n",
            "loss: 1.630730  [32064/60000]\n",
            "loss: 1.564920  [38464/60000]\n",
            "loss: 1.645021  [44864/60000]\n",
            "loss: 1.643910  [51264/60000]\n",
            "loss: 1.622020  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.1%, Avg loss: 1.614478 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKgcdWuenpcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}