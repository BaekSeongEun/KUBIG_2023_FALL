{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Za1rZ1yRbmj",
        "outputId": "d5ddbba0-7981-4799-9e24-1039fe145fc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a11c2d1f6f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): device=torch.device('cuda')\n",
        "print(device)\n",
        "# colab 환경 사용해서 runtime 유형 변경\n",
        "#Make sure that your runtime must be 'GPU'\n",
        "#Print out CUDA clearly\n",
        "#If not, this task must be really time-consuming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVC8YiLNRhfr",
        "outputId": "dc200b00-6f67-4596-ab1d-677d0a35eed0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "#This line is for transformation of Image. You don't need to consider about it.\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5),(0.5))])\n",
        "\n",
        "#Train data download\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#Test data download\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "#Make dataloader for iteration when training\n",
        "#would be covered on next toy project\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isxDUkwVR0DY",
        "outputId": "df130fed-aba8-4340-fd34-c73eef386240"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 10263295.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 170011.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3016214.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6118525.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.Sequential\n",
        "\n",
        "class Customnet1(nn.Module):\n",
        "  \"\"\"Define your network by using nn.Sequential. Primarily, this network should be inherited by nn.Module from torch.nn\n",
        "     There are two functions which you have to implement, 1) __init__ & 2) forward\n",
        "\n",
        "     In __init__, you have to pre-define your layers with dimension. For example, you have to decide the kernel size of Convolution layer, or the size of weight of Affine_forward.\n",
        "\n",
        "     In forward, you should pass your input through your network. You can use the network, which is pre-defined in __init__. Or make some creative operations which couldn't be implemented in __init__.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    #Intialize the nn.Module firstly\n",
        "    super().__init__()\n",
        "\n",
        "    #You can contain your layers here\n",
        "    #nn.conv2d(input_channel, output_channel, kernel_size, stride, padding, dilation, etc,,,)\n",
        "    #nn.MaxPool2d(kernel_size, stride, etc,,)\n",
        "    #The details could be searched in pytorch official docs\n",
        "    self.layer=nn.Sequential(\n",
        "                             nn.Conv2d(1,64,(2,2)), #You can control Kernel size, using the tuple like (3,3) or (3,2). The kernel must not be a square.\n",
        "                             nn.MaxPool2d(4), #This kernel size must also be able to become non-symmetric.\n",
        "                             nn.ReLU(),\n",
        "                             nn.Conv2d(64,64,(2,2)),\n",
        "                             nn.MaxPool2d(4),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Flatten(),\n",
        "                             nn.Linear(64, 10), # In_channel, Output Channel\n",
        "                             nn.Softmax())\n",
        "  #As you can see, nn.Sequential is a very comfortable function because that method wraps the whole layers! Just passing through the layer makes us design the network.\n",
        "  #This\n",
        "\n",
        "  #forwad has two variables: (self, x)\n",
        "  # x is the 'input' which is passed from the DataLoader\n",
        "  # This process is fully automatic so that you don't need to consider any other things.\n",
        "  def forward(self, x):\n",
        "\n",
        "    x=self.layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "pMaABY0hR-tc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# line-by-line\n",
        "# Not using the nn.Sequential, you can define whole layers line-by-line\n",
        "\n",
        "class Customnet2(nn.Module):\n",
        "  \"\"\"The main principle is same with above.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    #Define whole layers individually\n",
        "    self.conv1=nn.Conv2d(1,64,(2,2))\n",
        "    self.conv2=nn.Conv2d(64,64,(2,2))\n",
        "    self.maxpool=nn.MaxPool2d(4)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.flatten=nn.Flatten()\n",
        "    self.linear=nn.Linear(64,10)\n",
        "    self.softmax=nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    #forward should be complicated compared to nn.Sequential's one\n",
        "    #You can add some operations which are not included in nn.Module\n",
        "    #For example, if you want to print out the shape of x, you can add the line in this Class.\n",
        "    #But regretfully, since nn.Sequential does not contain the print method,  you cannot print out the shape of x if you use the nn.Sequential\n",
        "\n",
        "    x=self.conv1(x)\n",
        "    x=self.maxpool(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.conv2(x)\n",
        "    x=self.maxpool(x)\n",
        "    x=self.relu(x)\n",
        "\n",
        "    x=self.flatten(x)\n",
        "    x=self.linear(x)\n",
        "    x=self.softmax(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "p2SHalbPSMD-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-4\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "RBz71TXJTHJT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same with pytorch project 1\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "HkmkVhoeTglQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same with pytorch project 1\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "uSyka0PXTr21"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# 문제에서 주어진 조건으로 customnet을 훈련시켰을 때\n",
        "# 첫 accuarcy는 56.6% avg loss는 1.958에서 마지막 epoch에서는 69.7% 1.73까지 상승함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZb7-_5sTwDu",
        "outputId": "8082b0d8-d84f-49c1-ad53-fa7ce7ccadf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.303776  [   64/60000]\n",
            "loss: 2.296282  [ 6464/60000]\n",
            "loss: 2.255524  [12864/60000]\n",
            "loss: 2.214055  [19264/60000]\n",
            "loss: 2.126472  [25664/60000]\n",
            "loss: 2.128938  [32064/60000]\n",
            "loss: 2.067143  [38464/60000]\n",
            "loss: 2.021931  [44864/60000]\n",
            "loss: 2.091132  [51264/60000]\n",
            "loss: 1.997440  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.6%, Avg loss: 1.957711 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.977227  [   64/60000]\n",
            "loss: 1.989034  [ 6464/60000]\n",
            "loss: 1.933161  [12864/60000]\n",
            "loss: 1.971962  [19264/60000]\n",
            "loss: 1.859908  [25664/60000]\n",
            "loss: 1.889012  [32064/60000]\n",
            "loss: 1.885171  [38464/60000]\n",
            "loss: 1.863105  [44864/60000]\n",
            "loss: 1.931976  [51264/60000]\n",
            "loss: 1.846494  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 1.854669 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.848866  [   64/60000]\n",
            "loss: 1.937832  [ 6464/60000]\n",
            "loss: 1.850482  [12864/60000]\n",
            "loss: 1.896395  [19264/60000]\n",
            "loss: 1.799412  [25664/60000]\n",
            "loss: 1.836098  [32064/60000]\n",
            "loss: 1.836629  [38464/60000]\n",
            "loss: 1.835083  [44864/60000]\n",
            "loss: 1.897517  [51264/60000]\n",
            "loss: 1.800185  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 1.820592 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.819770  [   64/60000]\n",
            "loss: 1.912175  [ 6464/60000]\n",
            "loss: 1.821361  [12864/60000]\n",
            "loss: 1.858265  [19264/60000]\n",
            "loss: 1.769237  [25664/60000]\n",
            "loss: 1.815099  [32064/60000]\n",
            "loss: 1.809672  [38464/60000]\n",
            "loss: 1.813156  [44864/60000]\n",
            "loss: 1.877877  [51264/60000]\n",
            "loss: 1.772986  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 1.803066 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.804237  [   64/60000]\n",
            "loss: 1.897668  [ 6464/60000]\n",
            "loss: 1.806962  [12864/60000]\n",
            "loss: 1.845326  [19264/60000]\n",
            "loss: 1.745783  [25664/60000]\n",
            "loss: 1.803281  [32064/60000]\n",
            "loss: 1.788825  [38464/60000]\n",
            "loss: 1.794576  [44864/60000]\n",
            "loss: 1.863179  [51264/60000]\n",
            "loss: 1.758036  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.8%, Avg loss: 1.792149 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.794344  [   64/60000]\n",
            "loss: 1.890261  [ 6464/60000]\n",
            "loss: 1.799368  [12864/60000]\n",
            "loss: 1.835607  [19264/60000]\n",
            "loss: 1.729194  [25664/60000]\n",
            "loss: 1.794406  [32064/60000]\n",
            "loss: 1.772668  [38464/60000]\n",
            "loss: 1.781086  [44864/60000]\n",
            "loss: 1.851494  [51264/60000]\n",
            "loss: 1.750343  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 1.784607 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.787906  [   64/60000]\n",
            "loss: 1.883894  [ 6464/60000]\n",
            "loss: 1.795175  [12864/60000]\n",
            "loss: 1.828339  [19264/60000]\n",
            "loss: 1.718942  [25664/60000]\n",
            "loss: 1.788756  [32064/60000]\n",
            "loss: 1.761635  [38464/60000]\n",
            "loss: 1.770684  [44864/60000]\n",
            "loss: 1.840826  [51264/60000]\n",
            "loss: 1.744556  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 1.779093 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.782477  [   64/60000]\n",
            "loss: 1.878887  [ 6464/60000]\n",
            "loss: 1.793558  [12864/60000]\n",
            "loss: 1.821799  [19264/60000]\n",
            "loss: 1.712205  [25664/60000]\n",
            "loss: 1.783121  [32064/60000]\n",
            "loss: 1.753162  [38464/60000]\n",
            "loss: 1.762565  [44864/60000]\n",
            "loss: 1.831317  [51264/60000]\n",
            "loss: 1.739855  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 1.774808 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.777597  [   64/60000]\n",
            "loss: 1.873887  [ 6464/60000]\n",
            "loss: 1.793072  [12864/60000]\n",
            "loss: 1.816018  [19264/60000]\n",
            "loss: 1.707469  [25664/60000]\n",
            "loss: 1.778827  [32064/60000]\n",
            "loss: 1.746324  [38464/60000]\n",
            "loss: 1.755916  [44864/60000]\n",
            "loss: 1.823259  [51264/60000]\n",
            "loss: 1.736117  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.4%, Avg loss: 1.771282 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.773008  [   64/60000]\n",
            "loss: 1.869217  [ 6464/60000]\n",
            "loss: 1.792892  [12864/60000]\n",
            "loss: 1.811184  [19264/60000]\n",
            "loss: 1.704045  [25664/60000]\n",
            "loss: 1.775147  [32064/60000]\n",
            "loss: 1.740993  [38464/60000]\n",
            "loss: 1.750548  [44864/60000]\n",
            "loss: 1.815944  [51264/60000]\n",
            "loss: 1.732861  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.7%, Avg loss: 1.768316 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model2 - learning rate 감소\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-8\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TnxPMyJQTyPi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# learning rate를 e^-8로 변경 및 train test코드 다시 실행\n",
        "# 첫 accuarcy는 10.0%로 매우 낮고 avg loss는 2,303으로 더 크다\n",
        "#  epoch가 증가해도 accuarcy와 avg loss가 거의 그대로인 걸 확인할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWmRYY6CUtwi",
        "outputId": "b0aef56b-7466-48b5-99cf-f4fbde0462e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306494  [   64/60000]\n",
            "loss: 2.304280  [ 6464/60000]\n",
            "loss: 2.304470  [12864/60000]\n",
            "loss: 2.302670  [19264/60000]\n",
            "loss: 2.303493  [25664/60000]\n",
            "loss: 2.305059  [32064/60000]\n",
            "loss: 2.302422  [38464/60000]\n",
            "loss: 2.304759  [44864/60000]\n",
            "loss: 2.306313  [51264/60000]\n",
            "loss: 2.304266  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303070 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.306481  [   64/60000]\n",
            "loss: 2.304270  [ 6464/60000]\n",
            "loss: 2.304457  [12864/60000]\n",
            "loss: 2.302658  [19264/60000]\n",
            "loss: 2.303478  [25664/60000]\n",
            "loss: 2.305047  [32064/60000]\n",
            "loss: 2.302408  [38464/60000]\n",
            "loss: 2.304748  [44864/60000]\n",
            "loss: 2.306300  [51264/60000]\n",
            "loss: 2.304250  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303056 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.306469  [   64/60000]\n",
            "loss: 2.304260  [ 6464/60000]\n",
            "loss: 2.304445  [12864/60000]\n",
            "loss: 2.302644  [19264/60000]\n",
            "loss: 2.303465  [25664/60000]\n",
            "loss: 2.305036  [32064/60000]\n",
            "loss: 2.302392  [38464/60000]\n",
            "loss: 2.304736  [44864/60000]\n",
            "loss: 2.306287  [51264/60000]\n",
            "loss: 2.304234  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303043 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.306457  [   64/60000]\n",
            "loss: 2.304250  [ 6464/60000]\n",
            "loss: 2.304432  [12864/60000]\n",
            "loss: 2.302631  [19264/60000]\n",
            "loss: 2.303451  [25664/60000]\n",
            "loss: 2.305024  [32064/60000]\n",
            "loss: 2.302378  [38464/60000]\n",
            "loss: 2.304725  [44864/60000]\n",
            "loss: 2.306274  [51264/60000]\n",
            "loss: 2.304219  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303029 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.306444  [   64/60000]\n",
            "loss: 2.304240  [ 6464/60000]\n",
            "loss: 2.304419  [12864/60000]\n",
            "loss: 2.302619  [19264/60000]\n",
            "loss: 2.303437  [25664/60000]\n",
            "loss: 2.305013  [32064/60000]\n",
            "loss: 2.302363  [38464/60000]\n",
            "loss: 2.304714  [44864/60000]\n",
            "loss: 2.306262  [51264/60000]\n",
            "loss: 2.304203  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303015 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.306432  [   64/60000]\n",
            "loss: 2.304229  [ 6464/60000]\n",
            "loss: 2.304406  [12864/60000]\n",
            "loss: 2.302606  [19264/60000]\n",
            "loss: 2.303422  [25664/60000]\n",
            "loss: 2.305001  [32064/60000]\n",
            "loss: 2.302348  [38464/60000]\n",
            "loss: 2.304703  [44864/60000]\n",
            "loss: 2.306249  [51264/60000]\n",
            "loss: 2.304187  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.303002 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.306420  [   64/60000]\n",
            "loss: 2.304219  [ 6464/60000]\n",
            "loss: 2.304393  [12864/60000]\n",
            "loss: 2.302593  [19264/60000]\n",
            "loss: 2.303408  [25664/60000]\n",
            "loss: 2.304990  [32064/60000]\n",
            "loss: 2.302333  [38464/60000]\n",
            "loss: 2.304692  [44864/60000]\n",
            "loss: 2.306237  [51264/60000]\n",
            "loss: 2.304171  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302988 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.306407  [   64/60000]\n",
            "loss: 2.304210  [ 6464/60000]\n",
            "loss: 2.304380  [12864/60000]\n",
            "loss: 2.302580  [19264/60000]\n",
            "loss: 2.303394  [25664/60000]\n",
            "loss: 2.304978  [32064/60000]\n",
            "loss: 2.302318  [38464/60000]\n",
            "loss: 2.304681  [44864/60000]\n",
            "loss: 2.306224  [51264/60000]\n",
            "loss: 2.304155  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302974 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.306395  [   64/60000]\n",
            "loss: 2.304199  [ 6464/60000]\n",
            "loss: 2.304368  [12864/60000]\n",
            "loss: 2.302567  [19264/60000]\n",
            "loss: 2.303380  [25664/60000]\n",
            "loss: 2.304967  [32064/60000]\n",
            "loss: 2.302304  [38464/60000]\n",
            "loss: 2.304670  [44864/60000]\n",
            "loss: 2.306211  [51264/60000]\n",
            "loss: 2.304140  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302961 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.306383  [   64/60000]\n",
            "loss: 2.304189  [ 6464/60000]\n",
            "loss: 2.304355  [12864/60000]\n",
            "loss: 2.302554  [19264/60000]\n",
            "loss: 2.303365  [25664/60000]\n",
            "loss: 2.304956  [32064/60000]\n",
            "loss: 2.302289  [38464/60000]\n",
            "loss: 2.304659  [44864/60000]\n",
            "loss: 2.306199  [51264/60000]\n",
            "loss: 2.304124  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.302947 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model3 - learning rate 증가\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dR-El5kQUz6g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# learning rate를 e^-3로 변경 및 train test코드 다시 실행\n",
        "# 첫 epoch가 accuarcy 75.7% avg loss도 1.71로 꽤 좋은 값을 나타냄\n",
        "# epoch가 증가할 때 accuarcy는 조금씩 증가하여 81.1%까지 상승하였으며 avg loss는 1.65에서 더 이상 감소하지 않음\n",
        "# 추후 분석 시도 이 learning rate 활용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJfZgcimV6NS",
        "outputId": "95d35862-226e-485d-c071-dbdfc15570db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304287  [   64/60000]\n",
            "loss: 2.123026  [ 6464/60000]\n",
            "loss: 1.865740  [12864/60000]\n",
            "loss: 1.933408  [19264/60000]\n",
            "loss: 1.764824  [25664/60000]\n",
            "loss: 1.719012  [32064/60000]\n",
            "loss: 1.745906  [38464/60000]\n",
            "loss: 1.723997  [44864/60000]\n",
            "loss: 1.742696  [51264/60000]\n",
            "loss: 1.712629  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 1.714678 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.667711  [   64/60000]\n",
            "loss: 1.746578  [ 6464/60000]\n",
            "loss: 1.650918  [12864/60000]\n",
            "loss: 1.736564  [19264/60000]\n",
            "loss: 1.680282  [25664/60000]\n",
            "loss: 1.702725  [32064/60000]\n",
            "loss: 1.692104  [38464/60000]\n",
            "loss: 1.639371  [44864/60000]\n",
            "loss: 1.671891  [51264/60000]\n",
            "loss: 1.675994  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 1.683269 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.634984  [   64/60000]\n",
            "loss: 1.695371  [ 6464/60000]\n",
            "loss: 1.625947  [12864/60000]\n",
            "loss: 1.719544  [19264/60000]\n",
            "loss: 1.655523  [25664/60000]\n",
            "loss: 1.689841  [32064/60000]\n",
            "loss: 1.664420  [38464/60000]\n",
            "loss: 1.621344  [44864/60000]\n",
            "loss: 1.667566  [51264/60000]\n",
            "loss: 1.665516  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 1.671913 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.622619  [   64/60000]\n",
            "loss: 1.671236  [ 6464/60000]\n",
            "loss: 1.608637  [12864/60000]\n",
            "loss: 1.712096  [19264/60000]\n",
            "loss: 1.639006  [25664/60000]\n",
            "loss: 1.679537  [32064/60000]\n",
            "loss: 1.657373  [38464/60000]\n",
            "loss: 1.614806  [44864/60000]\n",
            "loss: 1.650926  [51264/60000]\n",
            "loss: 1.650301  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 1.662096 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.616043  [   64/60000]\n",
            "loss: 1.654787  [ 6464/60000]\n",
            "loss: 1.599956  [12864/60000]\n",
            "loss: 1.708131  [19264/60000]\n",
            "loss: 1.639094  [25664/60000]\n",
            "loss: 1.673104  [32064/60000]\n",
            "loss: 1.655144  [38464/60000]\n",
            "loss: 1.599738  [44864/60000]\n",
            "loss: 1.648744  [51264/60000]\n",
            "loss: 1.641141  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 1.657884 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.605458  [   64/60000]\n",
            "loss: 1.630847  [ 6464/60000]\n",
            "loss: 1.590744  [12864/60000]\n",
            "loss: 1.707138  [19264/60000]\n",
            "loss: 1.644043  [25664/60000]\n",
            "loss: 1.657161  [32064/60000]\n",
            "loss: 1.660939  [38464/60000]\n",
            "loss: 1.599345  [44864/60000]\n",
            "loss: 1.648387  [51264/60000]\n",
            "loss: 1.637744  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 1.660536 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.593181  [   64/60000]\n",
            "loss: 1.618271  [ 6464/60000]\n",
            "loss: 1.576635  [12864/60000]\n",
            "loss: 1.697204  [19264/60000]\n",
            "loss: 1.638557  [25664/60000]\n",
            "loss: 1.651698  [32064/60000]\n",
            "loss: 1.658104  [38464/60000]\n",
            "loss: 1.594910  [44864/60000]\n",
            "loss: 1.649381  [51264/60000]\n",
            "loss: 1.634523  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 1.657167 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.587580  [   64/60000]\n",
            "loss: 1.610103  [ 6464/60000]\n",
            "loss: 1.569897  [12864/60000]\n",
            "loss: 1.689910  [19264/60000]\n",
            "loss: 1.632774  [25664/60000]\n",
            "loss: 1.645872  [32064/60000]\n",
            "loss: 1.651086  [38464/60000]\n",
            "loss: 1.590008  [44864/60000]\n",
            "loss: 1.647236  [51264/60000]\n",
            "loss: 1.630517  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 1.656607 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.589170  [   64/60000]\n",
            "loss: 1.594150  [ 6464/60000]\n",
            "loss: 1.574054  [12864/60000]\n",
            "loss: 1.687812  [19264/60000]\n",
            "loss: 1.635162  [25664/60000]\n",
            "loss: 1.645933  [32064/60000]\n",
            "loss: 1.648819  [38464/60000]\n",
            "loss: 1.591753  [44864/60000]\n",
            "loss: 1.642933  [51264/60000]\n",
            "loss: 1.629818  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 1.654200 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.579198  [   64/60000]\n",
            "loss: 1.589264  [ 6464/60000]\n",
            "loss: 1.561613  [12864/60000]\n",
            "loss: 1.683512  [19264/60000]\n",
            "loss: 1.632248  [25664/60000]\n",
            "loss: 1.643724  [32064/60000]\n",
            "loss: 1.642868  [38464/60000]\n",
            "loss: 1.598246  [44864/60000]\n",
            "loss: 1.628982  [51264/60000]\n",
            "loss: 1.618277  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 1.651876 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model4 - loss function 변경(multi margin)\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.MultiMarginLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "saw12p_ZXONg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "# loss function을 multimargin으로 변경 후 분석 실시\n",
        "# accuarcy는 이전 case와 비슷한데 avg loss가 초기값부터 0.294로 굉장히 낮음\n",
        "# avg loss는 낮지만 최종 epoch에서의 accuarcy는 77% 정도로 오히려 낮음\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH_x3p_DX4Yv",
        "outputId": "3181d6a3-8dba-419c-ca70-7c166fc580a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.898860  [   64/60000]\n",
            "loss: 0.651965  [ 6464/60000]\n",
            "loss: 0.467618  [12864/60000]\n",
            "loss: 0.450539  [19264/60000]\n",
            "loss: 0.310964  [25664/60000]\n",
            "loss: 0.339141  [32064/60000]\n",
            "loss: 0.281062  [38464/60000]\n",
            "loss: 0.231473  [44864/60000]\n",
            "loss: 0.296274  [51264/60000]\n",
            "loss: 0.331300  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.293695 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.232056  [   64/60000]\n",
            "loss: 0.269252  [ 6464/60000]\n",
            "loss: 0.235746  [12864/60000]\n",
            "loss: 0.328808  [19264/60000]\n",
            "loss: 0.238930  [25664/60000]\n",
            "loss: 0.340291  [32064/60000]\n",
            "loss: 0.234024  [38464/60000]\n",
            "loss: 0.186111  [44864/60000]\n",
            "loss: 0.224922  [51264/60000]\n",
            "loss: 0.297970  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.1%, Avg loss: 0.259880 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.186246  [   64/60000]\n",
            "loss: 0.249681  [ 6464/60000]\n",
            "loss: 0.219208  [12864/60000]\n",
            "loss: 0.325037  [19264/60000]\n",
            "loss: 0.221319  [25664/60000]\n",
            "loss: 0.327434  [32064/60000]\n",
            "loss: 0.226967  [38464/60000]\n",
            "loss: 0.184233  [44864/60000]\n",
            "loss: 0.207068  [51264/60000]\n",
            "loss: 0.293620  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.250558 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.181856  [   64/60000]\n",
            "loss: 0.235553  [ 6464/60000]\n",
            "loss: 0.215854  [12864/60000]\n",
            "loss: 0.325612  [19264/60000]\n",
            "loss: 0.208299  [25664/60000]\n",
            "loss: 0.321575  [32064/60000]\n",
            "loss: 0.226472  [38464/60000]\n",
            "loss: 0.181709  [44864/60000]\n",
            "loss: 0.197807  [51264/60000]\n",
            "loss: 0.284711  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.1%, Avg loss: 0.245269 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.178235  [   64/60000]\n",
            "loss: 0.214329  [ 6464/60000]\n",
            "loss: 0.219361  [12864/60000]\n",
            "loss: 0.318462  [19264/60000]\n",
            "loss: 0.194923  [25664/60000]\n",
            "loss: 0.298844  [32064/60000]\n",
            "loss: 0.227998  [38464/60000]\n",
            "loss: 0.183672  [44864/60000]\n",
            "loss: 0.181614  [51264/60000]\n",
            "loss: 0.287656  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.239920 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.173636  [   64/60000]\n",
            "loss: 0.183406  [ 6464/60000]\n",
            "loss: 0.211075  [12864/60000]\n",
            "loss: 0.312938  [19264/60000]\n",
            "loss: 0.185481  [25664/60000]\n",
            "loss: 0.289628  [32064/60000]\n",
            "loss: 0.215216  [38464/60000]\n",
            "loss: 0.185064  [44864/60000]\n",
            "loss: 0.171190  [51264/60000]\n",
            "loss: 0.282969  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.234522 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.182462  [   64/60000]\n",
            "loss: 0.171506  [ 6464/60000]\n",
            "loss: 0.207275  [12864/60000]\n",
            "loss: 0.312953  [19264/60000]\n",
            "loss: 0.176670  [25664/60000]\n",
            "loss: 0.281053  [32064/60000]\n",
            "loss: 0.205036  [38464/60000]\n",
            "loss: 0.184130  [44864/60000]\n",
            "loss: 0.174418  [51264/60000]\n",
            "loss: 0.272366  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.231109 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.170406  [   64/60000]\n",
            "loss: 0.160790  [ 6464/60000]\n",
            "loss: 0.207299  [12864/60000]\n",
            "loss: 0.307954  [19264/60000]\n",
            "loss: 0.168536  [25664/60000]\n",
            "loss: 0.278447  [32064/60000]\n",
            "loss: 0.198852  [38464/60000]\n",
            "loss: 0.173169  [44864/60000]\n",
            "loss: 0.176383  [51264/60000]\n",
            "loss: 0.276230  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.233445 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.173802  [   64/60000]\n",
            "loss: 0.153894  [ 6464/60000]\n",
            "loss: 0.209351  [12864/60000]\n",
            "loss: 0.305197  [19264/60000]\n",
            "loss: 0.161432  [25664/60000]\n",
            "loss: 0.276633  [32064/60000]\n",
            "loss: 0.199906  [38464/60000]\n",
            "loss: 0.171772  [44864/60000]\n",
            "loss: 0.178410  [51264/60000]\n",
            "loss: 0.258898  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.228218 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.166713  [   64/60000]\n",
            "loss: 0.142403  [ 6464/60000]\n",
            "loss: 0.211526  [12864/60000]\n",
            "loss: 0.300819  [19264/60000]\n",
            "loss: 0.156164  [25664/60000]\n",
            "loss: 0.277605  [32064/60000]\n",
            "loss: 0.200150  [38464/60000]\n",
            "loss: 0.168413  [44864/60000]\n",
            "loss: 0.180104  [51264/60000]\n",
            "loss: 0.260939  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.229028 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model5 - optimizer 변경(Adagard)\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer =torch.optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=0.01)\n"
      ],
      "metadata": {
        "id": "X6lPitazXO6B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "\n",
        "# optimizer를 adagard로 변경 후 weight deacy는 0.01로 설정 후 학습\n",
        "# learning rate가 e^-3으로 높은데도 초기 accuarcy가 35.6으로 꽤 낮음 avg loss는 2.19임\n",
        "# 학습을 해도 accuarcy가 42% 선에서 멈추고 avg loss 또한 2.05정도로 굉장히 천천히 작아짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_hq3tv6Y_NC",
        "outputId": "6e49d364-d540-4a32-a9c1-a3df4ad76975"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.307628  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.290865  [ 6464/60000]\n",
            "loss: 2.269075  [12864/60000]\n",
            "loss: 2.250744  [19264/60000]\n",
            "loss: 2.231396  [25664/60000]\n",
            "loss: 2.266430  [32064/60000]\n",
            "loss: 2.218637  [38464/60000]\n",
            "loss: 2.225161  [44864/60000]\n",
            "loss: 2.210211  [51264/60000]\n",
            "loss: 2.188641  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 35.6%, Avg loss: 2.192800 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.196078  [   64/60000]\n",
            "loss: 2.205303  [ 6464/60000]\n",
            "loss: 2.184271  [12864/60000]\n",
            "loss: 2.177668  [19264/60000]\n",
            "loss: 2.156433  [25664/60000]\n",
            "loss: 2.179735  [32064/60000]\n",
            "loss: 2.146525  [38464/60000]\n",
            "loss: 2.146695  [44864/60000]\n",
            "loss: 2.159908  [51264/60000]\n",
            "loss: 2.102440  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.5%, Avg loss: 2.130341 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.150289  [   64/60000]\n",
            "loss: 2.151695  [ 6464/60000]\n",
            "loss: 2.133396  [12864/60000]\n",
            "loss: 2.140352  [19264/60000]\n",
            "loss: 2.104897  [25664/60000]\n",
            "loss: 2.126768  [32064/60000]\n",
            "loss: 2.109979  [38464/60000]\n",
            "loss: 2.106456  [44864/60000]\n",
            "loss: 2.137197  [51264/60000]\n",
            "loss: 2.063519  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.9%, Avg loss: 2.101226 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.127149  [   64/60000]\n",
            "loss: 2.127208  [ 6464/60000]\n",
            "loss: 2.108478  [12864/60000]\n",
            "loss: 2.121791  [19264/60000]\n",
            "loss: 2.080442  [25664/60000]\n",
            "loss: 2.102968  [32064/60000]\n",
            "loss: 2.092473  [38464/60000]\n",
            "loss: 2.088637  [44864/60000]\n",
            "loss: 2.126422  [51264/60000]\n",
            "loss: 2.045131  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.1%, Avg loss: 2.087088 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.114587  [   64/60000]\n",
            "loss: 2.115164  [ 6464/60000]\n",
            "loss: 2.095407  [12864/60000]\n",
            "loss: 2.110986  [19264/60000]\n",
            "loss: 2.067569  [25664/60000]\n",
            "loss: 2.090368  [32064/60000]\n",
            "loss: 2.082455  [38464/60000]\n",
            "loss: 2.078957  [44864/60000]\n",
            "loss: 2.120146  [51264/60000]\n",
            "loss: 2.034862  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.2%, Avg loss: 2.078827 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.106380  [   64/60000]\n",
            "loss: 2.107970  [ 6464/60000]\n",
            "loss: 2.087481  [12864/60000]\n",
            "loss: 2.104005  [19264/60000]\n",
            "loss: 2.059598  [25664/60000]\n",
            "loss: 2.082515  [32064/60000]\n",
            "loss: 2.075773  [38464/60000]\n",
            "loss: 2.072602  [44864/60000]\n",
            "loss: 2.115672  [51264/60000]\n",
            "loss: 2.028395  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.3%, Avg loss: 2.073166 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.100181  [   64/60000]\n",
            "loss: 2.102743  [ 6464/60000]\n",
            "loss: 2.082089  [12864/60000]\n",
            "loss: 2.099094  [19264/60000]\n",
            "loss: 2.053670  [25664/60000]\n",
            "loss: 2.076923  [32064/60000]\n",
            "loss: 2.070559  [38464/60000]\n",
            "loss: 2.067611  [44864/60000]\n",
            "loss: 2.111741  [51264/60000]\n",
            "loss: 2.023845  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 2.068552 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.094468  [   64/60000]\n",
            "loss: 2.098060  [ 6464/60000]\n",
            "loss: 2.077813  [12864/60000]\n",
            "loss: 2.095496  [19264/60000]\n",
            "loss: 2.048142  [25664/60000]\n",
            "loss: 2.071985  [32064/60000]\n",
            "loss: 2.065640  [38464/60000]\n",
            "loss: 2.062817  [44864/60000]\n",
            "loss: 2.107280  [51264/60000]\n",
            "loss: 2.020303  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 2.064029 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.088185  [   64/60000]\n",
            "loss: 2.092977  [ 6464/60000]\n",
            "loss: 2.074105  [12864/60000]\n",
            "loss: 2.093049  [19264/60000]\n",
            "loss: 2.042250  [25664/60000]\n",
            "loss: 2.067179  [32064/60000]\n",
            "loss: 2.060249  [38464/60000]\n",
            "loss: 2.057557  [44864/60000]\n",
            "loss: 2.101695  [51264/60000]\n",
            "loss: 2.017334  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 2.059194 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.080801  [   64/60000]\n",
            "loss: 2.086943  [ 6464/60000]\n",
            "loss: 2.070757  [12864/60000]\n",
            "loss: 2.091556  [19264/60000]\n",
            "loss: 2.035598  [25664/60000]\n",
            "loss: 2.062143  [32064/60000]\n",
            "loss: 2.053797  [38464/60000]\n",
            "loss: 2.051542  [44864/60000]\n",
            "loss: 2.094968  [51264/60000]\n",
            "loss: 2.014063  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 43.2%, Avg loss: 2.053819 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model6 - optimizer 변경(RMSprop)\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer =torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.9)"
      ],
      "metadata": {
        "id": "iaUfkHh4Z2y_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# RMSprop으로 변경 시 첫 accuaracy가 75.4% avg loss가 1.716으로 굉장히 낮음\n",
        "# 초기 예상은 ADAM이 RMSprop에 비해 accuarcy가 많이 높을 것이라 생각했는데 본 학습에서는 비슷한 성능을 보임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tac3Gbn7Z21M",
        "outputId": "79469528-d5c4-4ced-ca86-9c20c6ef1c0b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296600  [   64/60000]\n",
            "loss: 2.062076  [ 6464/60000]\n",
            "loss: 1.850274  [12864/60000]\n",
            "loss: 1.874274  [19264/60000]\n",
            "loss: 1.811322  [25664/60000]\n",
            "loss: 1.731295  [32064/60000]\n",
            "loss: 1.777835  [38464/60000]\n",
            "loss: 1.704476  [44864/60000]\n",
            "loss: 1.739373  [51264/60000]\n",
            "loss: 1.702033  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 1.716103 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.688532  [   64/60000]\n",
            "loss: 1.723668  [ 6464/60000]\n",
            "loss: 1.647938  [12864/60000]\n",
            "loss: 1.743416  [19264/60000]\n",
            "loss: 1.661476  [25664/60000]\n",
            "loss: 1.703732  [32064/60000]\n",
            "loss: 1.713250  [38464/60000]\n",
            "loss: 1.645413  [44864/60000]\n",
            "loss: 1.694991  [51264/60000]\n",
            "loss: 1.682664  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.0%, Avg loss: 1.699213 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.651886  [   64/60000]\n",
            "loss: 1.658438  [ 6464/60000]\n",
            "loss: 1.630722  [12864/60000]\n",
            "loss: 1.716839  [19264/60000]\n",
            "loss: 1.649394  [25664/60000]\n",
            "loss: 1.695916  [32064/60000]\n",
            "loss: 1.669011  [38464/60000]\n",
            "loss: 1.629928  [44864/60000]\n",
            "loss: 1.679725  [51264/60000]\n",
            "loss: 1.668866  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 1.692183 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.633741  [   64/60000]\n",
            "loss: 1.645532  [ 6464/60000]\n",
            "loss: 1.637838  [12864/60000]\n",
            "loss: 1.710078  [19264/60000]\n",
            "loss: 1.648155  [25664/60000]\n",
            "loss: 1.705888  [32064/60000]\n",
            "loss: 1.650693  [38464/60000]\n",
            "loss: 1.620857  [44864/60000]\n",
            "loss: 1.662042  [51264/60000]\n",
            "loss: 1.656828  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 1.675510 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.623355  [   64/60000]\n",
            "loss: 1.645290  [ 6464/60000]\n",
            "loss: 1.636290  [12864/60000]\n",
            "loss: 1.708664  [19264/60000]\n",
            "loss: 1.646074  [25664/60000]\n",
            "loss: 1.716050  [32064/60000]\n",
            "loss: 1.637253  [38464/60000]\n",
            "loss: 1.615201  [44864/60000]\n",
            "loss: 1.653442  [51264/60000]\n",
            "loss: 1.648879  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 1.670611 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.605493  [   64/60000]\n",
            "loss: 1.646572  [ 6464/60000]\n",
            "loss: 1.622180  [12864/60000]\n",
            "loss: 1.705696  [19264/60000]\n",
            "loss: 1.642321  [25664/60000]\n",
            "loss: 1.701868  [32064/60000]\n",
            "loss: 1.613570  [38464/60000]\n",
            "loss: 1.610280  [44864/60000]\n",
            "loss: 1.631555  [51264/60000]\n",
            "loss: 1.641741  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.663887 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.600277  [   64/60000]\n",
            "loss: 1.651175  [ 6464/60000]\n",
            "loss: 1.613490  [12864/60000]\n",
            "loss: 1.702432  [19264/60000]\n",
            "loss: 1.637674  [25664/60000]\n",
            "loss: 1.697163  [32064/60000]\n",
            "loss: 1.601818  [38464/60000]\n",
            "loss: 1.603425  [44864/60000]\n",
            "loss: 1.622910  [51264/60000]\n",
            "loss: 1.637677  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 1.653120 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.602866  [   64/60000]\n",
            "loss: 1.642102  [ 6464/60000]\n",
            "loss: 1.624345  [12864/60000]\n",
            "loss: 1.697376  [19264/60000]\n",
            "loss: 1.630718  [25664/60000]\n",
            "loss: 1.693063  [32064/60000]\n",
            "loss: 1.592805  [38464/60000]\n",
            "loss: 1.606318  [44864/60000]\n",
            "loss: 1.616473  [51264/60000]\n",
            "loss: 1.645160  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 1.651790 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.597083  [   64/60000]\n",
            "loss: 1.631598  [ 6464/60000]\n",
            "loss: 1.630365  [12864/60000]\n",
            "loss: 1.695889  [19264/60000]\n",
            "loss: 1.627428  [25664/60000]\n",
            "loss: 1.675638  [32064/60000]\n",
            "loss: 1.593639  [38464/60000]\n",
            "loss: 1.605892  [44864/60000]\n",
            "loss: 1.609014  [51264/60000]\n",
            "loss: 1.631778  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 1.649193 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.593794  [   64/60000]\n",
            "loss: 1.628147  [ 6464/60000]\n",
            "loss: 1.618600  [12864/60000]\n",
            "loss: 1.665339  [19264/60000]\n",
            "loss: 1.616757  [25664/60000]\n",
            "loss: 1.666799  [32064/60000]\n",
            "loss: 1.594935  [38464/60000]\n",
            "loss: 1.607626  [44864/60000]\n",
            "loss: 1.605416  [51264/60000]\n",
            "loss: 1.632925  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 1.641243 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet1 model7 - optimizer 변경(SGD)\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer =torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ],
      "metadata": {
        "id": "Mn5ldoWWaBgP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# 또 다른 optimizer인 sgd로 학습할 때는 첫 epoch에서의 accuarcy는 24.5%인데 두 번째 accuarcy는 12%대로 낮아짐\n",
        "# 다시 accuarcy가 상승하여 마지막 apoch에는 50%를 넘겨 오히려 Adagrad보다 높은 것을 알 수 있음 AVG loss도 점점 작아져 2보다 낮아짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwxiGrq6aBkZ",
        "outputId": "656b329b-6725-4d73-e858-f1cab7ff661d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297557  [   64/60000]\n",
            "loss: 2.298979  [ 6464/60000]\n",
            "loss: 2.298034  [12864/60000]\n",
            "loss: 2.301540  [19264/60000]\n",
            "loss: 2.300325  [25664/60000]\n",
            "loss: 2.298663  [32064/60000]\n",
            "loss: 2.297675  [38464/60000]\n",
            "loss: 2.297965  [44864/60000]\n",
            "loss: 2.293558  [51264/60000]\n",
            "loss: 2.297538  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 24.5%, Avg loss: 2.294572 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.288501  [   64/60000]\n",
            "loss: 2.292382  [ 6464/60000]\n",
            "loss: 2.289253  [12864/60000]\n",
            "loss: 2.293307  [19264/60000]\n",
            "loss: 2.287500  [25664/60000]\n",
            "loss: 2.292384  [32064/60000]\n",
            "loss: 2.286513  [38464/60000]\n",
            "loss: 2.288982  [44864/60000]\n",
            "loss: 2.276459  [51264/60000]\n",
            "loss: 2.289790  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 12.7%, Avg loss: 2.278842 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.264313  [   64/60000]\n",
            "loss: 2.275727  [ 6464/60000]\n",
            "loss: 2.265461  [12864/60000]\n",
            "loss: 2.274645  [19264/60000]\n",
            "loss: 2.254979  [25664/60000]\n",
            "loss: 2.278110  [32064/60000]\n",
            "loss: 2.256521  [38464/60000]\n",
            "loss: 2.260255  [44864/60000]\n",
            "loss: 2.233426  [51264/60000]\n",
            "loss: 2.257275  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 19.5%, Avg loss: 2.231154 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.210887  [   64/60000]\n",
            "loss: 2.235243  [ 6464/60000]\n",
            "loss: 2.209958  [12864/60000]\n",
            "loss: 2.215520  [19264/60000]\n",
            "loss: 2.185065  [25664/60000]\n",
            "loss: 2.222101  [32064/60000]\n",
            "loss: 2.184298  [38464/60000]\n",
            "loss: 2.179646  [44864/60000]\n",
            "loss: 2.177658  [51264/60000]\n",
            "loss: 2.178121  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 32.2%, Avg loss: 2.153945 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.147711  [   64/60000]\n",
            "loss: 2.167739  [ 6464/60000]\n",
            "loss: 2.129603  [12864/60000]\n",
            "loss: 2.151793  [19264/60000]\n",
            "loss: 2.105766  [25664/60000]\n",
            "loss: 2.129075  [32064/60000]\n",
            "loss: 2.109943  [38464/60000]\n",
            "loss: 2.087600  [44864/60000]\n",
            "loss: 2.089380  [51264/60000]\n",
            "loss: 2.069379  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.2%, Avg loss: 2.065585 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.073416  [   64/60000]\n",
            "loss: 2.086738  [ 6464/60000]\n",
            "loss: 2.061990  [12864/60000]\n",
            "loss: 2.095398  [19264/60000]\n",
            "loss: 2.013691  [25664/60000]\n",
            "loss: 2.036132  [32064/60000]\n",
            "loss: 2.029803  [38464/60000]\n",
            "loss: 2.002880  [44864/60000]\n",
            "loss: 2.020524  [51264/60000]\n",
            "loss: 1.994774  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 48.8%, Avg loss: 2.004043 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.008141  [   64/60000]\n",
            "loss: 2.033603  [ 6464/60000]\n",
            "loss: 2.020762  [12864/60000]\n",
            "loss: 2.058630  [19264/60000]\n",
            "loss: 1.963415  [25664/60000]\n",
            "loss: 1.993119  [32064/60000]\n",
            "loss: 1.998971  [38464/60000]\n",
            "loss: 1.971278  [44864/60000]\n",
            "loss: 1.991212  [51264/60000]\n",
            "loss: 1.959586  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.7%, Avg loss: 1.979539 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.977293  [   64/60000]\n",
            "loss: 2.013717  [ 6464/60000]\n",
            "loss: 1.999517  [12864/60000]\n",
            "loss: 2.038154  [19264/60000]\n",
            "loss: 1.939857  [25664/60000]\n",
            "loss: 1.974319  [32064/60000]\n",
            "loss: 1.983193  [38464/60000]\n",
            "loss: 1.960310  [44864/60000]\n",
            "loss: 1.977447  [51264/60000]\n",
            "loss: 1.937486  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 1.967536 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.959573  [   64/60000]\n",
            "loss: 2.003908  [ 6464/60000]\n",
            "loss: 1.986114  [12864/60000]\n",
            "loss: 2.024256  [19264/60000]\n",
            "loss: 1.924310  [25664/60000]\n",
            "loss: 1.962975  [32064/60000]\n",
            "loss: 1.972101  [38464/60000]\n",
            "loss: 1.955029  [44864/60000]\n",
            "loss: 1.969512  [51264/60000]\n",
            "loss: 1.921185  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.959796 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.947828  [   64/60000]\n",
            "loss: 1.997229  [ 6464/60000]\n",
            "loss: 1.976811  [12864/60000]\n",
            "loss: 2.012639  [19264/60000]\n",
            "loss: 1.911619  [25664/60000]\n",
            "loss: 1.954790  [32064/60000]\n",
            "loss: 1.962982  [38464/60000]\n",
            "loss: 1.951237  [44864/60000]\n",
            "loss: 1.964647  [51264/60000]\n",
            "loss: 1.905187  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.7%, Avg loss: 1.945470 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## customnet2는 customnet1과 굉장히 유사하기 때문에 customnet1의 model1 4 6번에 대해서 똑같이 실행\n",
        "## customnet2 model1\n",
        "#model\n",
        "model=Customnet2().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-4\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TQymmDLTabV3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# customnet1의 경우 첫 accuarcy는 56.6% avg loss는 1.958에서 마지막 epoch에서는 69.7% 1.73까지 상승함\n",
        "# customnet2에서 학습했을 때 첫 accuarcy는 53.4% avg loss는 1.986으로 어느 정도 유사함\n",
        "# accuarcy가 비슷할 줄 알았으나 accuarcy가 epoch3에서 70%를 넘기더니 마지막 epoch에서 accuarcy 78% avg loss 1.69까지 변화함\n",
        "# epoch를 더 늘려봤을 떄 둘의 차이도 확인해보면 좋을 듯함(제 컴퓨터 이슈로 거기까지는 힘드네요...)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNV8AeX6avYN",
        "outputId": "f357779f-3860-44e3-a293-5ecdfb35d8cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298865  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b5607274dec7>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.286464  [ 6464/60000]\n",
            "loss: 2.252849  [12864/60000]\n",
            "loss: 2.227568  [19264/60000]\n",
            "loss: 2.153438  [25664/60000]\n",
            "loss: 2.114349  [32064/60000]\n",
            "loss: 2.080132  [38464/60000]\n",
            "loss: 2.031701  [44864/60000]\n",
            "loss: 2.034065  [51264/60000]\n",
            "loss: 2.006605  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.4%, Avg loss: 1.986317 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.949072  [   64/60000]\n",
            "loss: 1.962610  [ 6464/60000]\n",
            "loss: 1.876100  [12864/60000]\n",
            "loss: 1.931191  [19264/60000]\n",
            "loss: 1.877311  [25664/60000]\n",
            "loss: 1.825539  [32064/60000]\n",
            "loss: 1.863250  [38464/60000]\n",
            "loss: 1.828765  [44864/60000]\n",
            "loss: 1.855518  [51264/60000]\n",
            "loss: 1.846874  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 1.811678 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.779472  [   64/60000]\n",
            "loss: 1.824034  [ 6464/60000]\n",
            "loss: 1.726490  [12864/60000]\n",
            "loss: 1.814656  [19264/60000]\n",
            "loss: 1.797148  [25664/60000]\n",
            "loss: 1.744746  [32064/60000]\n",
            "loss: 1.786983  [38464/60000]\n",
            "loss: 1.777323  [44864/60000]\n",
            "loss: 1.795780  [51264/60000]\n",
            "loss: 1.787469  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 1.764624 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.733299  [   64/60000]\n",
            "loss: 1.776024  [ 6464/60000]\n",
            "loss: 1.677630  [12864/60000]\n",
            "loss: 1.779642  [19264/60000]\n",
            "loss: 1.763980  [25664/60000]\n",
            "loss: 1.718104  [32064/60000]\n",
            "loss: 1.757169  [38464/60000]\n",
            "loss: 1.757925  [44864/60000]\n",
            "loss: 1.765374  [51264/60000]\n",
            "loss: 1.761955  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 1.742694 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.708831  [   64/60000]\n",
            "loss: 1.758961  [ 6464/60000]\n",
            "loss: 1.657457  [12864/60000]\n",
            "loss: 1.760968  [19264/60000]\n",
            "loss: 1.737946  [25664/60000]\n",
            "loss: 1.704893  [32064/60000]\n",
            "loss: 1.739532  [38464/60000]\n",
            "loss: 1.745715  [44864/60000]\n",
            "loss: 1.744043  [51264/60000]\n",
            "loss: 1.746755  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 1.728699 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.695800  [   64/60000]\n",
            "loss: 1.749632  [ 6464/60000]\n",
            "loss: 1.645671  [12864/60000]\n",
            "loss: 1.749785  [19264/60000]\n",
            "loss: 1.715637  [25664/60000]\n",
            "loss: 1.698195  [32064/60000]\n",
            "loss: 1.728620  [38464/60000]\n",
            "loss: 1.734060  [44864/60000]\n",
            "loss: 1.727418  [51264/60000]\n",
            "loss: 1.733530  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 1.718135 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.688412  [   64/60000]\n",
            "loss: 1.742951  [ 6464/60000]\n",
            "loss: 1.636427  [12864/60000]\n",
            "loss: 1.743045  [19264/60000]\n",
            "loss: 1.696890  [25664/60000]\n",
            "loss: 1.695350  [32064/60000]\n",
            "loss: 1.721803  [38464/60000]\n",
            "loss: 1.717831  [44864/60000]\n",
            "loss: 1.710683  [51264/60000]\n",
            "loss: 1.721119  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 1.709383 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.682517  [   64/60000]\n",
            "loss: 1.735609  [ 6464/60000]\n",
            "loss: 1.629048  [12864/60000]\n",
            "loss: 1.740070  [19264/60000]\n",
            "loss: 1.682349  [25664/60000]\n",
            "loss: 1.696537  [32064/60000]\n",
            "loss: 1.717535  [38464/60000]\n",
            "loss: 1.699058  [44864/60000]\n",
            "loss: 1.695394  [51264/60000]\n",
            "loss: 1.711561  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 1.701826 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.673499  [   64/60000]\n",
            "loss: 1.726542  [ 6464/60000]\n",
            "loss: 1.628705  [12864/60000]\n",
            "loss: 1.739737  [19264/60000]\n",
            "loss: 1.672603  [25664/60000]\n",
            "loss: 1.699096  [32064/60000]\n",
            "loss: 1.713051  [38464/60000]\n",
            "loss: 1.683376  [44864/60000]\n",
            "loss: 1.682946  [51264/60000]\n",
            "loss: 1.705986  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 1.695640 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.663234  [   64/60000]\n",
            "loss: 1.718273  [ 6464/60000]\n",
            "loss: 1.632653  [12864/60000]\n",
            "loss: 1.737993  [19264/60000]\n",
            "loss: 1.665794  [25664/60000]\n",
            "loss: 1.698807  [32064/60000]\n",
            "loss: 1.708081  [38464/60000]\n",
            "loss: 1.672433  [44864/60000]\n",
            "loss: 1.674057  [51264/60000]\n",
            "loss: 1.703832  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 1.690700 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet2 model4 - loss function 변경(multi margin)\n",
        "model=Customnet2().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.MultiMarginLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "iAxbNX89azha"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# customnet2에서 learing rate e-3, loss function을 multi-margin으로 하였을 때 학습 비교\n",
        "# 첫 epoch에서 accuracy는 64.9%로 customnet1의 72.2%보다 낮고 avg loss가 0.368로 customnet1보다 높긴 하지만 굉장히 낮음\n",
        "# avg loss는 0.23정도까지 낮아지고 최종 epoch에서의 accuarcy는 78% 정도로 customnet1에서와 굉장히 유사함을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmezjbRyazm0",
        "outputId": "ae3fea79-c473-4972-9dd8-be67c9c7d4f1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.898772  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b5607274dec7>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.688860  [ 6464/60000]\n",
            "loss: 0.472018  [12864/60000]\n",
            "loss: 0.471097  [19264/60000]\n",
            "loss: 0.328204  [25664/60000]\n",
            "loss: 0.427985  [32064/60000]\n",
            "loss: 0.330902  [38464/60000]\n",
            "loss: 0.327388  [44864/60000]\n",
            "loss: 0.421901  [51264/60000]\n",
            "loss: 0.381837  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 0.368470 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.346029  [   64/60000]\n",
            "loss: 0.374907  [ 6464/60000]\n",
            "loss: 0.365523  [12864/60000]\n",
            "loss: 0.419771  [19264/60000]\n",
            "loss: 0.274033  [25664/60000]\n",
            "loss: 0.427115  [32064/60000]\n",
            "loss: 0.263612  [38464/60000]\n",
            "loss: 0.290129  [44864/60000]\n",
            "loss: 0.371680  [51264/60000]\n",
            "loss: 0.355707  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.338069 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.307622  [   64/60000]\n",
            "loss: 0.343020  [ 6464/60000]\n",
            "loss: 0.360501  [12864/60000]\n",
            "loss: 0.415045  [19264/60000]\n",
            "loss: 0.239905  [25664/60000]\n",
            "loss: 0.420004  [32064/60000]\n",
            "loss: 0.258867  [38464/60000]\n",
            "loss: 0.293451  [44864/60000]\n",
            "loss: 0.254473  [51264/60000]\n",
            "loss: 0.309053  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.273860 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.202027  [   64/60000]\n",
            "loss: 0.213309  [ 6464/60000]\n",
            "loss: 0.237505  [12864/60000]\n",
            "loss: 0.337938  [19264/60000]\n",
            "loss: 0.214177  [25664/60000]\n",
            "loss: 0.329963  [32064/60000]\n",
            "loss: 0.212532  [38464/60000]\n",
            "loss: 0.203507  [44864/60000]\n",
            "loss: 0.254919  [51264/60000]\n",
            "loss: 0.299965  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.255220 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.185910  [   64/60000]\n",
            "loss: 0.196157  [ 6464/60000]\n",
            "loss: 0.220208  [12864/60000]\n",
            "loss: 0.317873  [19264/60000]\n",
            "loss: 0.211549  [25664/60000]\n",
            "loss: 0.337498  [32064/60000]\n",
            "loss: 0.200706  [38464/60000]\n",
            "loss: 0.185053  [44864/60000]\n",
            "loss: 0.238434  [51264/60000]\n",
            "loss: 0.277813  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.1%, Avg loss: 0.246374 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.175502  [   64/60000]\n",
            "loss: 0.181689  [ 6464/60000]\n",
            "loss: 0.223199  [12864/60000]\n",
            "loss: 0.315409  [19264/60000]\n",
            "loss: 0.198814  [25664/60000]\n",
            "loss: 0.320810  [32064/60000]\n",
            "loss: 0.206896  [38464/60000]\n",
            "loss: 0.180027  [44864/60000]\n",
            "loss: 0.225868  [51264/60000]\n",
            "loss: 0.272891  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.237977 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.169930  [   64/60000]\n",
            "loss: 0.172782  [ 6464/60000]\n",
            "loss: 0.219694  [12864/60000]\n",
            "loss: 0.289522  [19264/60000]\n",
            "loss: 0.198858  [25664/60000]\n",
            "loss: 0.299157  [32064/60000]\n",
            "loss: 0.197949  [38464/60000]\n",
            "loss: 0.174061  [44864/60000]\n",
            "loss: 0.223235  [51264/60000]\n",
            "loss: 0.261955  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.233758 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.165139  [   64/60000]\n",
            "loss: 0.165310  [ 6464/60000]\n",
            "loss: 0.210965  [12864/60000]\n",
            "loss: 0.282215  [19264/60000]\n",
            "loss: 0.188996  [25664/60000]\n",
            "loss: 0.301486  [32064/60000]\n",
            "loss: 0.194233  [38464/60000]\n",
            "loss: 0.172733  [44864/60000]\n",
            "loss: 0.221812  [51264/60000]\n",
            "loss: 0.256599  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.230343 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.163362  [   64/60000]\n",
            "loss: 0.156428  [ 6464/60000]\n",
            "loss: 0.210753  [12864/60000]\n",
            "loss: 0.285335  [19264/60000]\n",
            "loss: 0.179748  [25664/60000]\n",
            "loss: 0.289052  [32064/60000]\n",
            "loss: 0.194594  [38464/60000]\n",
            "loss: 0.172209  [44864/60000]\n",
            "loss: 0.221932  [51264/60000]\n",
            "loss: 0.253665  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.228078 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.165011  [   64/60000]\n",
            "loss: 0.168198  [ 6464/60000]\n",
            "loss: 0.199079  [12864/60000]\n",
            "loss: 0.278217  [19264/60000]\n",
            "loss: 0.185864  [25664/60000]\n",
            "loss: 0.281843  [32064/60000]\n",
            "loss: 0.186467  [38464/60000]\n",
            "loss: 0.173359  [44864/60000]\n",
            "loss: 0.210266  [51264/60000]\n",
            "loss: 0.253906  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.225460 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customnet2 model6 - optimizer 변경(RMSprop)\n",
        "model=Customnet1().to(device) #You can load your model on GPU!\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer =torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.9)"
      ],
      "metadata": {
        "id": "ZHdvpoPSa0dB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X.to(device)) #Also load the input on GPU\n",
        "        loss = loss_fn(pred.cpu(), y) #Detach your predicted result on CPU #Loss could be calculated on CPU\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval() #test mode\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred.cpu(), y).item()\n",
        "            correct += (pred.cpu().argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "# Customnet2에서 RMSprop으로 변경 시 첫 accuaracy가 76.4% avg loss가 1.707로 customnet1에서와 굉장히 유사함\n",
        "# 최종 epoch에서도 accuracy 82.4% avg loss 1.63로 customnet1의 82.4% 1.61로 거의 일치함\n",
        "# 몇몇 예시를 통해 customnet1과 customnet2에서의 학습이 일부 사례를 제외하면 거의 일치하는 것을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wN-rA_ra0fo",
        "outputId": "6eaf5d69-8ffa-43c8-a78b-1204fcf51344"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300474  [   64/60000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.049211  [ 6464/60000]\n",
            "loss: 1.865687  [12864/60000]\n",
            "loss: 1.843297  [19264/60000]\n",
            "loss: 1.758742  [25664/60000]\n",
            "loss: 1.705935  [32064/60000]\n",
            "loss: 1.760592  [38464/60000]\n",
            "loss: 1.699112  [44864/60000]\n",
            "loss: 1.729528  [51264/60000]\n",
            "loss: 1.700549  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 1.706823 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.674795  [   64/60000]\n",
            "loss: 1.719642  [ 6464/60000]\n",
            "loss: 1.653118  [12864/60000]\n",
            "loss: 1.732027  [19264/60000]\n",
            "loss: 1.649596  [25664/60000]\n",
            "loss: 1.675847  [32064/60000]\n",
            "loss: 1.707992  [38464/60000]\n",
            "loss: 1.635135  [44864/60000]\n",
            "loss: 1.678058  [51264/60000]\n",
            "loss: 1.672011  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.2%, Avg loss: 1.679669 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.624713  [   64/60000]\n",
            "loss: 1.675228  [ 6464/60000]\n",
            "loss: 1.651397  [12864/60000]\n",
            "loss: 1.739381  [19264/60000]\n",
            "loss: 1.640641  [25664/60000]\n",
            "loss: 1.679266  [32064/60000]\n",
            "loss: 1.666852  [38464/60000]\n",
            "loss: 1.624706  [44864/60000]\n",
            "loss: 1.670846  [51264/60000]\n",
            "loss: 1.668296  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 1.672740 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.607836  [   64/60000]\n",
            "loss: 1.656950  [ 6464/60000]\n",
            "loss: 1.644233  [12864/60000]\n",
            "loss: 1.742863  [19264/60000]\n",
            "loss: 1.645787  [25664/60000]\n",
            "loss: 1.679312  [32064/60000]\n",
            "loss: 1.647296  [38464/60000]\n",
            "loss: 1.618595  [44864/60000]\n",
            "loss: 1.653936  [51264/60000]\n",
            "loss: 1.651772  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.665637 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.591456  [   64/60000]\n",
            "loss: 1.637274  [ 6464/60000]\n",
            "loss: 1.633505  [12864/60000]\n",
            "loss: 1.731148  [19264/60000]\n",
            "loss: 1.632315  [25664/60000]\n",
            "loss: 1.677481  [32064/60000]\n",
            "loss: 1.638584  [38464/60000]\n",
            "loss: 1.612029  [44864/60000]\n",
            "loss: 1.654139  [51264/60000]\n",
            "loss: 1.646159  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.5%, Avg loss: 1.660157 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.580962  [   64/60000]\n",
            "loss: 1.624265  [ 6464/60000]\n",
            "loss: 1.620304  [12864/60000]\n",
            "loss: 1.723124  [19264/60000]\n",
            "loss: 1.618812  [25664/60000]\n",
            "loss: 1.666007  [32064/60000]\n",
            "loss: 1.646623  [38464/60000]\n",
            "loss: 1.613717  [44864/60000]\n",
            "loss: 1.662256  [51264/60000]\n",
            "loss: 1.644990  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 1.652514 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.573496  [   64/60000]\n",
            "loss: 1.626711  [ 6464/60000]\n",
            "loss: 1.608219  [12864/60000]\n",
            "loss: 1.715228  [19264/60000]\n",
            "loss: 1.606350  [25664/60000]\n",
            "loss: 1.651775  [32064/60000]\n",
            "loss: 1.653643  [38464/60000]\n",
            "loss: 1.614872  [44864/60000]\n",
            "loss: 1.656735  [51264/60000]\n",
            "loss: 1.637502  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 1.648934 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.568676  [   64/60000]\n",
            "loss: 1.612375  [ 6464/60000]\n",
            "loss: 1.599134  [12864/60000]\n",
            "loss: 1.698843  [19264/60000]\n",
            "loss: 1.610806  [25664/60000]\n",
            "loss: 1.639276  [32064/60000]\n",
            "loss: 1.647558  [38464/60000]\n",
            "loss: 1.609078  [44864/60000]\n",
            "loss: 1.664786  [51264/60000]\n",
            "loss: 1.624469  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 1.644481 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.561646  [   64/60000]\n",
            "loss: 1.605201  [ 6464/60000]\n",
            "loss: 1.586758  [12864/60000]\n",
            "loss: 1.693291  [19264/60000]\n",
            "loss: 1.606681  [25664/60000]\n",
            "loss: 1.646973  [32064/60000]\n",
            "loss: 1.636708  [38464/60000]\n",
            "loss: 1.604582  [44864/60000]\n",
            "loss: 1.681590  [51264/60000]\n",
            "loss: 1.616081  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 1.641433 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.567603  [   64/60000]\n",
            "loss: 1.602149  [ 6464/60000]\n",
            "loss: 1.579431  [12864/60000]\n",
            "loss: 1.686752  [19264/60000]\n",
            "loss: 1.603404  [25664/60000]\n",
            "loss: 1.641456  [32064/60000]\n",
            "loss: 1.638922  [38464/60000]\n",
            "loss: 1.597998  [44864/60000]\n",
            "loss: 1.666741  [51264/60000]\n",
            "loss: 1.617221  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 1.639325 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#So, many people use the mixed way like below:\n",
        "\n",
        "class Customnet3(nn.Module):\n",
        "  \"\"\"The main principle is same with above.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    #More simpler than line-by-line but still intuitive to revise\n",
        "    #Making blocks!\n",
        "    self.conv1=nn.Sequential(nn.Conv2d(1,64,(2,2)),\n",
        "                             nn.MaxPool(4),\n",
        "                             nn.ReLU())\n",
        "    self.conv2=nn.Sequential(nn.Conv2d(64,64,(2,2)),\n",
        "                             nn.MaxPool(4),\n",
        "                             nn.ReLU())\n",
        "    self.flatten=nn.Flatten()\n",
        "    self.linear=nn.Sequential(nn.Linear(64,10),\n",
        "                              nn.Softmax())\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    #More simpler than line-by-line forward\n",
        "\n",
        "    x=self.conv1(x)\n",
        "    x=self.conv2(x)\n",
        "    x=self.flatten(x)\n",
        "    x=self.linear(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tofgE4l3bm0B"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((227, 227)), #Size is different with original paper, since we have to obtain 55*55 feature maps after 1st convolution layers\n",
        "                                   #224*224 images do not lead to 55*55 feature maps\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "\n",
        "#Use CIFAR100, instead of ImageNet Dataset\n",
        "trainset = torchvision.datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100('./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPYwMqY_fsL1",
        "outputId": "1055d0e1-f523-49d1-8920-2cfe444541f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:13<00:00, 12476783.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model=Alexnet().to(device)\n",
        "\n",
        "#learning rate\n",
        "learning_rate=1e-4\n",
        "\n",
        "#epoch\n",
        "epoch=10\n",
        "\n",
        "#Loss function\n",
        "loss=nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "jN5WsLpdbUQe",
        "outputId": "b7538112-7c61-4a4c-d958-da1c21b79acb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-03d4c26dcb37>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAlexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Alexnet' is not defined"
          ]
        }
      ]
    }
  ]
}