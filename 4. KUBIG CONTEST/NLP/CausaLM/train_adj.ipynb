{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import SENTIMENT_EXPERIMENTS_DIR, MAX_SENTIMENT_SEQ_LENGTH, SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR, \\\n",
    "    SENTIMENT_ADJECTIVES_DATASETS_DIR\n",
    "from pytorch_lightning import Trainer\n",
    "from BERT.bert_text_classifier import LightningBertPretrainedClassifier, LightningHyperparameters\n",
    "from BERT.bert_pos_tagger import LightningBertPOSTagger\n",
    "from Sentiment_Adjectives.pipeline.predict import predict_models, print_final_metrics\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "from datasets.utils import NUM_POS_TAGS_LABELS\n",
    "from utils import init_logger\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Constants\n",
    "BATCH_SIZE = 128\n",
    "ACCUMULATE = 4\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 50\n",
    "FP16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_train_eval(hparams, task, output_dir):\n",
    "    trainer = Trainer(gpus=1 if DEVICE.type =='cuda' else 0,\n",
    "                      default_save_path=output_dir,\n",
    "                      show_progress_bar=True,\n",
    "                      accumulate_grad_batches=hparams['accumulate'],\n",
    "                      max_nb_epochs=hparams['epochs'],\n",
    "                      early_stop_callback=None)\n",
    "    hparams['output_path'] = trainer.logger.experiment.log_dir.rstrip('tf')\n",
    "    logger = init_logger(\"training\",hparams['output_path'])\n",
    "    logger.info(f\"Training {task} for {hparams['epochs']} epochs\")\n",
    "    if task == \"Sentiment\":\n",
    "        hparams['bert_params']['batch_size'] = hparams['batch_size']\n",
    "        model = LightningBertPretrainedClassifier(LightningHyperparameters(hparams))\n",
    "    \n",
    "    else:\n",
    "        model = LightningBertPOSTagger(LightningHyperparameters(hparams))\n",
    "    trainer.fit(model)\n",
    "    trainer.test()\n",
    "    print_final_metrics(hparams['bert_params']['name'], trainer.tqdm_metrics, logger)\n",
    "    return model\n",
    "\n",
    "def train_models_unit(hparams: Dict, task, group, pretrained_control):\n",
    "    label_size = 2\n",
    "    if task == \"POS_Tagging\":\n",
    "        label_size = NUM_POS_TAGS_LABELS\n",
    "        label_column = f\"{task.lower()}_{group.lower()}_labels\"\n",
    "    elif task == \"IMA\":\n",
    "        label_column = f\"{task.lower()}_{group.lower()}_labels\"\n",
    "    else:\n",
    "        label_column = f\"{task.lower()}_label\"\n",
    "    \n",
    "    hparams['label_column'] = label_column\n",
    "    hparams['num_labels'] = label_size\n",
    "    hparams['bert_params']['label_size'] = label_size\n",
    "    \n",
    "    if hparams['bert_params']['bert_state_dict']:\n",
    "        if pretrained_control:\n",
    "            hparams['bert_params']['name'] = f\"{task}_{group}_ima_control_treated\"\n",
    "        else:\n",
    "            hparams['bert_params']['name'] = f\"{task}_{group}_ima_treated\"\n",
    "        \n",
    "    else:\n",
    "        hparams['bert_params']['name'] = f\"{task}_{group}\"\n",
    "    \n",
    "    OUTPUT_DIR = f\"{SENTIMENT_EXPERIMENTS_DIR}/{hparams['treatment']}/{hparams['bert_params']['name']}\"\n",
    "    model = bert_train_eval(hparams, task, OUTPUT_DIR)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(hparams: Dict, group: str, pretrained_masking_method, pretrained_epoch: int, pretrained_control: bool):\n",
    "    print(f\"Training {hparams['treatment']} models\")\n",
    "    sentiment_model = train_models_unit(hparams, \"Sentiment\",group,pretrained_control)\n",
    "    ima_model = train_models_unit(hparams,\"IMA\",group, pretrained_control)\n",
    "    pos_tagging_model = train_models_unit(hparams, \"POS_Tagging\", group, pretrained_control)\n",
    "    \n",
    "    if hparams['bert_params']['bert_state_dict']:\n",
    "        if pretrained_control:\n",
    "            group = f\"{group}_ima_control_treated\"\n",
    "        else:\n",
    "            group = f\"{group}_ima_treated\"\n",
    "    predict_models(hparams['treatment'], group,\n",
    "                   pretrained_masking_method, pretrained_epoch, pretrained_control,\n",
    "                   sentiment_model, ima_model, pos_tagging_model,\n",
    "                   hparams[\"bert_params\"][\"bert_state_dict\"])\n",
    "\n",
    "def train_all_models(args):\n",
    "    treatment = 'adjectives'\n",
    "    if args.group == 'F':\n",
    "        text_column = 'review'\n",
    "    else:\n",
    "        text_column = \"no_adj_review\"\n",
    "    if args.pretrained_control:\n",
    "        pretrained_treated_model_dir = f\"{SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR}/{args.masking_method}/model_control\"\n",
    "    else:\n",
    "        pretrained_treated_model_dir = f\"{SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR}/{args.masking_method}/model\"\n",
    "\n",
    "    if args.pretrained_epoch is not None:\n",
    "        pretrained_treated_model_dir = f\"{pretrained_treated_model_dir}/epoch_{args.pretrained_epoch}\"\n",
    "    \n",
    "    hparams = {\n",
    "        \"data_path\": SENTIMENT_ADJECTIVES_DATASETS_DIR,\n",
    "        \"treatment\": treatment,\n",
    "        \"masking_method\": args.masking_method,\n",
    "        \"pretrain_conrol\": args.pretrained_control,\n",
    "        \"text_column\": text_column,\n",
    "        \"label_column\": \"sentiment_label\",\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"accumulate\": ACCUMULATE,\n",
    "        \"max_seq_len\": MAX_SENTIMENT_SEQ_LENGTH,\n",
    "        \"num_labels\": 2,\n",
    "        \"name\": f\"Sentiment_{args.group}\",\n",
    "        \"bert_params\": {\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"bert_state_dict\": None,\n",
    "            \"label_size\": 2,\n",
    "            \"name\": f\"Sentiment_{args.group}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    train_models(hparams, args.group, args.masking_method, args.pretrained_epoch, args.pretrained_control)\n",
    "    \n",
    "    hparams['bert_params']['bert_state_dict'] = f\"{pretrained_treated_model_dir}/pytorch_model.bin\"\n",
    "    hparams['treatment']= treatment\n",
    "    train_models(hparams, args.group, args.masking_method, args.pretrained_epoch, args.pretrained_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace()\n",
    "args.treatment = 'adjectives'\n",
    "args.group = 'F'\n",
    "args.masking_method = 'double_num_adj'\n",
    "args.pretrained_epoch = 0\n",
    "args.pretrained_control = True\n",
    "args.batch_size = BATCH_SIZE\n",
    "args.epochs = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:52:57 - Training Sentiment for 50 epochs\n",
      "INFO:training:Training Sentiment for 50 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adjectives models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train-convert_examples_to_features: 100%|██████████| 6400/6400 [00:12<00:00, 524.65it/s]\n",
      "test-convert_examples_to_features: 100%|██████████| 2000/2000 [00:03<00:00, 545.30it/s]\n",
      "dev-convert_examples_to_features: 100%|██████████| 1600/1600 [00:02<00:00, 560.07it/s]\n",
      "Epoch 13:  56%|█████▌    | 35/63 [01:24<01:05,  2.33s/batch, batch_nb=34, gpu=0, loss=0.312, v_nb=2, val_accuracy=0.883, val_loss=0.292]"
     ]
    }
   ],
   "source": [
    "train_all_models(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
