{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import shelve\n",
    "from constants import BERT_PRETRAINED_MODEL, SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR, \\\n",
    "    MAX_SENTIMENT_SEQ_LENGTH, NUM_CPU, SENTIMENT_ADJECTIVES_DATASETS_DIR\n",
    "from datasets.utils import TOKEN_SEPARATOR, ADJ_POS_TAGS, MASK_TOKEN, CLS_TOKEN, SEP_TOKEN, WORDPIECE_PREFIX, \\\n",
    "    POS_TAG_IDX_MAP\n",
    "from BERT.bert_pos_tagger import BertTokenClassificationDataset\n",
    "from random import random, randrange, choice\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "\n",
    "EPOCHS = 5\n",
    "MLM_PROB = 0.15\n",
    "MAX_PRED_PER_SEQ = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTaggedDocumentDatabase:\n",
    "    def __init__(self, reduce_memory=False):\n",
    "        if reduce_memory:\n",
    "            self.temp_dir = TemporaryDirectory()\n",
    "            self.working_dir = Path(self.temp_dir.name)\n",
    "            self.document_shelf_filepath = self.working_dir / 'shelf.db'\n",
    "            self.document_shelf = shelve.open(str(self.document_shelf_filepath),\n",
    "                                              flag='n', protocol=-1)\n",
    "            self.documents = None\n",
    "        else:\n",
    "            self.documents = []\n",
    "            self.documents_pos_idx = []\n",
    "            self.documents_pos_labels = []\n",
    "            self.document_ids = []\n",
    "            self.document_shelf = None\n",
    "            self.document_shelf_filepath = None\n",
    "            self.temp_dir = None\n",
    "        self.doc_lengths = []\n",
    "        self.doc_pos_lengths = []\n",
    "        self.doc_cumsum = None\n",
    "        self.cumsum_max = None\n",
    "        self.reduce_memory = reduce_memory\n",
    "\n",
    "    def add_document(self, document, doc_pos_idx, doc_pos_labels, unique_id):\n",
    "        if not document:\n",
    "            return\n",
    "        if self.reduce_memory:\n",
    "            current_idx = len(self.doc_lengths)\n",
    "            self.document_shelf[str(current_idx)] = document\n",
    "        else:\n",
    "            self.documents.append(document)\n",
    "            self.documents_pos_idx.append(doc_pos_idx)\n",
    "            self.documents_pos_labels.append(doc_pos_labels)\n",
    "            self.document_ids.append(unique_id)\n",
    "        self.doc_lengths.append(len(document))\n",
    "        self.doc_pos_lengths.append(len(doc_pos_idx))\n",
    "\n",
    "    def _precalculate_doc_weights(self):\n",
    "        self.doc_cumsum = np.cumsum(self.doc_lengths)\n",
    "        self.cumsum_max = self.doc_cumsum[-1]\n",
    "\n",
    "    def sample_doc(self, current_idx, sentence_weighted=True):\n",
    "        # Uses the current iteration counter to ensure we don't sample the same doc twice\n",
    "        if sentence_weighted:\n",
    "            # # sentence 간의 weight통해 sampling 확률 차이 (With sentence weighting, we sample docs proportionally to their sentence length)\n",
    "            if self.doc_cumsum is None or len(self.doc_cumsum) != len(self.doc_lengths):\n",
    "                self._precalculate_doc_weights()\n",
    "            rand_start = self.doc_cumsum[current_idx]\n",
    "            rand_end = rand_start + self.cumsum_max - self.doc_lengths[current_idx]\n",
    "            sentence_index = randrange(rand_start, rand_end) % self.cumsum_max\n",
    "            sampled_doc_index = np.searchsorted(self.doc_cumsum, sentence_index, side='right')\n",
    "        else:\n",
    "            # do not use weighting -> random \n",
    "            sampled_doc_index = (current_idx + randrange(1, len(self.doc_lengths))) % len(self.doc_lengths)\n",
    "        assert sampled_doc_index != current_idx\n",
    "        if self.reduce_memory:\n",
    "            return self.document_shelf[str(sampled_doc_index)]\n",
    "        else:\n",
    "            return self.documents[sampled_doc_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_lengths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.reduce_memory:\n",
    "            return self.document_shelf[str(item)]\n",
    "        else:\n",
    "            return self.documents[item], self.documents_pos_idx[item], self.documents_pos_labels[item], self.document_ids[item]\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, traceback):\n",
    "        if self.document_shelf is not None:\n",
    "            self.document_shelf.close()\n",
    "        if self.temp_dir is not None:\n",
    "            self.temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq(tokens, max_num_tokens, doc_pos_idx, doc_pos_labels):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length. Lifted from Google's BERT repo.\"\"\"\n",
    "    l = 0\n",
    "    r = len(tokens)\n",
    "    trunc_tokens = list(tokens)\n",
    "    trunc_doc_pos_labels = list(doc_pos_labels)\n",
    "    while r - l > max_num_tokens:\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if random() < 0.5:\n",
    "            l += 1\n",
    "        else:\n",
    "            r -= 1\n",
    "    if l > 0 or r < len(tokens):\n",
    "        trunc_doc_pos_idx = [i - l for i in doc_pos_idx if l <= i <= r]\n",
    "    else:\n",
    "        trunc_doc_pos_idx = list(doc_pos_idx)\n",
    "    return trunc_tokens[l:r], trunc_doc_pos_idx, trunc_doc_pos_labels[l:r]\n",
    "\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\", [\"index\", \"label\"])\n",
    "\n",
    "def create_masked_adj_predictions(tokens, tokens_pos_idx, cand_indices, num_to_mask, vocab_list):\n",
    "    # Positive examples: M is on Adj x num_adj + word is Adj x num_adj\n",
    "    # Negative examples: M is not on Adj x num_adj + word is not Adj x num_adj\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for i, index_set in enumerate(cand_indices):\n",
    "        if len(masked_lms) >= num_to_mask:\n",
    "            break\n",
    "        # If adding a whole-word mask would exceed the maximum number of\n",
    "        # predictions, then just skip this candidate.\n",
    "        if len(masked_lms) + len(index_set) > num_to_mask:\n",
    "            continue\n",
    "        is_any_index_covered = False\n",
    "        for index in index_set:\n",
    "            if index in covered_indexes:\n",
    "                is_any_index_covered = True\n",
    "                break\n",
    "        if is_any_index_covered:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            covered_indexes.add(index)\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random() < 0.8:\n",
    "                masked_token = MASK_TOKEN\n",
    "            else:\n",
    "                # 10% of the time, keep original\n",
    "                if random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                # 10% of the time, replace with random word\n",
    "                else:\n",
    "                    masked_token = choice(vocab_list)\n",
    "            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    assert len(masked_lms) <= num_to_mask\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "    mask_indices = [p.index for p in masked_lms]\n",
    "    masked_token_labels = [p.label for p in masked_lms]\n",
    "    masked_token_adj_labels = [int(p.index in tokens_pos_idx) for p in masked_lms]\n",
    "\n",
    "    return tokens, mask_indices, masked_token_labels, masked_token_adj_labels\n",
    "\n",
    "\n",
    "def mlm_prob(num_adj:int,num_tokens: int, masked_lm_prob:float) -> int:\n",
    "    return min(num_adj * 2, max(1, int(round(num_tokens * masked_lm_prob))))\n",
    "\n",
    "def double_num_adj(num_adj: int, num_tokens: int, masked_adj_ratio: float) -> int:\n",
    "    adj_ratio = float(num_adj) / num_tokens\n",
    "    if adj_ratio <= masked_adj_ratio:\n",
    "        return num_adj * 2\n",
    "    else:\n",
    "        return int(round(num_adj * (1 + (1 - adj_ratio))))\n",
    "\n",
    "def generate_cand_indices(num_tokens: int, tokens_pos_idx: List[int]) -> List[List[int]]:\n",
    "    adj_idx_list = np.random.permutation(tokens_pos_idx)\n",
    "    non_adj_idx_list = np.random.permutation(list(set(range(1, num_tokens + 1, 1)) - set(tokens_pos_idx)))\n",
    "    cand_indices = []\n",
    "    for i, j in zip_longest(adj_idx_list, non_adj_idx_list, fillvalue=None):\n",
    "        if i:\n",
    "            cand_indices.append([i])\n",
    "        if j:\n",
    "            cand_indices.append([j])\n",
    "    \n",
    "    return cand_indices\n",
    "\n",
    "def create_instances_from_document(\n",
    "        doc_database, doc_idx, max_seq_length, short_seq_prob,\n",
    "        masked_lm_prob, max_predictions_per_seq, whole_word_mask, vocab_list, masking_method):\n",
    "    \"\"\"This code is mostly a duplicate of the equivalent function from Google BERT's repo.\n",
    "    However, we make some changes and improvements. Sampling is improved and no longer requires a loop in this function.\n",
    "    Also, documents are sampled proportionally to the number of sentences they contain, which means each sentence\n",
    "    (rather than each document) has an equal chance of being sampled as a false example for the NextSentence task.\"\"\"\n",
    "    document, doc_pos_idx, doc_pos_labels, unique_id = doc_database[doc_idx]\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length - 2\n",
    "\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    target_seq_length = max_num_tokens\n",
    "    if random() < short_seq_prob:\n",
    "        target_seq_length = max_num_tokens / 2\n",
    "\n",
    "    tokens_a, tokens_pos_idx_list, tokens_pos_labels = truncate_seq(document, max_num_tokens, doc_pos_idx, doc_pos_labels)\n",
    "\n",
    "    assert len(tokens_a) >= 1\n",
    "\n",
    "    tokens = tuple([CLS_TOKEN] + tokens_a + [SEP_TOKEN])\n",
    "    tokens_pos_labels = tuple([BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX] + tokens_pos_labels + [BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX])\n",
    "    # The segment IDs are 0 for the [CLS] token, the A tokens and the first [SEP]\n",
    "    # They are 1 for the B tokens and the final [SEP]\n",
    "    # segment_ids = [0 for _ in range(len(tokens_a) + 2)]\n",
    "    tokens_pos_idx = [i + 1 for i in tokens_pos_idx_list]\n",
    "    num_adj = len(tokens_pos_idx)\n",
    "    num_tokens = len(tokens) - 2\n",
    "    num_non_adj = num_tokens - num_adj\n",
    "\n",
    "    # Currently we follow original MLM training regime where at most 15% of tokens in sequence are masked.\n",
    "    # For each adjective we add a non-adjective to be masked, to preserve balanced classes\n",
    "    # This means that if in a given sequence there are more than 15% adjectives,\n",
    "    # we produce sequences where not all adjectives are masked and they will appear in context\n",
    "    # We produce as many such sequences as needed in order to mask all adjectives in original sequence during training\n",
    "    # if num_to_mask > max_predictions_per_seq:\n",
    "    #     print(f\"{num_to_mask} is more than max per seq of {max_predictions_per_seq}\")\n",
    "    # if num_to_mask > int(round(len(tokens) * masked_lm_prob)):\n",
    "    #     print(f\"{num_to_mask} is more than {masked_lm_prob} of {num_tokens}\")\n",
    "    # if num_to_mask > len(tokens):\n",
    "    #     print(f\"{num_to_mask} is more than {num_tokens}\")\n",
    "\n",
    "    if masking_method == \"mlm_prob\":\n",
    "        num_to_mask = mlm_prob(num_adj, num_tokens, masked_lm_prob)\n",
    "    else:\n",
    "        num_to_mask = double_num_adj(num_adj, num_tokens, 0.4)\n",
    "\n",
    "    cand_indices = generate_cand_indices(num_tokens, tokens_pos_idx)\n",
    "\n",
    "    instances = []\n",
    "    num_adj_masked = 0\n",
    "    num_masked = 0\n",
    "    while num_masked < len(cand_indices) and num_adj_masked < num_adj:\n",
    "        instance_tokens, masked_lm_positions, masked_lm_labels, masked_adj_labels = create_masked_adj_predictions(\n",
    "            list(tokens), tokens_pos_idx, cand_indices[num_masked:], num_to_mask, vocab_list)\n",
    "\n",
    "        instance = {\n",
    "            \"unique_id\": str(unique_id),\n",
    "            \"tokens\": [str(i) for i in instance_tokens],\n",
    "            \"masked_lm_positions\": [str(i) for i in masked_lm_positions],\n",
    "            \"masked_lm_labels\": [str(i) for i in masked_lm_labels],\n",
    "            \"masked_adj_labels\": [str(i) for i in masked_adj_labels],\n",
    "            \"pos_tag_labels\": [str(i) for i in tokens_pos_labels]\n",
    "        }\n",
    "        instances.append(instance)\n",
    "\n",
    "        num_adj_masked += sum(masked_adj_labels)\n",
    "        num_masked = len(masked_lm_labels)\n",
    "\n",
    "    return instances\n",
    "\n",
    "\n",
    "def create_training_file(docs, vocab_list, args, epoch_num, output_dir):\n",
    "    epoch_filename = output_dir / f\"{BERT_PRETRAINED_MODEL}_epoch_{epoch_num}.json\"\n",
    "    num_instances = 0\n",
    "    with epoch_filename.open('w') as epoch_file:\n",
    "        for doc_idx in trange(len(docs), desc=\"Document\"):\n",
    "            doc_instances = create_instances_from_document(\n",
    "                docs, doc_idx, max_seq_length=args.max_seq_len, short_seq_prob=args.short_seq_prob,\n",
    "                masked_lm_prob=args.masked_lm_prob, max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                whole_word_mask=args.do_whole_word_mask, vocab_list=vocab_list, masking_method=args.masking_method)\n",
    "            doc_instances = [json.dumps(instance) for instance in doc_instances]\n",
    "            for instance in doc_instances:\n",
    "                epoch_file.write(instance + '\\n')\n",
    "                num_instances += 1\n",
    "    metrics_file = output_dir / f\"{BERT_PRETRAINED_MODEL}_epoch_{epoch_num}_metrics.json\"\n",
    "    with metrics_file.open('w') as metrics_file:\n",
    "        metrics = {\n",
    "            \"num_training_examples\": num_instances,\n",
    "            \"max_seq_len\": args.max_seq_len\n",
    "        }\n",
    "        metrics_file.write(json.dumps(metrics))\n",
    "    print(\"\\nTotal Number of training instances:\", num_instances)\n",
    "\n",
    "def generate_data_for_treatment(tokenizer, args):\n",
    "    print(f\"\\nGenerating data for adjectives\")\n",
    "    output_dir = Path(SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR) / args.masking_method\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    with POSTaggedDocumentDatabase(reduce_memory=args.reduce_memory) as docs:\n",
    "        id_reviews = list()\n",
    "        tagged_reviews = list()\n",
    "        for dataset in (\"train\", \"dev\"):\n",
    "            DATASET_FILE = f\"{SENTIMENT_ADJECTIVES_DATASETS_DIR}/adjectives_{dataset}_aggressive.csv\"\n",
    "            df = pd.read_csv(DATASET_FILE, header=0, encoding='utf-8')\n",
    "            id_reviews += df[\"id\"].tolist()\n",
    "            tagged_reviews += df[\"tagged_review\"].tolist()\n",
    "        for idx, review in zip(id_reviews, tagged_reviews):\n",
    "            tagged_tokens = []\n",
    "            adj_adv_idx = []\n",
    "            adj_adv_tokens = []\n",
    "            review_tokens = []\n",
    "            pos_tag_labels = []\n",
    "            for i, token_pos in enumerate(review.strip().split(TOKEN_SEPARATOR)):\n",
    "                token_pos_match = re.match(\"(.*)_([A-Z]+)\", token_pos)\n",
    "                if token_pos_match:\n",
    "                    token, pos = token_pos_match.group(1), token_pos_match.group(2)\n",
    "                    tagged_tokens.append((token, pos))\n",
    "                    review_tokens.append(token)\n",
    "                    if pos in ADJ_POS_TAGS:\n",
    "                        adj_adv_tokens.append((i, token))\n",
    "                        pos_tag_labels.append(BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX)\n",
    "                    else:\n",
    "                        pos_tag_labels.append(POS_TAG_IDX_MAP[pos])\n",
    "            # line_words = re.sub(\"_[A-Z]+\", \"\", line)\n",
    "            doc = tokenizer.tokenize(TOKEN_SEPARATOR.join(review_tokens))\n",
    "            if doc:\n",
    "                pos_tag_labels = BertTokenClassificationDataset.align_labels_to_bert_tokenization(tokenizer,\n",
    "                                                                                                  doc,\n",
    "                                                                                                  review_tokens,\n",
    "                                                                                                  pos_tag_labels)\n",
    "                if len(doc) == len(tagged_tokens):\n",
    "                    adj_adv_idx = [i for i, _ in adj_adv_tokens]\n",
    "                else:\n",
    "                    adj_token_idx = 0\n",
    "                    for j, bert_token in enumerate(doc):\n",
    "                        if adj_token_idx == len(adj_adv_tokens):\n",
    "                            break\n",
    "                        adj_token = adj_adv_tokens[adj_token_idx][1]\n",
    "                        if bert_token == adj_token:\n",
    "                            adj_adv_idx.append(j)\n",
    "                            adj_token_idx += 1\n",
    "                        elif bert_token in adj_token:\n",
    "                            adj_adv_idx.append(j)\n",
    "                            # if args.do_whole_word_mask:\n",
    "                            k = 1\n",
    "                            while j + k < len(doc) and doc[j + k].startswith(WORDPIECE_PREFIX):\n",
    "                                adj_adv_idx.append(j + k)\n",
    "                                k += 1\n",
    "                            adj_token_idx += 1\n",
    "                docs.add_document(tuple(doc), tuple(adj_adv_idx), tuple(pos_tag_labels), idx)  # If the last doc didn't end on a newline, make sure it still gets added\n",
    "        if len(docs) <= 1:\n",
    "            exit(\n",
    "                \"ERROR: No document breaks were found in the input file! These are necessary to allow the script to \"\n",
    "                \"ensure that random NextSentences are not sampled from the same document. Please add blank lines to \"\n",
    "                \"indicate breaks between documents in your input file. If your dataset does not contain multiple \"\n",
    "                \"documents, blank lines can be inserted at any natural boundary, such as the ends of chapters, \"\n",
    "                \"sections or paragraphs.\")\n",
    "        vocab_list = list(tokenizer.vocab.keys())\n",
    "        for epoch in trange(args.epochs_to_generate, desc=\"Epoch\"):\n",
    "            create_training_file(docs, vocab_list, args, epoch, output_dir)\n",
    "\n",
    "                                                                                                     \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace()\n",
    "#args.train_corpus \n",
    "#args.output_dir\n",
    "args.bert_model = BERT_PRETRAINED_MODEL\n",
    "args.do_lower_case = True\n",
    "args.do_whole_word_mask = True\n",
    "args.reduce_memory = False\n",
    "args.num_workers = int(NUM_CPU)\n",
    "args.epochs_to_generate = int(EPOCHS)\n",
    "args.max_seq_len = int(MAX_SENTIMENT_SEQ_LENGTH)\n",
    "args.short_seq_prob = float(0.1)\n",
    "args.masked_lm_prob = float(0.1)\n",
    "args.masked_lm_prob = float(MLM_PROB)\n",
    "args.max_predictions_per_seq = int(MAX_PRED_PER_SEQ)\n",
    "args.masking_method = \"double_num_adj\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data for adjectives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document: 100%|██████████| 4000/4000 [00:01<00:00, 3574.16it/s]\n",
      "Epoch:  20%|██        | 1/5 [00:01<00:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of training instances: 3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document: 100%|██████████| 4000/4000 [00:01<00:00, 3308.80it/s]\n",
      "Epoch:  40%|████      | 2/5 [00:02<00:03,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of training instances: 3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document: 100%|██████████| 4000/4000 [00:01<00:00, 2694.60it/s]\n",
      "Epoch:  60%|██████    | 3/5 [00:03<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of training instances: 3364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document: 100%|██████████| 4000/4000 [00:01<00:00, 2616.26it/s]\n",
      "Epoch:  80%|████████  | 4/5 [00:05<00:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of training instances: 3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document: 100%|██████████| 4000/4000 [00:01<00:00, 2649.71it/s]\n",
      "Epoch: 100%|██████████| 5/5 [00:06<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of training instances: 3363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_MODEL, \n",
    "                                          do_lower_case=bool(BERT_PRETRAINED_MODEL.endswith('uncased')))\n",
    "generate_data_for_treatment(tokenizer, args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
