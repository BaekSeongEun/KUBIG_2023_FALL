{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from BERT.lm_finetune.bert_mlm_finetune import BertForMLMPreTraining\n",
    "from Sentiment_Adjectives.lm_finetune.pregenerate_training_data import EPOCHS\n",
    "from BERT.bert_text_dataset import BertTextDataset\n",
    "from utils import init_logger\n",
    "from constants import RANDOM_SEED, BERT_PRETRAINED_MODEL, NUM_CPU, \\\n",
    "    SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR, SENTIMENT_ADJECTIVES_PRETRAIN_MLM_DIR\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "FP16 = False\n",
    "\n",
    "InputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask lm_label_ids\")\n",
    "logger = init_logger(\"MLM-pretraining\", f\"{SENTIMENT_ADJECTIVES_PRETRAIN_MLM_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, tokenizer, max_seq_length):\n",
    "    tokens = example[\"tokens\"]\n",
    "    masked_lm_positions = np.array([int(i) for i in example[\"masked_lm_positions\"]])\n",
    "    masked_lm_labels = example[\"masked_lm_labels\"]\n",
    "\n",
    "    # assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "    input_array = np.zeros(max_seq_length, dtype=int)\n",
    "    input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "    mask_array = np.zeros(max_seq_length, dtype=bool)\n",
    "    mask_array[:len(input_ids)] = 1\n",
    "\n",
    "    lm_label_array = np.full(max_seq_length, dtype=int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "    lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "\n",
    "    features = InputFeatures(input_ids=input_array,\n",
    "                             input_mask=mask_array,\n",
    "                             lm_label_ids=lm_label_array)\n",
    "    return features\n",
    "\n",
    "class PregeneratedDataset(Dataset):\n",
    "    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.epoch = epoch\n",
    "        self.data_epoch = epoch % num_data_epochs\n",
    "        data_file = training_path / f\"{BERT_PRETRAINED_MODEL}_epoch_{self.data_epoch}.json\"\n",
    "        metrics_file = training_path / f\"{BERT_PRETRAINED_MODEL}_epoch_{self.data_epoch}_metrics.json\"\n",
    "        assert data_file.is_file() and metrics_file.is_file()\n",
    "        metrics = json.loads(metrics_file.read_text())\n",
    "        num_samples = metrics['num_training_examples']\n",
    "        seq_len = metrics['max_seq_len']\n",
    "        self.temp_dir = None\n",
    "        self.working_dir = None\n",
    "        if reduce_memory:\n",
    "            self.temp_dir = TemporaryDirectory()\n",
    "            self.working_dir = Path(self.temp_dir.name)\n",
    "            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n",
    "                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n",
    "            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n",
    "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
    "            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n",
    "                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n",
    "            lm_label_ids[:] = BertTextDataset.MLM_IGNORE_LABEL_IDX\n",
    "        else:\n",
    "            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
    "            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "        logging.info(f\"Loading training examples for epoch {epoch}\")\n",
    "        with data_file.open() as f:\n",
    "            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
    "                line = line.strip()\n",
    "                example = json.loads(line)\n",
    "                features = convert_example_to_features(example, tokenizer, seq_len)\n",
    "                input_ids[i] = features.input_ids\n",
    "                input_masks[i] = features.input_mask\n",
    "                lm_label_ids[i] = features.lm_label_ids\n",
    "        assert i == num_samples - 1  # Assert that the sample count metric was true\n",
    "        logging.info(\"Loading complete!\")\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_ids = input_ids\n",
    "        self.input_masks = input_masks\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n",
    "                torch.tensor(self.input_masks[item].astype(np.int64)),\n",
    "                torch.tensor(self.lm_label_ids[item].astype(np.int64)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_on_treatment(args):\n",
    "    assert args.pregenerated_data.is_dir(), \\\n",
    "        \"--pregenerated_data should point to the folder of files made by pregenerate_training_data.py!\"\n",
    "\n",
    "    samples_per_epoch = []\n",
    "    for i in range(args.epochs):\n",
    "        epoch_file = args.pregenerated_data / f\"{BERT_PRETRAINED_MODEL}_epoch_{i}.json\"\n",
    "        metrics_file = args.pregenerated_data / f\"{BERT_PRETRAINED_MODEL}_epoch_{i}_metrics.json\"\n",
    "        if epoch_file.is_file() and metrics_file.is_file():\n",
    "            metrics = json.loads(metrics_file.read_text())\n",
    "            samples_per_epoch.append(metrics['num_training_examples'])\n",
    "        else:\n",
    "            if i == 0:\n",
    "                exit(\"No training data was found!\")\n",
    "            print(f\"Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({args.epochs}).\")\n",
    "            print(\"This script will loop over the available data, but training diversity may be negatively impacted.\")\n",
    "            num_data_epochs = i\n",
    "            break\n",
    "    else:\n",
    "        num_data_epochs = args.epochs\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logging.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if args.output_dir.is_dir() and list(args.output_dir.iterdir()):\n",
    "        logging.warning(f\"Output directory ({args.output_dir}) already exists and is not empty!\")\n",
    "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    total_train_examples = 0\n",
    "    for i in range(args.epochs):\n",
    "        # The modulo takes into account the fact that we may loop over limited epochs of data\n",
    "        total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n",
    "\n",
    "    num_train_optimization_steps = int(\n",
    "        total_train_examples / args.train_batch_size / args.gradient_accumulation_steps)\n",
    "    if args.local_rank != -1:\n",
    "        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    # Prepare model\n",
    "    model = BertForMLMPreTraining.from_pretrained(args.bert_model)\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "        model = DDP(model)\n",
    "    #elif n_gpu > 1:\n",
    "    #    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FusedAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                              lr=args.learning_rate,\n",
    "                              bias_correction=False,\n",
    "                              max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0:\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "    loss_dict = defaultdict(list)\n",
    "    global_step = 0\n",
    "    #logging.info(\"***** Running training *****\")\n",
    "    #logging.info(f\"  Num examples = {total_train_examples}\")\n",
    "    #logging.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    #logging.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    model.train()\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_dataset = PregeneratedDataset(epoch=epoch, training_path=args.pregenerated_data, tokenizer=tokenizer,\n",
    "                                            num_data_epochs=num_data_epochs, reduce_memory=args.reduce_memory)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(epoch_dataset)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(epoch_dataset)\n",
    "        train_dataloader = DataLoader(epoch_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=NUM_CPU)\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, lm_label_ids = batch\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=input_mask, masked_lm_labels=lm_label_ids)\n",
    "                loss = outputs[0]\n",
    "                #if n_gpu > 1:\n",
    "                #    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                if args.fp16:\n",
    "                    optimizer.backward(loss)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                pbar.update(1)\n",
    "                mean_loss = tr_loss * args.gradient_accumulation_steps / nb_tr_steps\n",
    "                pbar.set_postfix_str(f\"Loss: {mean_loss:.5f}\")\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                loss_dict[\"epoch\"].append(epoch)\n",
    "                loss_dict[\"batch_id\"].append(step)\n",
    "                loss_dict[\"mlm_loss\"].append(loss.item())\n",
    "        # Save a trained model\n",
    "        if epoch < num_data_epochs:\n",
    "            logging.info(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "            epoch_output_dir = args.output_dir / f\"epoch_{epoch}\"\n",
    "            epoch_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_pretrained(epoch_output_dir)\n",
    "            tokenizer.save_pretrained(epoch_output_dir)\n",
    "\n",
    "    # Save a trained model\n",
    "    if n_gpu > 1 : #and torch.distributed.get_rank() == 0 or n_gpu <=1:\n",
    "        logging.info(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "        model.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        df = pd.DataFrame.from_dict(loss_dict)\n",
    "        df.to_csv(args.output_dir/\"losses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(bert_model='bert-large-uncased', do_lower_case=True, reduce_memory=False, epochs=5, local_rank=-1, no_cuda=False, gradient_accumulation_steps=1, train_batch_size=10, fp16=False, loss_scale=0.0, warmup_steps=0, adam_epsilon=1e-08, learning_rate=3e-05, seed=212, masking_method='double_num_adj', pregenerated_data=PosixPath('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj'), output_dir=PosixPath('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/MLM/double_num_adj/model'))\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace()\n",
    "#args.train_corpus \n",
    "#args.output_dir\n",
    "args.bert_model = BERT_PRETRAINED_MODEL\n",
    "args.do_lower_case = True\n",
    "args.reduce_memory = False\n",
    "args.epochs= int(EPOCHS)\n",
    "args.local_rank = -1\n",
    "args.no_cuda = False\n",
    "args.gradient_accumulation_steps = int(1)\n",
    "args.train_batch_size = int(BATCH_SIZE)\n",
    "args.fp16 = False\n",
    "args.loss_scale = float(0)\n",
    "args.warmup_steps = int(0)\n",
    "args.adam_epsilon = float(1e-8)\n",
    "args.learning_rate = float(3e-5)\n",
    "args.seed = int(RANDOM_SEED)\n",
    "args.masking_method = str('double_num_adj')\n",
    "args.pregenerated_data = Path(SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR) / args.masking_method\n",
    "args.output_dir = Path(SENTIMENT_ADJECTIVES_PRETRAIN_MLM_DIR) / args.masking_method / \"model\"\n",
    "args.fp16 = FP16\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Output directory (/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/MLM/double_num_adj/model) already exists and is not empty!\n",
      "/tmp/ipykernel_2872431/1537350247.py:50: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 8273.17it/s] \n",
      "Epoch 0:   0%|          | 1/337 [00:04<24:21,  4.35s/it, Loss: 3.63192]/home/wldn/.local/lib/python3.10/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Epoch 0: 100%|██████████| 337/337 [03:24<00:00,  1.65it/s, Loss: 2.81542]\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 6097.57it/s] \n",
      "Epoch 1: 100%|██████████| 337/337 [03:26<00:00,  1.63it/s, Loss: 2.48364]\n",
      "Training examples: 100%|██████████| 3364/3364 [00:00<00:00, 6030.21it/s] \n",
      "Epoch 2: 100%|██████████| 337/337 [03:26<00:00,  1.63it/s, Loss: 2.30836]\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 5997.21it/s] \n",
      "Epoch 3: 100%|██████████| 337/337 [03:26<00:00,  1.63it/s, Loss: 2.18986]\n",
      "Training examples: 100%|██████████| 3363/3363 [00:00<00:00, 5807.67it/s] \n",
      "Epoch 4: 100%|██████████| 337/337 [03:27<00:00,  1.63it/s, Loss: 2.11485]\n"
     ]
    }
   ],
   "source": [
    "pretrain_on_treatment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3856986/3279059602.py:50: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
      "Training examples:   0%|          | 0/5992 [00:00<?, ?it/s]/tmp/ipykernel_3856986/3279059602.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_array = np.zeros(max_seq_length, dtype=np.int)\n",
      "/tmp/ipykernel_3856986/3279059602.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
      "/tmp/ipykernel_3856986/3279059602.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
      "Training examples: 100%|██████████| 5992/5992 [00:01<00:00, 5966.68it/s] \n"
     ]
    }
   ],
   "source": [
    "num_data_epochs=5\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "dataset=PregeneratedDataset(epoch=0, training_path=args.pregenerated_data, tokenizer=tokenizer,\n",
    "                                        num_data_epochs=num_data_epochs, reduce_memory=args.reduce_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class epochDataset(Dataset):\n",
    "    def __init__(self,input_ids,input_masks,lm_label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_masks = input_masks\n",
    "        self.lm_label_ids = lm_label_ids \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n",
    "                torch.tensor(self.input_masks[item].astype(np.int64)),\n",
    "                torch.tensor(self.lm_label_ids[item].astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3856986/2350645100.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
      "Training examples:   0%|          | 0/3995 [00:00<?, ?it/s]/tmp/ipykernel_3856986/3279059602.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_array = np.zeros(max_seq_length, dtype=np.int)\n",
      "/tmp/ipykernel_3856986/3279059602.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
      "/tmp/ipykernel_3856986/3279059602.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
      "Training examples: 100%|██████████| 3995/3995 [00:00<00:00, 4826.47it/s] \n"
     ]
    }
   ],
   "source": [
    "training_path=args.pregenerated_data\n",
    "vocab=tokenizer.vocab\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "epoch = 4 \n",
    "data_epoch = epoch % 5 \n",
    "data_file = training_path / f\"{BERT_PRETRAINED_MODEL}_epoch_{data_epoch}.json\"\n",
    "metrics_file = training_path / f\"{BERT_PRETRAINED_MODEL}_epoch_{data_epoch}_metrics.json\"\n",
    "metrics = json.loads(metrics_file.read_text())\n",
    "num_samples = metrics['num_training_examples']\n",
    "seq_len = metrics['max_seq_len']\n",
    "temp_dir = None\n",
    "working_dir = None\n",
    "input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
    "input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "with data_file.open() as f:\n",
    "    for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
    "        line = line.strip()\n",
    "        example = json.loads(line)\n",
    "        features = convert_example_to_features(example, tokenizer, seq_len)\n",
    "        input_ids[i] = features.input_ids\n",
    "        input_masks[i] = features.input_mask\n",
    "        lm_label_ids[i] = features.lm_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch0=dict()\n",
    "epoch0['input_ids'] = input_ids\n",
    "epoch0['input_masks'] = input_masks\n",
    "epoch0['lm_label_ids'] = lm_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./datasets/aggressive/epoch4', 'wb') as file:\n",
    "    pickle.dump(epoch0,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
