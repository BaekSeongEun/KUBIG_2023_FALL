{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import count_num_cpu_gpu\n",
    "count_num_cpu_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "from utils import init_logger\n",
    "\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from BERT.bert_text_dataset import BertTextDataset\n",
    "from BERT.bert_pos_tagger import BertTokenClassificationDataset\n",
    "from constants import BERT_PRETRAINED_MODEL, RANDOM_SEED, SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR, SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR, NUM_CPU\n",
    "from datasets.utils import NUM_POS_TAGS_LABELS\n",
    "from Sentiment_Adjectives.lm_finetune.pregenerate_training_data import EPOCHS\n",
    "from Sentiment_Adjectives.lm_finetune.bert_ima_finetune import BertForIMAPreTraining, BertForIMAwControlPreTraining\n",
    "\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "FP16 = False\n",
    "\n",
    "AdjInputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask lm_label_ids adj_labels pos_tag_labels unique_id\")\n",
    "\n",
    "logger = init_logger(\"IMA-pretraining\", f\"{SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PregeneratedPOSTaggedDataset(Dataset):\n",
    "    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.epoch = epoch\n",
    "        self.data_epoch = epoch % num_data_epochs\n",
    "        data_file = Path('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj/bert-large-uncased_epoch_%d.json'%self.epoch) \n",
    "        metrics_file = training_path / f\"{BERT_PRETRAINED_MODEL}_epoch_{self.data_epoch}_metrics.json\"\n",
    "        assert data_file.is_file() and metrics_file.is_file()\n",
    "        metrics = json.loads(metrics_file.read_text())\n",
    "        num_samples = metrics['num_training_examples']\n",
    "        seq_len = metrics['max_seq_len']\n",
    "        self.temp_dir = None\n",
    "        self.working_dir = None\n",
    "        if reduce_memory:\n",
    "            self.temp_dir = TemporaryDirectory()\n",
    "            self.working_dir = Path(self.temp_dir.name)\n",
    "            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n",
    "                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n",
    "            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n",
    "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
    "            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n",
    "                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n",
    "            lm_label_ids[:] = BertTextDataset.MLM_IGNORE_LABEL_IDX\n",
    "            adj_labels = np.memmap(filename=self.working_dir/'adj_labels.memmap',\n",
    "                                   shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n",
    "            adj_labels[:] = BertTextDataset.MLM_IGNORE_LABEL_IDX\n",
    "        else:\n",
    "            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
    "            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "            adj_labels = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "            pos_tag_labels = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX)\n",
    "            unique_ids = np.zeros(shape=(num_samples,), dtype=np.int32)\n",
    "        logger.info(f\"Loading training examples for epoch {epoch}\")\n",
    "        with data_file.open() as f:\n",
    "            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
    "                line = line.strip()\n",
    "                example = json.loads(line)\n",
    "                features = self.convert_example_to_features(example, tokenizer, seq_len)\n",
    "                input_ids[i] = features.input_ids\n",
    "                input_masks[i] = features.input_mask\n",
    "                lm_label_ids[i] = features.lm_label_ids\n",
    "                adj_labels[i] = features.adj_labels\n",
    "                pos_tag_labels[i] = features.pos_tag_labels\n",
    "                unique_ids[i] = features.unique_id\n",
    "        assert i == num_samples - 1\n",
    "        logger.info(\"Loading complete\")\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_ids = input_ids\n",
    "        self.input_masks = input_masks\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "        self.adj_labels = adj_labels\n",
    "        self.pos_tag_labels = pos_tag_labels\n",
    "        self.unique_ids = unique_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return  (torch.tensor(self.input_ids[item].astype(np.int64)),\n",
    "                    torch.tensor(self.input_masks[item].astype(np.int64)),\n",
    "                    torch.tensor(self.lm_label_ids[item].astype(np.int64)),\n",
    "                    torch.tensor(self.adj_labels[item].astype(np.int64)),\n",
    "                    torch.tensor(self.pos_tag_labels[item].astype(np.int64)),\n",
    "                    torch.tensor(self.unique_ids[item].astype(np.int64)))\n",
    "    @staticmethod\n",
    "    def convert_example_to_features(example, tokenizer, max_seq_length):\n",
    "        tokens = example['tokens']\n",
    "        masked_lm_positions = np.array([int(i) for i in example['masked_lm_positions']])\n",
    "        masked_lm_labels = example['masked_lm_labels']\n",
    "        masked_adj_labels = [int(i) for i in example['masked_adj_labels']]\n",
    "        pos_tag_labels = [int(i) for i in example['pos_tag_labels']]\n",
    "        unique_id = int(example['unique_id'])\n",
    "\n",
    "        assert len(tokens) <= max_seq_length\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "        input_array = np.zeros(max_seq_length, dtype=int)\n",
    "        input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "        mask_array = np.zeros(max_seq_length, dtype = bool)\n",
    "        mask_array[:len(input_ids)] = 1\n",
    "\n",
    "        lm_label_array = np.full(max_seq_length, dtype=int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "        lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "\n",
    "        adj_label_array = np.full(max_seq_length, dtype=int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "        adj_label_array[masked_lm_positions] = masked_adj_labels\n",
    "\n",
    "        pos_tag_labels_array = np.full(max_seq_length, dtype=int, fill_value=BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX)\n",
    "        pos_tag_labels_array[:len(input_ids)] = pos_tag_labels\n",
    "\n",
    "        features = AdjInputFeatures(input_ids=input_array,\n",
    "                                    input_mask=mask_array,\n",
    "                                    lm_label_ids=lm_label_array,\n",
    "                                    adj_labels = adj_label_array,\n",
    "                                    pos_tag_labels=pos_tag_labels_array,\n",
    "                                unique_id=unique_id)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_on_treatment(args):\n",
    "    assert args.pregenerated_data.is_dir(), \\\n",
    "            \"--pregenerated_data should point to the folder of files made by pregenerate_training_data.py!\"\n",
    "    samples_per_epoch=[]\n",
    "    for i in range(args.epochs):\n",
    "        epoch_file = Path('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj/bert-large-uncased_epoch_%d.json'%i)\n",
    "        metrics_file = args.pregenerated_data / f\"{BERT_PRETRAINED_MODEL}_epoch_{i}_metrics.json\"\n",
    "        if epoch_file.is_file() and metrics_file.is_file():\n",
    "            metrics = json.loads(metrics_file.read_text())\n",
    "            samples_per_epoch.append(metrics['num_training_examples'])\n",
    "        else:\n",
    "            if i==0:\n",
    "                exit('No training data was found!')\n",
    "            logger.warn(f\"Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({args.epochs}).\")\n",
    "            logger.warn(\"This script will loop over the available data, but training diversity may be negatively impacted.\")\n",
    "            num_data_epochs = i\n",
    "            break\n",
    "    else:\n",
    "        num_data_epochs = args.epochs\n",
    "    \n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "    \n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "            args.gradient_accumulation_steps))\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "    \n",
    "    if args.output_dir.is_dir() and list(args.output_dir.iterdir()):\n",
    "        logger.warning(f\"Output directory ({args.output_dir}) already exists and is not empty!\")\n",
    "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case = args.do_lower_case)\n",
    "    total_train_examples = 0 \n",
    "    for i in range(args.epochs):\n",
    "        total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n",
    "    \n",
    "    num_train_optimization_steps = int(\n",
    "        total_train_examples / args.train_batch_size / args.gradient_accumulation_steps)\n",
    "    if args.local_rank != -1:\n",
    "        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "     # Prepare model\n",
    "    if args.control_task:\n",
    "        config = BertConfig.from_pretrained(args.bert_model, num_labels=NUM_POS_TAGS_LABELS)\n",
    "        model = BertForIMAwControlPreTraining.from_pretrained(pretrained_model_name_or_path=args.bert_model, config = config)\n",
    "    \n",
    "    else:\n",
    "        model = BertForIMAPreTraining.from_pretrained(args.bert_model)\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"\n",
    "            )\n",
    "        model = DDP(model)\n",
    "    \n",
    "    # Prepare optimzer \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias','LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01},\n",
    "        {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "    ]\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FuseAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                    \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "        optimizer = FuseAdam(optimizer_grouped_parameters,\n",
    "                            lr=args.learning_rate,\n",
    "                            bias_correction=False,\n",
    "                            max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0 :\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "    else:\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps = num_train_optimization_steps)\n",
    "    loss_dict = defaultdict(list)\n",
    "    global_step = 0\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {total_train_examples}\")\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    model.train()\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_dataset = PregeneratedPOSTaggedDataset(epoch=epoch, training_path = args.pregenerated_data,\n",
    "                                                    tokenizer = tokenizer,\n",
    "                                                    num_data_epochs = num_data_epochs, reduce_memory=args.reduce_memory)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(epoch_dataset)\n",
    "        else: \n",
    "            train_sampler = DistributedSampler(epoch_dataset)\n",
    "        train_dataloader = DataLoader(epoch_dataset, sampler= train_sampler, batch_size=args.train_batch_size,num_workers=NUM_CPU)\n",
    "        tr_loss=0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0 \n",
    "        with tqdm(total=len(train_dataloader), desc=f\"Epoch{epoch}\") as pbar:\n",
    "            for step,batch in enumerate(train_dataloader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, lm_label_ids, adj_labels, pos_tag_labels, unique_id = batch\n",
    "                if args.control_task:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=input_mask,\n",
    "                                    masked_lm_labels=lm_label_ids, masked_adj_labels=adj_labels,\n",
    "                                        pos_tagging_labels=pos_tag_labels)\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=input_mask,\n",
    "                                        masked_lm_labels=lm_label_ids, masked_adj_labels=adj_labels)\n",
    "                loss = outputs[0]\n",
    "                mlm_loss = outputs[1]\n",
    "                adversarial_loss= outputs[2]\n",
    "                if args.control_task:\n",
    "                    control_loss = outputs[3]\n",
    "                #if n_gpu > 1:\n",
    "                #    loss = loss.mean()\n",
    "                #    mlm_loss = mlm_loss.mean()\n",
    "                #    adversarial_loss = adversarial_loss.mean()\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                if args.fp16:\n",
    "                    optimizer.backward(loss)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps +=1\n",
    "                pbar.update(1)\n",
    "                mean_loss = tr_loss * args.gradient_accumulation_steps / nb_tr_steps\n",
    "                pbar.set_postfix_str(f\"Loss: {mean_loss:.5f}\")\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                for i in range(unique_id.size(0)):\n",
    "                    loss_dict[\"epoch\"].append(epoch)\n",
    "                    loss_dict[\"unique_id\"].append(unique_id[i].item())\n",
    "                    loss_dict[\"mlm_loss\"].append(mlm_loss[i].item())\n",
    "                    loss_dict[\"adversarial_loss\"].append(adversarial_loss[i].item())\n",
    "                    if args.control_task:\n",
    "                        loss_dict[\"control_loss\"].append(control_loss[i].item())\n",
    "                        loss_dict[\"total_loss\"].append(mlm_loss[i].item() + adversarial_loss[i].item() + control_loss[i].item())\n",
    "                    else:\n",
    "                        loss_dict[\"total_loss\"].append(mlm_loss[i].item() + adversarial_loss[i].item())\n",
    "        # Save a trained model\n",
    "        if epoch < num_data_epochs :#and (n_gpu > 1 and torch.distributed.get_rank() == 0 or n_gpu <= 1):\n",
    "            logger.info(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "            epoch_output_dir = args.output_dir / f\"epoch_{epoch}\"\n",
    "            epoch_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_pretrained(epoch_output_dir)\n",
    "            tokenizer.save_pretrained(epoch_output_dir)\n",
    "\n",
    "    # Save a trained model\n",
    "    if n_gpu > 1 :#and torch.distributed.get_rank() == 0 or n_gpu <=1:\n",
    "        logger.info(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "        model.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        df = pd.DataFrame.from_dict(loss_dict)\n",
    "        df.to_csv(args.output_dir/\"losses.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(bert_model='bert-large-uncased', do_lower_case=True, reduce_memory=False, epochs=5, local_rank=-1, no_cuda=False, gradient_accumulation_steps=1, train_batch_size=6, fp16=False, loss_scale=0.0, warmup_steps=0, adam_epsilon=1e-08, learning_rate=3e-05, seed=212, masking_method='double_num_adj', control_task=True, pregenerated_data=PosixPath('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj'), output_dir=PosixPath('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/IMA/double_num_adj/model_control'))\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace()\n",
    "#args.train_corpus \n",
    "#args.output_dir\n",
    "args.bert_model = BERT_PRETRAINED_MODEL\n",
    "args.do_lower_case = True\n",
    "args.reduce_memory = False\n",
    "args.epochs= int(EPOCHS)\n",
    "args.local_rank = -1\n",
    "args.no_cuda = False\n",
    "args.gradient_accumulation_steps = int(1)\n",
    "args.train_batch_size = int(BATCH_SIZE)\n",
    "args.fp16 = False\n",
    "args.loss_scale = float(0)\n",
    "args.warmup_steps = int(0)\n",
    "args.adam_epsilon = float(1e-8)\n",
    "args.learning_rate = float(3e-5)\n",
    "args.seed = int(RANDOM_SEED)\n",
    "args.masking_method = str('double_num_adj')\n",
    "args.control_task = True \n",
    "args.pregenerated_data = Path(SENTIMENT_ADJECTIVES_PRETRAIN_DATA_DIR) / args.masking_method\n",
    "if args.control_task:\n",
    "    args.output_dir = Path(SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR) / args.masking_method / \"model_control\"\n",
    "else:\n",
    "    args.output_dir = Path(SENTIMENT_ADJECTIVES_PRETRAIN_IMA_DIR) / args.masking_method / \"model\"\n",
    "args.fp16 = FP16\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:44:29 - device: cuda n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "04:44:29 - Output directory (/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/IMA/double_num_adj/model_control) already exists and is not empty!\n",
      "04:44:40 - ***** Running training *****\n",
      "04:44:40 -   Num examples = 16822\n",
      "04:44:40 -   Batch size = 6\n",
      "04:44:40 -   Num steps = 2803\n",
      "/tmp/ipykernel_2881716/393936303.py:30: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
      "04:44:40 - Loading training examples for epoch 0\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 4738.10it/s]\n",
      "04:44:40 - Loading complete\n",
      "Epoch0:   0%|          | 1/561 [00:04<46:05,  4.94s/it, Loss: 6.78456]/home/wldn/.local/lib/python3.10/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Epoch0: 100%|██████████| 561/561 [03:52<00:00,  2.42it/s, Loss: 9.13666] \n",
      "04:48:33 - ** ** * Saving fine-tuned model ** ** * \n",
      "04:48:37 - Loading training examples for epoch 1\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 4904.54it/s]\n",
      "04:48:38 - Loading complete\n",
      "Epoch1: 100%|██████████| 561/561 [03:52<00:00,  2.41it/s, Loss: 4.46212]\n",
      "04:52:31 - ** ** * Saving fine-tuned model ** ** * \n",
      "04:52:35 - Loading training examples for epoch 2\n",
      "Training examples: 100%|██████████| 3364/3364 [00:00<00:00, 4870.66it/s]\n",
      "04:52:36 - Loading complete\n",
      "Epoch2: 100%|██████████| 561/561 [03:53<00:00,  2.41it/s, Loss: 3.46608]\n",
      "04:56:29 - ** ** * Saving fine-tuned model ** ** * \n",
      "04:56:34 - Loading training examples for epoch 3\n",
      "Training examples: 100%|██████████| 3365/3365 [00:00<00:00, 4878.98it/s]\n",
      "04:56:35 - Loading complete\n",
      "Epoch3: 100%|██████████| 561/561 [03:52<00:00,  2.41it/s, Loss: 3.26388]\n",
      "05:00:28 - ** ** * Saving fine-tuned model ** ** * \n",
      "05:00:32 - Loading training examples for epoch 4\n",
      "Training examples: 100%|██████████| 3363/3363 [00:00<00:00, 4960.04it/s]\n",
      "05:00:33 - Loading complete\n",
      "Epoch4: 100%|██████████| 561/561 [03:52<00:00,  2.41it/s, Loss: 3.13730]\n",
      "05:04:26 - ** ** * Saving fine-tuned model ** ** * \n",
      "05:04:31 - ** ** * Saving fine-tuned model ** ** * \n"
     ]
    }
   ],
   "source": [
    "pretrain_on_treatment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training examples: 100%|██████████| 5992/5992 [00:01<00:00, 3850.13it/s]\n"
     ]
    }
   ],
   "source": [
    "samples_per_epoch=[]\n",
    "for i in range(args.epochs):\n",
    "    epoch_file = Path('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj/bert-base-cased_epoch_%d.json'%i)\n",
    "    metrics_file = args.pregenerated_data / f\"{BERT_PRETRAINED_MODEL}_epoch_{i}_metrics.json\"\n",
    "    if epoch_file.is_file() and metrics_file.is_file():\n",
    "        metrics = json.loads(metrics_file.read_text())\n",
    "        samples_per_epoch.append(metrics['num_training_examples'])\n",
    "num_data_epochs = args.epochs\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case = args.do_lower_case)\n",
    "total_train_examples = 0 \n",
    "for i in range(args.epochs):\n",
    "    total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n",
    "vocab = tokenizer.vocab\n",
    "epoch = 4\n",
    "data_epoch = epoch % num_data_epochs\n",
    "data_file = Path('/home/wldn/prj-nlp/jiwoo/CausaLM/Models/Sentiment/Adjectives/Pretrain/data/double_num_adj/bert-base-cased_epoch_%d.json'%epoch) \n",
    "metrics_file = args.pregenerated_data / f\"{BERT_PRETRAINED_MODEL}_epoch_{data_epoch}_metrics.json\"\n",
    "assert data_file.is_file() and metrics_file.is_file()\n",
    "metrics = json.loads(metrics_file.read_text())\n",
    "num_samples = metrics['num_training_examples']\n",
    "seq_len = metrics['max_seq_len']\n",
    "temp_dir = None\n",
    "working_dir = None\n",
    "input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
    "input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "adj_labels = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "pos_tag_labels = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX)\n",
    "unique_ids = np.zeros(shape=(num_samples,), dtype=np.int32)\n",
    "with data_file.open() as f:\n",
    "    for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
    "        line = line.strip()\n",
    "        example = json.loads(line)\n",
    "        features = convert_example_to_features(example, tokenizer, seq_len)\n",
    "        input_ids[i] = features.input_ids\n",
    "        input_masks[i] = features.input_mask\n",
    "        lm_label_ids[i] = features.lm_label_ids\n",
    "        adj_labels[i] = features.adj_labels\n",
    "        pos_tag_labels[i] = features.pos_tag_labels\n",
    "        unique_ids[i] = features.unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch0=dict()\n",
    "epoch0['input_ids'] = input_ids\n",
    "epoch0['input_masks'] = input_masks\n",
    "epoch0['lm_label_ids'] = lm_label_ids\n",
    "epoch0['adj_labels'] = adj_labels\n",
    "epoch0['pos_tag_labels'] = pos_tag_labels\n",
    "epoch0['unique_ids'] = unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./datasets/gentle/epoch4_ima', 'wb') as file:\n",
    "    pickle.dump(epoch0,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, tokenizer, max_seq_length):\n",
    "        tokens = example['tokens']\n",
    "        masked_lm_positions = np.array([int(i) for i in example['masked_lm_positions']])\n",
    "        masked_lm_labels = example['masked_lm_labels']\n",
    "        masked_adj_labels = [int(i) for i in example['masked_adj_labels']]\n",
    "        pos_tag_labels = [int(i) for i in example['pos_tag_labels']]\n",
    "        unique_id = int(example['unique_id'])\n",
    "\n",
    "        assert len(tokens) <= max_seq_length\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "        input_array = np.zeros(max_seq_length, dtype=np.int)\n",
    "        input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "        mask_array = np.zeros(max_seq_length, dtype = np.bool)\n",
    "        mask_array[:len(input_ids)] = 1\n",
    "\n",
    "        lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "        lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "\n",
    "        adj_label_array = np.full(max_seq_length, dtype=np.int, fill_value=BertTextDataset.MLM_IGNORE_LABEL_IDX)\n",
    "        adj_label_array[masked_lm_positions] = masked_adj_labels\n",
    "\n",
    "        pos_tag_labels_array = np.full(max_seq_length, dtype=np.int, fill_value=BertTokenClassificationDataset.POS_IGNORE_LABEL_IDX)\n",
    "        pos_tag_labels_array[:len(input_ids)] = pos_tag_labels\n",
    "\n",
    "        features = AdjInputFeatures(input_ids=input_array,\n",
    "                                    input_mask=mask_array,\n",
    "                                    lm_label_ids=lm_label_array,\n",
    "                                    adj_labels = adj_label_array,\n",
    "                                    pos_tag_labels=pos_tag_labels_array,\n",
    "                                unique_id=unique_id)\n",
    "        return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
